\chapter{Discussion and future work}
\label{sec:discussion}
In this thesis, we have looked at survival data in a high-dimensional setting.
While Cox regression is by far the most popular method used to estimate survival data models, and especially so in such settings, it has shortcomings that we have discussed.
One of them is the fact that it relies on a proportional hazards assumption, which does not hold when performing variable selection, which is necessary when we perform regression with high-dimensional data.
First hitting time (FHT) models are flexible alternatives that do not rely on the proportional hazards assumption.
As of the time of writing this thesis, no methods for FHT models exist to estimate parameters in a high-dimensional setting.
We have therefore discussed ways of estimating models that work well in such a setting.
In particular, we have discussed gradient boosting \citep{friedman2001}, starting with methods for estimating one parameter, and then extensions to several parameters, which we use to make an algorithm for FHTBoost.

Our goal with the thesis work was to combine FHT models and gradient boosting.
To this end, we have developed a gradient boosting algorithm for fitting an FHT model with Wiener process, with linear additive predictors corresponding to the two parameters that describe the process.
To estimate using this algorithm, we do as follows.
We start by initializing the additive predictors to intercepts, specifically the intercepts that jointly maximize the log-likelihood of the training set.
We then perform iterations where we in each step include a regularized linear least squares function in one of the covariates and one of the parameters, namely the combination of covariate and parameter which leads to the largest increase in the log-likelihood given the training data set.
The algorithm stops after $\mstop$ number of steps, where $\mstop$, which is found via repeated K-fold cross validation, is an estimate of the number of iterations which maximizes the log-likelihood with unseen data.
The resulting additive predictors of the FHT model parameters only use a small number of the available predictors in the data, effectively selecting variables which are effective in predicting the survival.
Furthermore, an important advantage of our algorithm is that we avoid having to perform a grid search to find a stopping iteration for each of the two parameters, since we implemented a \textit{noncyclical} algorithm \citep{thomas2018}, instead of the cyclical versions which were used previously \citep{schmid}.
This algorithm was implemented from scratch as a package that we called \textit{FHTBoost}, and that is freely available for download at \verb|https://github.com/vegarsti/fhtboost|.
It can be installed directly in R by using a command in the DevTools R package \citep{devtools} called \verb|install_github|, namely \verb|install_github("vegarsti/fhtboost")|.

In our boosting algorithm, we used component-wise linear learners without intercepts.
We developed two versions of FHTBoost, one with fixed intercepts, and one where the intercepts are treated as nuisance parameters, such that they are changed in each boosting iteration to the intercepts that maximize the likelihood of the current estimated model.
In a simulation study, we found that both versions consistently manage to select informative variables and shrink parameter sizes, and, importantly, consistently increase model fit while doing so.
In the uncorrelated simulation scenario, however, the fixed intercept version achieves markedly better deviance on a test set, i.e. its log-likelihood on average increased a lot more by including covariates.
In the correlated scenario, which is more realistic, the two versions achieve very similar deviance.
However, to limit the scope of the analysis in the next chapter, we chose to use only the fixed intercept version.
There we looked at a survival data set described in \citet{oberthuer-data}, where we had two clinical covariates and 9978 genetic covariates from 273 children.
We performed 100 splits into training and test sets.
FHTBoost (with fixed intercepts) in all cases estimated a model with negative deviance, i.e. covariates increase the log-likelihood of the model given an unseen test set.
The FHT models estimated with \textit{FHTBoost} have good predictive power, and achieve a mean integrated Brier score of 0.074, but is beaten by the regular Cox model, which has a mean of 0.064.
In other words, FHT models estimated by \textit{FHTBoost} achieve comparable predictive power as state of the art Cox models on this data, but did not outperform it.

There are several interesting directions for further work.
One is to apply \textit{FHTBoost} on other high-dimensional survival data sets that have been studies.
This would allow for a broader assessment of its usefulness and predictive power in realistic high-dimensional settings.
One of the strengths of gradient boosting is that it is easy to use in combination with different component-wise base learners.
In this thesis, we have only considered linear base learners.
It would be interesting to include nonlinearity into these, which could be done by e.g. using regularized splines of one dimension as base learners.
Another interesting direction of further work would be to incorporate the FHT model into the existing ecosystem of gradient boosting packages in R.
This ecosystem is built on the package \textit{mboost} \citep{mboost}, which is actively maintained.
The framework used for FHT regression fits into the GAMLSS \citep{gamlss} framework, for which the \textit{gamboostLSS} package \citep{gamboostlss-paper, gamboostLSS-manual} has been developed.
There also exists an extension for fitting GAMLSS models to censored data, \textit{gamlss.cens} \citep{gamlsscens}.
It should be possible to include our parameterization of the inverse Gaussian into it.
If this is done, which requires implementing functions for the log-likelihood and its partial derivatives, it should be possible to use the entire library of base learners in \textit{mboost}.
This would let statisticians studying high-dimensional survival data sets include the FHT model alongside boosted Cox models in their analyses.
