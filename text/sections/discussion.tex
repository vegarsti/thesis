\chapter{Discussion and future work}
\label{sec:discussion}
In this thesis, we have considered survival data in a high-dimensional setting.
While Cox regression is by far the most popular method used to estimate survival data models in such settings, it has shortcomings that we have discussed.
One of them is the fact that it relies on a proportional hazards assumption.
This does not hold when performing variable selection, which is necessary when we perform regression with high-dimensional data.
First hitting time (FHT) models are flexible alternatives that do not rely on the proportional hazards assumption.
As of the time of writing this thesis, no methods for FHT models exist to estimate parameters in a high-dimensional setting.
We have therefore discussed ways of estimating models that work well in such a setting.
In particular, we have discussed gradient boosting \citep{friedman2001}, starting with methods for estimating one parameter, and then extensions to several parameters.

Our goal with this thesis was to combine FHT models and gradient boosting.
To this end, we have developed a gradient boosting algorithm for fitting the parameters of an FHT model with a Wiener health process, with linear additive predictors corresponding to the two parameters that describe the process.
To estimate using this algorithm, we do as follows.
We start by initializing the additive predictors to intercepts, specifically the intercepts that jointly maximize the log-likelihood of the training set.
We then perform iterations where we in each step include a regularized linear least squares function in one of the covariates and one of the parameters, namely the combination of covariate and parameter which leads to the largest increase in the log-likelihood given the training data set.
The algorithm stops after $\mstop$ number of steps, where $\mstop$, which is found via repeated K-fold cross validation, is an estimate of the number of iterations which maximizes the log-likelihood with unseen data.
The resulting additive predictors of the FHT model parameters use only a small number of the available predictors in the data, effectively selecting those variables which are most effective in predicting the survival.
Furthermore, an important advantage of our algorithm is that we avoid having to perform a grid search to find a separate stopping iteration for each of the two parameters, since we implemented a \textit{noncyclical} algorithm \citep{thomas2018}, instead of the cyclical versions which were developed earlier \citep{schmid}.
This algorithm was implemented from scratch as a package that we called \textit{FHTBoost}, and that is freely available for download at \verb|https://github.com/vegarsti/fhtboost|.
It can be installed directly in R by invoking \verb|install_github("vegarsti/fhtboost")|, assuming the DevTools package \citep{devtools} is installed.

In \textit{FHTBoost}, we used component-wise linear learners without intercepts.
We developed two versions of \textit{FHTBoost}, one where the intercepts of the additive predictors are fixed, i.e. not changed after they are initialized before the first boosting iteration, and one where the intercepts are treated as nuisance parameters.
In the latter, they are changed in each boosting iteration to the intercepts that maximize the likelihood of the current estimated model.
In a simulation study, we found that both versions consistently manage to select informative variables and shrink parameter sizes, and, importantly, consistently increase model fit while doing so.
Surprisingly, in the uncorrelated simulation scenario, the fixed intercept version achieves markedly better deviance on a test set, i.e. its log-likelihood on average increased more by including covariates, than did the changing intercepts version.
In the correlated scenario, which is more realistic, the two versions achieve very similar deviance.
However, to limit the scope of the analysis in the next chapter, we chose to use only the fixed intercepts version.
In that chapter, we looked at a survival data set described in \citet{oberthuer-data}, where we had two clinical covariates and 9978 genetic covariates from 273 children.
We allow the parameter which corresponds to the initial level of the Wiener process, to depend on the genetic covariates, while we allow the drift to depend on the clinical covariates.
We performed 100 splits into training and test sets.
\textit{FHTBoost} (with fixed intercepts) in all cases estimated a model with negative deviance on the test set.
The FHT models estimated with \textit{FHTBoost} have good predictive power, and achieve a mean integrated Brier score of 0.074 across the 100 splits.
However, the FHT models are on average beaten by the regular Cox model, which achieves a mean integrated Brier score of 0.064.
In other words, FHT models estimated by \textit{FHTBoost} achieve comparable predictive power as state of the art Cox models on this data, but did not outperform it.
The FHT model was also on average slightly beaten by the estimated mandatory Cox model, where we did not penalize the clinical covariates.
It achieved a mean score of 0.069.
We also compared these to FHT models where we only estimated one of the parameters, i.e. only using clinical covariates or only using genetic covariates, but these performed worse than those above.

There are several interesting directions for further work which would build on our contribution.
One is to apply \textit{FHTBoost} on other high-dimensional survival data sets.
This would allow for a broader assessment of its usefulness and predictive power in realistic high-dimensional settings.
Furthermore, due to the flexibility of both the FHT framework and gradient boosting, it would easily be possible to use other FHT models, such as the inverse Gamma (see subsection \ref{subsec:fht-examples}), in \textit{FHTBoost}.
By implementing functions to calculate the log-likelihood and partial derivatives, \textit{FHTBoost} would be able to estimate the parameters in these other FHT models.
Another of the strengths of gradient boosting is its modularity, and that it is easy to use in combination with different component-wise base learners, and in this thesis, we have only considered linear base learners.
It would be interesting to include nonlinearity into the additive predictors of the FHT parameters.
This could be done by e.g. using regularized splines of one dimension as base learners.
Another interesting direction of further work would be to incorporate the FHT model into the existing ecosystem of gradient boosting packages in R.
This ecosystem is built on the package \textit{mboost} \citep{mboost}, which is actively maintained.
The framework used for FHT regression fits into the GAMLSS \citep{gamlss} framework, for which the \textit{gamboostLSS} package \citep{gamboostlss-paper, gamboostLSS-manual} has been developed.
There also exists an extension for fitting GAMLSS models to censored data, \textit{gamlss.cens} \citep{gamlsscens}.
It should be possible to include our parameterization of the inverse Gaussian into it.
If this is done, which requires implementing functions for the log-likelihood and its partial derivatives, it should be possible to use the entire library of base learners in \textit{mboost}.
This would let statisticians studying high-dimensional survival data sets include the FHT model alongside boosted Cox models in their analyses.
