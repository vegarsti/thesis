\chapter{Evaluation measures}
In this chapter, we will explain different evaluation measures which we will use to assess different aspects of models, later in the thesis.

\section{Assessing model fit with difference in deviance between an estimated model and a null model}
\label{sec:deviance}
To comparing the performance of two models of the same kind, e.g., two different estimated FHT models, we can calculate their difference in deviance.

\subsection{Deviance}
In general, the deviance of an FHT model with covariate vectors $\bbeta$ and $\bgamma$, which we concatenate as
\begin{equation*}
    \btheta=(\bbeta,\bgamma),
\end{equation*}
is
\begin{equation*}
    \text{dev}(\btheta)=2\cdot l(\btheta),
\end{equation*}
where $l(\btheta)$ is the log-likelihood value attained by an FHT model with covariate vector $\btheta$.
As we know, the better the model fit of $\btheta$, the higher the value of $l(\btheta)$.
Deviance is used in the generalized linear models (GLM) framework, where there exist general results related to the deviance.
These results, which we will not go into here, are the reason why there is a factor of 2 in the deviance.
This factor does not matter for us, but we will use it to adhere to convention.

\subsection{Difference in deviance}
Consider two FHT models $\hat{\btheta}_1$ and $\hat{\btheta}_2$, where $\hat{\btheta}_1$ is a less complex model, i.e., it has fewer, or possibly smaller, covariates than $\hat{\btheta}_2$.
The difference in deviance between $\hat{\btheta}_1$ and $\hat{\btheta}_2$, which we denote $d$, is simply the deviance of the more complex model, $\hat{\btheta}_2$, subtracted from the deviance of the less complex model, $\hat{\btheta}_1$,
\begin{equation*}
    d=\text{dev}(\hat{\btheta}_1)-\text{dev}(\hat{\btheta}_2)=2\cdot l(\hat{\btheta}_1)-2\cdot l(\hat{\btheta}_2)=2\left(l(\hat{\btheta}_1)-l(\hat{\btheta}_2)\right).
\end{equation*}
Since a more complex model should achieve a higher log-likelihood, we expect the difference of deviance to be negative if the model is a good fit to the data.
For our purposes, the difference of deviance is a measure of how much the estimated model improves on a model with no covariates, a so-called null model.

\subsection{Null model}
A model which incorporates no covariate information is called a \textit{null model}, and it will behave the same, no matter the covariate information it is given.
A null model thus acts as a baseline to which we can compare our estimated model.
If an estimated model does not explain the variation of the data better than the null model, and consequently the chosen model is not a good model fit.

In our case, when using FHTBoost, the null model is an FHT model which only contains the intercepts in the covariate vectors.
In our procedure, before we start the boosting iterations, we find the maximum likelihood intercepts of the covariate vectors, $\beta_0$ and $\gamma_0$.
These are found by numerically maximizing the likelihood of the training set.
This is the model we would get if we performed FHTBoost with 0 iterations.
The covariate vectors of the null model, i.e., the covariate vectors before starting boosting, are thus vectors containing only the intercepts, and all other elements being 0:
\begin{equation*}
    \hat{\bbeta}^{[0]}_{\text{train}}=(\overbrace{\hat{\beta}_0^{[0]},0,0,\ldots,0}^{\text{length }p_1})
\end{equation*}
and
\begin{equation*}
    \hat{\bgamma}^{[0]}_{\text{train}}=(\overbrace{\hat{\gamma}_0^{[0]},0,0,\ldots,0}^{\text{length }p_2}).
\end{equation*}
For simplicity of notation, we again denote the concatenated vector of these as
\begin{equation*}
    \hat{\btheta}^{[0]}_{\text{train}}=\left(\hat{\bbeta}_{\text{train}}^{[0]},\hat{\bgamma}_{\text{train}}^{[0]}\right).
\end{equation*}
Similarly, an estimated model, which is estimated by using FHTBoost on the training set with $\mstop$ steps, has covariate vectors
\begin{equation*}
    \hat{\bbeta}^{[\mstop]}_{\text{train}}=(\hat{\beta}_0^{[\mstop]},\hat{\beta}_1^{[\mstop]},\hat{\beta}_2^{[\mstop]},\ldots,\hat{\beta}_{p_1}^{[\mstop]})
\end{equation*}
and
\begin{equation*}
    \hat{\bgamma}^{[\mstop]}_{\text{train}}=(\hat{\gamma}_0^{[\mstop]},\hat{\gamma}_1^{[\mstop]},\hat{\gamma}_2^{[\mstop]},\ldots,\hat{\gamma}_{p_2}^{[\mstop]}).
\end{equation*}
Again let their concatenation be denoted
\begin{equation*}
    \hat{\btheta}^{[\mstop]}_{\text{train}}=\left(\hat{\bbeta}^{[\mstop]}_{\text{train}},\hat{\bgamma}^{[\mstop]}_{\text{train}}\right).
\end{equation*}
To avoid overfitting, we calculate the log-likelihood values on data from a separate test set, for reasons we have discussed previously.
A model estimated on the training set, $\hat{\btheta}_{\text{train}}$, has a likelihood on the test set of
\begin{equation}
    l^{\text{test}}\left(\hat{\btheta}_{\text{train}}\right)=\sum_{i=1}^{N_{\text{test}}}\rho(y_i, \hat{\btheta}).
\end{equation}
Hence the difference in deviance between a fitted model and the null model containing no covariates is
\begin{equation*}
    d=2\left(l^{\text{test}}\left(\hat{\btheta}^{[0]}_{\text{train}}\right)-l^{\text{test}}\left(\hat{\btheta}^{[\mstop]}_{\text{train}}\right)\right).
\end{equation*}
The performance of the estimated model $\hat{\btheta}^{[\mstop]}_{\text{train}}$ is good when $d$ is small, meaning ``very negative.''
We may still end up with a \textit{positive} difference of deviance.
This means that the null model achieved a higher log-likelihood value than the fully estimated model, which is opposite of what we would expect.
This is the typical effect of overfitting, where the model ``follows'' too much random variability in the training set, and hence performs badly on the test set.


\section{Variable selection measures}
\label{sec:variable-selection}
As shown in section \ref{sec:variable-selection}, a component-wise gradient boosting algorithm, like FHTBoost, performs data-driven variable selection.
The two major objectives of a component-wise gradient boosting algorithm is to perform selection of variables, and shrinkage of variables.
We may wish to measure the first of these two, namely the effectiveness of the variable selection.
We will, however, not in this section be able to discuss the shrinkage as well.
One way to measure the variable selection is to treat it as a classification problem.
In this view, correctly selecting a true variable (adding this variable to the boosting model) is an instance of correct classification.
In this section we discuss such classification measures.

\subsection{Terminology}
We denote a variable selected by the algorithm as ``positive,'' or $P$ for short, and a variable that is not selected as ``negative,'' or $N$ for short.
Since we know which variables actually affect the response, we know how many of the variables selected are selected correctly, in the sense
that they are selected and they have an effect. We call these ``true positives,'' or $TP$ for short.
Similarly, we know which variables do not affect the response, and hence we can calculate the number of non-informative variables
which were not selected, i.e., true negative, or $TN$ for short.
Furthermore, we say that selected variables which in truth do not have an effect, are false positives ($FP$).
Similarly, false negatives ($FN$) are variables which do have an effect, but which were not selected in the boosting model.
We now give explain three such metrics that we will use to determine how well a model performs variable selection.

\subsection{Classification measures}
\textbf{Sensitivity} measures the proportion of the selected variables which are informative.
The ideal sensitivity is 1, whereas the worst possible sensitivity is 0.
\begin{equation}\label{eq:sensitivity}
    \text{Sensitivity}=\frac{TP}{P}
\end{equation}
\textbf{Specificity} measures the proportion of variables \textit{not} selected which were not informative.
The ideal specificity is 1, and the worst is 0.
\begin{equation}\label{eq:specificity}
    \text{Specificity}=\frac{TN}{N}
\end{equation}
\textbf{False discovery rate} measures the proportion of selected variables which are in truth not informative.
The ideal false discovery rate is 0, and the worst is 1.
Furthermore, we might note that if the rate is above 0.5, then there are more false positives than true positives in the variable selection.
\begin{equation}\label{eq:accuracy}
    \text{FDR}=\frac{FP}{FP+TP}
\end{equation}

\section{Evaluating survival prediction with the Brier score}\label{sec:brier}
The Brier score \citep{brier1950} was first introduced as a way to measure the accuracy of weather forecasts, and later translated into survival analysis \citep{graf}.
Let us first consider, for ease of presentation, a survival data case where there is

\subsection{Brier score on uncensored survival data}
Consider a test set of $N_{\text{test}}$ individuals.
For all observations $i=1,\ldots,N_{\text{test}}$, the test set contains an observed survival time $t_i$, and a covariate vector $\x_i$.
The estimated survival probability of individual $i$ at time $t^*$ is
\begin{equation}\label{eq:phat}
    %\widehat{\Pr}(i \text{ has not experienced an event at }t^*)\coloneqq
    \hat{\surv}(t^*|\x_i),
\end{equation}
where the prediction function $\hat{\surv}(t^*|\x_i)$ is obtained from an FHT model.
The Brier score aims to evaluate how well $\hat{\surv}(t^*|\x_i)$ \eqref{eq:phat} is able to predict the event status of individual $i$ at a given time $t^*$, namely
\begin{equation}
    \indicator(t_i>t^*).
\end{equation}
The error made in predicting the event status for a patient in the test set can be given as
\begin{align*}
BS(t^*)&=\frac{1}{N_{\text{test}}}\sum_{i=1}^{N_{\text{test}}}\left(I(t_i>t^*)-\hat{\surv}(t^*|\x_i)\right)^2 \\
    &=\frac{1}{N_{\text{test}}}\sum_{i=1}^{N_{\text{test}}}\left[\hat{\surv}(t^*|\x_i)^2I(t_i\leq t^*)+(1-\hat{\surv}(t^*|\x_i))(I(t_i>t^*)\right].
\end{align*}
In the first formulation, the Brier score is similar to a residual-sum-of-squares (RSS) measure, since we sum the squared error between the observed event and the estimated probability.
In the second formulation, we see that the $BS(t^*)$ measure is the average of the Brier score of events that have occurred before time $t^*$, and events that have not yet occurred.
In the case of censored data, however, the above is not enough.

\subsection{Brier score on censored survival data}
The Brier score was consequently adapted to handle censored survival times by \citet{graf}.
A crucial assumption is that the censored survival times have the independent censoring property (see subsection \ref{subsec:survdata}).
They showed that the loss of information due to censoring can be accounted for by using an inverse probability of censoring weighting \citep{graf}.
This version of the Brier score for censored data is defined as
\begin{equation}\label{eq:brier}
    BS^c(t^*)=\frac{1}{N}\sum_{i=1}^N\left[\frac{\hat{S}(t^*|\x_i)^2I(t_i\leq t^*,\delta_i=1)}{\hat{G}(t_i)}+\frac{(1-\hat{S}(t^*|\x_i))^2(I(t_i>t^*)}{\hat{G}(t^*)}\right].
\end{equation}
Here $\hat{G}(\cdot)$, defined as
\begin{equation*}
    \hat{G}(t)=\prod_{i\in \overline{R}(t)}\left(1-\frac{1-\delta_i}{\sum_{i=1}^NY_i(t)}\right),
\end{equation*}
is the Kaplan-Meier estimate of the \textit{censoring distribution} \citep{bovelstadborgan}, and where further $Y_i(t)$ is an indicator of whether individual $i$ is at risk at time $t$, and where $\overline{R}(t)$ is the set of individuals \textit{not} at risk at time $t$, i.e.,
\begin{equation*}
    \overline{R}(t)=\{i\colon t_i<t,i=1,2,\ldots,N\}.
\end{equation*}
If we look closely at \eqref{eq:brier}, we see that the $BS^c(t^*)$ measure is a weighted average of the Brier score of events having happened at or before time $t^*$, and the events that have not yet occurred.

%\subsection{$R^2$ measure based on Brier score}
%The Brier score may also be used to define an $R^2$ measure, where one can benchmark the performance of a fitted model to a so-called null model, i.e., one where each regression coefficient is set to zero. A measure of explained variation can be found by calculating the gain in accuracy when adding covariates. Thus we define the Brier $R^2$ measure as
%\begin{equation*}
%    R^2_{\text{Brier}}(t^*)=1-\frac{BS^c(t^*)}{BS^c_0(t^*)},
%\end{equation*}
%where $BS^c_0(t^*)$ is the Brier score for the null model.
%One advantage with $R^2_{\text{Brier}}$ is that it adjusts for variation due to the specific data under study, which the Brier score itself does not \citep{bovelstadborgan}.
%Note, though, that this $R^2$ measure is model specific, since it is based on a comparison between two models of the same kind.
%It should therefore be used to assess how well a specific model, such as FHTBoost, improves with covariates.
%However, to compare the predictive power of different models, such as FHTBoost and CoxBoost, one should use the ``raw'' Brier score $BS^c(\cdot)$.

\subsection{Integrated Brier score}\label{subsec:integrated-brier}
The Brier score $BS^c(\cdot)$ \eqref{eq:brier} only considers a given timepoint.
We might also be interested in a time interval, say, from time $t_1$ to time $t_2$.
The integrated Brier score is simply the Brier score integrated over this time interval,
\begin{equation*}
    \text{IBS}(t_1, t_2)=\frac{1}{t_2-t_1}\int_{t_1}^{t_2}BS^c(t)\d t.
\end{equation*}
In practice, we can obtain this by computing the Brier score \ref{eq:brier} over a fine grid of time points $t\in[t_1,t_2]$, and then taking the average of these values.

By partitioning this time interval into smaller adjoint time intervals
\begin{equation*}
    t_{\text{start}}=t_0<t_1<\cdots<t_k=t_{\text{end}},
\end{equation*}
we can construct an approximation of the integrated Brier score by
\begin{equation*}
    \text{IBS}(t_{\text{start}}, t_{\text{end}})\approx\sum_{j=1}^k BS^c(t_j)\cdot(t_{j+1}-t_{j}).
\end{equation*}
\todo[inline]{This needs work still}