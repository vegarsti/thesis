\chapter{Multivariate component-wise boosting on survival data}
In this chapter, we propose a component-wise boosting algorithm for fitting the inverse gaussian first hitting time model to survival data.

\section{FHTBoost}
The first-hitting-time model with Wiener processes, as shown, leads to inverse Gaussian lifetimes. As in the usual
regression scheme (see subsection \ref{subsec:IG-reg}) for this setup, we we have $K=2$ distribution parameters,
\begin{equation}
    \btheta_i=(\theta_1,\theta_2)^T=(y_0,\mu)^T.
\end{equation}
We choose link functions
\begin{equation}
    g_1(x)=\log(x)
\end{equation}
and
\begin{equation}
    g_2(x)=\id(x)=x,
\end{equation}
for parameters $y_0$ and $\mu$, respectively.

We wish to model the effect of covariates on these parameters. In particular, we will let $y_0$ be modeled by a high-dimensional matrix $X$,
in practice typically gene expression data, and we will let $\mu$ be modeled by a low-dimensional matrix $Z$, typically clinical data.
We let a vector of covariate information from one individual $i,i=1,2,\ldots,n$ be labeled $\x_i$ and $\z_i$, where these are
gene data and clinical data, respectively.
These matrices are defined as
\begin{equation}
    X=(\x_1,\x_2,\ldots,\x_{p_1}),
\end{equation}
and
\begin{equation}
    Z=(\z_1,\z_2,\ldots,\z_{p_2}),
\end{equation}
where the number of dimensions $p_1$ of $X$ is high (in particular, $p_1$ is much larger than $n$, $p_1 >> n$),
and the number of dimensions $p_2$ of $Z$ is relatively small, and in particular, $p_2 < n$.

We apply the GAMLSSBoost algorithm shown previously (see section \ref{sec:gamlssboost}) to this setup. The loss function of interest is
the negative log likelihood of the censored inverse Gaussian distribution, derived in the Appendix, i.e.,
\begin{equation*}
    \rho(y,\btheta)=-l((t,d), (y_0,\mu)).
\end{equation*}
(See Appendix for the full expression!)
We wish to build up an additive predictor $\boldeta$, consisting of components $\eta_1$ and $\eta_2$. As in the GAMLSS setting, these are given by
\begin{equation}
    \eta_1\coloneqq \exp(y_0)=\beta_{1,0}+\sum_{j=1}^{p_1}f_{1,j}(x_{1,j})
\end{equation},
and
\begin{equation}
    \eta_2\coloneqq \mu=\beta_{2,0}+\sum_{j=1}^{p_2}f_{2,j}(x_{2,j}).
\end{equation},
We wish to estimate these additive predictors by a gradient boosting algorithm.
Given an estimated additive predictor $\hat{\boldeta}$, we get the corresponding estimated distribution parameters by
transforming the additive predictors via the inverse of their link functions. Thus,
\begin{equation}
    y_0=\theta_1=g_1^{-1}(\eta_1)=\exp(\eta_1),
\end{equation}
and
\begin{equation}
    \mu=\theta_2=g_2^{-2}(\eta_2)=\eta_2.
\end{equation}
This means that as a function of the additive predictors, the loss function is
\begin{equation*}
    \rho(y,\boldeta)=-l(\exp(\eta_1), \eta_2).
\end{equation*}
To use the gradient boosting algorithm, we of course need to have the negative gradient of the loss function, i.e.,
the negative derivative. The negative partial derivative of the loss function is equal to the (positive) derivative of the (positive) 
log-likelihood function, with regard to each parameter. These are
\begin{equation}
    -\frac{\partial}{\partial\eta_1}\rho(y_i,\btheta)=\frac{\partial}{\partial\eta_1}l(\exp(\eta_1), \eta_2)
\end{equation}
and
\begin{equation}
    -\frac{\partial}{\partial\eta_2}\rho(y_i,\btheta)=\frac{\partial}{\partial\eta_1}l(\exp(\eta_1), \eta_2).
\end{equation}
(See Appendix for the full expressions for these!)

\begin{algorithm}
\caption{FHT Boost with twodimensional loss function}
\label{algo:fhtboost}
\begin{enumerate}
    \item Lorem ipsum
\end{enumerate}
\end{algorithm}

\subsection{Modification: Changing the intercept in each iteration}
Another way to do this is to only boost one component in each iteration. The component might be corresponding to $X$, or it might be corresponding to $Z$.