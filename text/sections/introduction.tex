\chapter{Introduction and outline of the thesis}
\label{sec:intro}
Survival data are data that is concerned with the time to an event.
We typically study a cohort of patients after a cancer surgery, and we record the time to a possible recurrence of the cancer, or we study the time it takes from a child from a set of parents is born until a second child is born.
If the event happens, we record the observed time.
Not all statistical units experience an event during the time in which the study is conducted.
For some of them we only know the last time that is recorded, e.g. at the latest check-up.
These observations are right-censored, and add some complications in the data analysis.

Almost all practical modeling of survival data is done with the famous Cox regression model \citep{cox-model}.
One of its characteristics is that it models the hazard.
The hazard of an individual at a time $t$ is the probability that the individual experiences an event, given that the event has not happened yet, in the infinitesimal time interval at $t$.
A key underlying assumption of the Cox model is that individuals have proportional hazards at all times, i.e. the ratio of their hazards does not change over time.
This assumption is not always valid, and there is a need for more flexible models.
Among several options, in this thesis we will focus on first hitting time models (FHT) \citep{leewhitmore2006}.
Instead of modeling the hazard, the main idea of FHT models is to model the underlying process of an individual before an event occurs.
To model its life, so to speak.
The FHT model framework is a rich and flexible modeling framework, where we specify a stochastic process and an appropriate threshold, barrier, or boundary, depending on the association we wish to evoke.
When the process hits the barrier, or crosses the boundary, the event is triggered.
The time to event is therefore the time that passes from the process starts until it hits the barrier.
For certain choices of processes, there exist fully parameterized expressions which can be used in regression.
This is the case for the Wiener process with initial level and drift, which leads to a bivariate probability distribution.
As usual, the parameters of this distribution are related to explanatory variables, in a regression setup with additive predictors.

An important part of modern biomedical statistics is the ability to deal with high-dimensional data, for example genetic data.
Genetic data are nowadays widely available, and typical data sets include gene expressions from genes numbering in the tens of thousands.
Such data are called high-dimensional data, because there are usually more variables than observations.
A scenario where the number of covariates $p$, is much larger than the number of predictors, $n$, also often referred to as the $p\gg n$ scenario.
One can think of the genes from one individual as one point in a high-dimensional space where each gene spans one dimension.
Somewhat counterintuitively, virtually all points in high-dimensional space will be far apart.
Especially if there are few points in this space compared with the number of dimensions of the space, it makes it very easy for statistical models to overfit, i.e. to explain the variation in the data in a way that is not really true.
Furthermore, estimated models that are overfit would not carry over predictive power to unseen data of a similar kind.
Many classical statistical models are simply unable to use so many predictors, at least directly.

When it comes to survival analysis, there are models which extend Cox regression to such settings.
They perform empirically well, but it is somewhat unclear how the check the proportional hazards assumption.
In contrast, there do not exist similar extensions for the FHT model.
To the best of our knowledge, there has been no attempt at developing methods for first hitting time regression in a high-dimensional setting.
In this thesis we try to fill this gap by developing a boosting algoritm which allows fitting a FHT model with high-dimensional data.
Gradient boosting is an algorithmical framework that has been very successful in high-dimensional ($p\gg n$) scenarios.

It originated at the end of the 20th century in the field of machine learning.
\citet{friedman2001} later provided a statistical view.
Gradient boosting algorithms are iterative algorithms for performing regularized minimization of a loss function, which can very well be a negative log likelihood function.
Gradient boosting is still very much an active field of research, and various methods exist for model fitting.
Most traditional gradient boosting algorithms are concerned with modelling one parameter.
The Wiener FHT model we use is a bivariate model, and we can therefore not use the lasso, nor can we use the standard gradient boosting methods.
In more recent years, however, gradient boosting methods for regression of parameters beyond the mean have also been introduced.

In Chapter 2, we discuss the survival analysis setting, Cox regression, and FHT models.
These are first discussed in general, before addressing in detail the specific case where we use a Wiener process.
In Chapter 3, we discuss gradient boosting, including recent methods for estimating multidimensional models \citep{thomas2018}.
The main result of this thesis can be found in Chapter 4, where we develop a multidimensional, component-wise gradient boosting algorithm to fit the bivariate FHT model with Wiener processes, which we call \textit{FHTBoost}.
We implement this boosting algorithm entirely from scratch.
Using \textit{FHTBoost}, we can estimate the linear additive predictors of the FHT model.

Chapter 5 explains evaluation measures that we then use in subsequent Chapters.
In Chapter 6, we perform a simulation study where we verify that \textit{FHTBoost} manages to estimate the parameters of the FHT model.
We consider two scenarios, one with uncorrelated data, and a more realistic scenario with highly correlated data.
Chapter 7 looks at a survival data set consisting children diagnosed with neuroblastoma.
We estimate an FHT model on this data, and discuss the results.
We then compare its predictive performance with that of a boosted Cox regression.
Finally, we provide a conclusion and some ideas for future work.