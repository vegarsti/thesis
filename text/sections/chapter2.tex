\chapter{Survival analysis}

\section{Survival data and basic definitions}
Survival analysis is the field of studying lifetime and time-to-death data. We look at a stochastic variable $T>0$ which is the time to some event. To observe such data in real life, we must wait until the event actually happens. This might in some cases never happen, or it might take a very long time. One example is a clinical trial of $n$ patients who have been treated for some disease, and where $T_i$, $i=1,\ldots,n$, is the time until their relapse. Typically these trials are for a set amount of time, say, until $\tau$. Luckily, not every patient relapses during that time, and so their time of relapse $T_i$ is not observed. We could throw away these observations, but we at least know that they survived until $t=\tau$. We therefore work with the concept of incomplete data, which we call \textit{censored} data. An observed lifetime $\tT$ is censored if the actual lifetime $T$ is larger than $\tT$. We can say that we have a censoring mechanism which works such that the observed $\tT=\min(T, C)$, where $C$ is a censoring time. In the clinical trial example mentioned, $C=\tau$. We also need a censoring indicator, $D=I(\tT=T)$, indicating if we have observed the actual event.

An overview of modelling survival data is \citet{ABG}.

\subsection{Censored survival data}
If the event has occurred, the indicator $d_i$ is 1. 

\subsection{The survival function $S(t)$}
In survival analysis, one of the things we are interested in is the survival function. The survival function $S(t)$ is the probability of surviving until time $t$,
\begin{align*}
    S(t)=\Pr(T>t)=1-\Pr(T<t)=1-F(t).
\end{align*}
Here $F(t)$ is the familiar cumulative distribution function. If the derivative $f(t)$ of $F(t)$ exists, the lifetime $T$ has probability distribution function $f(t)$.

\subsection{The hazard function $\hz(t)$}
We are also interested in the hazard function. This is the probability of the event happening at time $t$, conditioned on the event not having happened yet. More formally, the hazard function is defined as
\begin{equation*}
    \hz(t)=\lim_{\epsilon\to0}\frac{\Pr(T<t+\epsilon|T>t)}{\epsilon}.
\end{equation*}
The estimation of the hazard function is hard, and we do not achieve the usual $\sqrt{n}$ convergence. \todo[inline]{add citation}
Note that
\begin{equation*}
    \Pr(T<t+\epsilon|T>t)=\frac{\Pr(T<t+\epsilon,T>t)}{\Pr(T>t)}=\frac{F(t+\epsilon)-F(t)}{S(t)},
\end{equation*}
and inserting this into the hazard function yields
\begin{equation}
\label{eq:hfs}
    \hz(t)=\frac{1}{S(t)}\lim_{\epsilon\to0}\frac{F(t+\epsilon)-F(t)}{\epsilon}=\frac{f(t)}{S(t)}=\frac{-S^\prime(t)}{S(t)},
\end{equation}
where the probability distribution function $f(t)$ is obtained by its limit definition, and we note that $S^\prime(t)$ is the derivative of $1-F(t)$, which is $-f(t)$. By integrating the hazard from 0 to time $t$, we get the cumulative hazard function $A(t)=\int_0^t\hz(s)\d s$,
\begin{equation}\label{eq:cumulative-hazard}
    A(t)=-\int_0^t\frac{S^\prime(s)}{S(s)}\d s=-\int_0^t\frac{\frac{\d S}{\d f}}{S(s)}\d s=-\int_0^t\frac{1}{S(s)}\d s=-\log(S(t)).
\end{equation}
Given censored survival data $(t_i,d_i),i=1,\ldots,n$, we introduce the at-risk function $Y(t)$, which is equal to the number of individuals still at risk at time $t$,
\begin{equation*}
    Y(t)=\#\{t\colon t_i\geq t\},
\end{equation*}
where $\#(\cdot)$ is the counting operator over a set. We may then estimate the survival function $S(t)$ by the Kaplan-Meier estimator \citep{kaplan-meier}
\begin{equation*}
    \hat{S}(t)=\prod_{t_i\leq t}1-\frac{d_i}{Y(t_i)},
\end{equation*}
and the cumulative hazard function $A(t)$ by the Nelson-Aalen estimator \citep{nelson, aalen1978}.
\begin{equation*}
    \hat{A}(t)=\sum_{t_i\leq t}\frac{d_i}{Y(t_i)}.
\end{equation*}

\section{Survival data likelihood regression setup}
Given survival data with covariates, $(t_i,d_i,\x_i)$, and parameterized functions $S(t|\x_i,\bbeta)$ and $f(t|\x_i,\bbeta)$ corresponding to a survival distribution, where $\bbeta=(\beta_1,\ldots,\beta_p)$ is a vector of regression coefficients, we want to set up a likelihood for the data. Assume that the data is independent and identically distributed. We can then use the information about the lifetime distribution, such that the single individual $i$ contributes
\begin{equation}\label{eq:f}
    f(t_i|\xi)
\end{equation}
to the likelihood. If the event has not occurred, the observation is censored, and $d_i$ is 0. In this case, we do not have the actual lifetime, and so we cannot use the lifetime distribution, but we must rather use the survival distribution. Therefore this observation contributes
\begin{equation}\label{eq:S}
    S(t_i|\xi)
\end{equation}
to the likelihood. Of course, since an observation can only be either censored or not censored at the same time, $\di$ is either 0 or 1. We can cmombine expressions \eqref{eq:f} and \eqref{eq:S} in a way that a single observation contributes
\begin{equation*}
    f(\ti|\x_i)^{\di}S(\ti|\x_i)^{1-\di}
\end{equation*}
to the likelihood. Since we assume the observations to be independent, the likelihood is the product of the single complete contributions. The complete likelihood becomes
\begin{equation}\label{eq:surv-lik}
    L(\bbeta)=\prod_{i=1}^n f(t_i|\x_i,\bbeta)^{d_i} S(t_i|\x_i,\bbeta)^{1-d_i}.
\end{equation}
Since it is more convenient to work with the log likelihood, we calculate this as well,
\begin{align}\label{eq:surv-lik}
\begin{split}
    l(\bbeta)&=\log L(\bbeta) \\
    &=\sum_{i=1}^n\left[ d_i \log f(t_i|\x_i,\bbeta) + (1-d_i)\log S(t_i|\x_i,\bbeta)\right].
\end{split}
\end{align}
Note that since $\log S(t)=-A(t)$ and $f(t)=\alpha(t)S(t)$, see \eqref{eq:cumulative-hazard} and \eqref{eq:hfs}, respectively, \eqref{eq:surv-lik} further simplifies to
\begin{equation*}
    l(\bbeta)=\sum_{i=1}^n\left[ d_i \log \hz(t_i|\x_i,\bbeta) - A(t_i|\x_i,\bbeta)\right].
\end{equation*}

\section{Proportional hazards regression}
How may we use a covariate vector $\x$ in modelling, say, the hazard rate? A very common model to choose here is that of a proportional hazards model, which assumes
\begin{equation}\label{PH}
    \hz(t|\x)=\hz_0(t)r(\x|\bbeta),
\end{equation}
where $\hz_0(t)$ is an \textit{unspecified} baseline hazard function shared between all individuals, and $r(\x|\beta)$ is a so-called relative risk function parameterized with regression coefficients $\bbeta=(\beta_1,\ldots,\beta_p)$. We choose $r(\x)$ such that it is appropriately normalized, meaning $r(\0)=1$. A crucial assumption here is that the effects of the covariates are fixed in time. In this setup, it turns out that we can do regression without specifying the baseline hazard. This is a major advantage, because we then do not have to think about modelling effects in time. Given data $(t_i,d_i),i=1,\ldots,n$, we may set up a so-called partial likelihood.  For all observations $i=1,\ldots,n$ with $d_i=1$, we know that there is an event at time $t_i$. The probability of the event happening for some individual $j$ is the hazard, i.e., the instantaneous probability of that individual at that time, divided by the sum of all such hazards for those individuals still alive. Assuming that observations are independent and identically distributed, the partial likelihood for the data is then the product of all such ratios,
\begin{equation*}
    \pl(\bbeta)=\prod_{d_i=1}\frac{\Pr(\text{event happens to }i\text{ at time }t_i)}{\sum_{j\in R(t_i)}\Pr(\text{event happens to }j\text{ at time }t_i)}=\prod_{d_i=1}\frac{\hz_0(t_i)r(\x_i)}{\sum_{j\in R(t_i)} \hz_0(t_i)r(\x_j)},
\end{equation*}
where we see that the baseline hazard will cancel out, and we are left with just the relative risk functions.

The most common choice, by far, for $r(\x)$ is
\begin{equation*}
    r(\x)=\exp(\x^T\bbeta),
\end{equation*}
which leads to the famous Cox model \citep{cox}. The Cox model is an attractive model because the effect of a unit increase in an element of $\bbeta$ has a nice interpretation. Assume we have two covariates $\x_1$ and $\x_2$, and that $\x_2$ is equal to $\x_1$ except for in element $j$, where $x_{2j}=x_{1j}+1$. Then the ratio of the two hazard rates becomes
\begin{equation*}
    \frac{\exp(\x_2^T\bbeta)}{\exp(\x_1^T\bbeta)}=\exp((\x_2-\x_1)^T\bbeta)=\exp(\beta_j).
\end{equation*}
Cox regression is used very much in applied research.

\todo[inline]{add Cox regression example}

%\subsection{Cox regression example}
%Lorem ipsum.

\subsection{Issues with the proportional hazards assumption}
Assuming \eqref{PH}, i.e. $\hz(t|\x)=\hz_0(t)r(x|\bbeta)$, we are making a relatively strong assumption, i.e. that the ratio between the hazard function of two individuals is the same \textit{at all times}. This assumption goes under the name of the proportional hazards (PH) assumption. While, as we saw, this assumption greatly simplifies the inference, it is not necessarily satisfied in practice, or it is in any case very difficult to verify. There exist alternative models, which do not assume the PH assumption. One of these is Aalen's additive model, which is an example of additive hazard modelling. In Aalen's model, the hazard function takes the form
\begin{equation}
    \alpha(t|\x)=\beta_0(t)+\beta_1(t)x_1(t)+\beta_2(t)x_2(t)+\ldots+\beta_p(t)x_p(t),
\end{equation}
where $\beta_j(t),j=1,\ldots,p$ is the increase in the hazard at time $t$ corresponding to a unit's increase in the $j$-th covariate. Another alternative model to the Cox model is the first hitting threshold model. In this thesis we will focus on this model.

\subsection{Robustness of Cox when the PH assumption is violated}
Although the PH assumption is often not valid, in practice, Cox regression tends to work well. \todo[inline]{Need to find a citation here.}

\section{First hitting time models or threshold regression}\label{sec:FHT}
Blabla.

\subsection{General idea}
So far we have not thought much about how a time-to-event is generated. Instead, we have modelled the hazard rate directly. We have simply said that we have stochastic lifetimes. At one time, an individual is alive, and at a slightly later time, it is perhaps dead. One way to think about how these times are generated is to imagine that each individual has an underlying stochastic process, a health process $Y(t)$, say. Since the process is a function of time, it has a non-negative domain, $t\geq0$. This health process is not observable, but when it hits a certain boundary set $\mathcal{B}$, the individual dies. $\mathcal{B}$ is also called a barrier or a threshold, depending on what kind of set it is, and what association one wishes to envoke. The lifetime $T$, then, becomes the time taken by the health process $Y(t)$ to enter the boundary set $\mathcal{B}$. In general, the health process $Y(t)$ takes values in a set $\mathcal{Y}$, with an initial value $y_0=Y(0)$. The barrier is a subset of this set of values, $\mathcal{B}\in\mathcal{Y}$, with the initial health process value $y_0\notin\mathcal{B}$. In other words, the lifetime is
\begin{equation}\label{eq:fht-t}
    T=\min_t\left(t\colon Y(t)\in\setB\right)
\end{equation}
Models based on this view are called first hitting threshold models (FHT). FHT models were introduced in \citet{whitmore1986}, see \citet{leewhitmore2006} for a complete overview. Note that these authors use the term threshold regression. We have, following \citet{caroni2017}, chosen to not use this term, as it is already referring to a well established, and quite different, topic in econometrics. FHT models have been applied to many different fields, including medicine, engineering, and economics, and used, for example to describe the survival time of a transplant patient\todo[inline]{citation needed}, the duration time of a strike\todo{citation needed}, the failure time of an engineering system\todo[inline]{citation}, and so on.

Assess lung cancer risk in railroad workers \citep{leewhitmore2004}.

Length of stay as a stochastic process: A general approach and application to hospitalization for schizophrenia \citep{eaton-whitmore}.

Model for the duration of a strike \citep{lancaster}.

Hospital stay \citep{whitmore1975}.

Labour turnover \citep{whitmore1979}.

Degradation \citep{whitmore1995}.

\subsection{Choice of the health process}
The first hitting time model framework is highly flexible. We have flexibility both in choice of process, boundary and initial value. The most important part is the stochastic process. Examples include Wiener processes, Markov chains, Bernoulli processes, and Gamma processes. We choose to use the Wiener process, because it has a simple intuition behind it, and it yields a fully parametric regression model.

\subsection{Wiener process}
Let $W(t)$ be a continuous stochastic process defined for $t\in[0,\infty)$, taking values in $\R$, and with initial value $W(0)=0$. If $W$ has increments that are independent and normally distributed with
\begin{equation*}
    \E[W(s+t)-W(t)]=0\text{   and   }\Var[W(s+t)-W(t)]=s,
\end{equation*}
we call $W$ a Wiener process. In other words, each increment has expectation 0 and has standard deviation proportional to the length of the time interval. The position of the process at time $t$ always follows a Gaussian distribution $N(0, t)$ \citep{ABG}. To increase the flexibility of the Wiener process, we can introduce a new process $Y$,
\begin{equation}\label{wiener}
    Y(t)=y_0-\mu t+\sigma W(t),
\end{equation}
which is called a Wiener process with initial value $y_0$, drift coefficient $\mu$, and diffusion coefficient $\sigma$. A good introduction to Wiener processes can be found in \citet{cox1965}. Figure \ref{plot:wiener} shows examples of 5 Wiener process paths with initial value $y_0=10$ and negative drift $\mu=1$.
\begin{figure}[H]
\label{plot:wiener}
\caption{Example of 5 Wiener process paths with initial value $y_0=10$ and negative drift $\mu=1$.}
\centering
\includegraphics[scale=0.4]{figures/wiener_processes.pdf}
\end{figure}
\noindent{}Clearly, for a Wiener process starting in $y_0>0$, with a negative drift, i.e. $\mu>0$, the movement is markedly in the direction of zero. If $\sigma^2$ is small in comparison to the drift, the process will move in almost a straight line, such that $X(t)\approx y_0-\mu t$. The hitting time will then be nearly a deterministic function of $y_0$ and $\mu$, $T\approx y_0/\mu$. For a larger relative $\sigma^2$, the diffusion part is more dominant and the hitting time thus less predictable \citep{ABG}.

This is a very conceptually appealing model, because it assumes that individuals might have different initial levels, and that also the drift might be different between individuals. It is also attractive because it has closed-form probability and cumulative density functions, and its likelihood is computationally simple. There are no restrictions on the movements of the process, meaning, it is non-monotonic. If we consider the health process to be analogous with a person's health, this makes sense. A person's health, although generally decreasing over longer periods of time, will fluctuate, at times going up, and at times going down. If we do, however, want a monotonic restriction on the movement of the health process, we may use a gamma process \citep{leewhitmore2006}. This might make sense if the health process is meant to model e.g. the breakdown of a structure.


\subsection{FHT with Wiener process and Inverse Gaussian lifetimes}
If we choose the stochastic process to be a Wiener process like in \eqref{wiener}, and we let the boundary be the non-positive numbers, $\setB=(-\infty,0]$, then \eqref{eq:fht-t} becomes
\begin{equation}
    T=\min_t\left(t\colon Y(t)\in\setB\right),
\end{equation}
i.e. the the lifetime is the time it takes for the process to first reach a non-positive value.
Note that since the Wiener process is continuous, there will not be a difference between $\leq$ and $<$. It can be shown that the first hitting time of the Wiener process follows an inverse Gaussian distribution \citep{chhikara1988}, with probability distribution function
\begin{equation}
\label{eq:ig-pdf}
    f(t|y_0,\mu,\sigma^2)=\frac{y_0}{\sqrt{2\pi\sigma^2t^3}}\exp\left[-\frac{(y_0+\mu t)^2}{2\sigma^2t}\right],
\end{equation}
and cumulative distribution function
\begin{equation}
\label{eq:ig-cdf}
    F(t|\mu,\sigma^2,y_0)=\Phi\sqb*{-\frac{\mu t+y_0}{\sqrt{\sigma^2t}}}+\exp\p*{-\frac{2y_0\mu}{\sigma^2}}\Phi\sqb*{\frac{\mu t-y_0}{\sqrt{\sigma^2t}}}.
\end{equation}
%See Appendix \todo{TO DO!} for the mathematical derivation.
Note that if the drift $\mu$ is positive, then it is not certain that the process will ever reach 0. Hence the probability distribution function in \eqref{eq:ig-pdf} is improper. In this case, the probability of the time not being finite is
\begin{equation*}
    \Pr{}(T=\infty)=1-\Pr{}(T<\infty)=1-\exp{(-2y_0\mu)},
\end{equation*}
see \citet{cox1965}. Since we in survival analysis prefer working with the survival function $S(t)=1-F(t)$ rather than the cdf $F(t)$, we note that $S(t)$ becomes
\begin{equation}
\label{eq:ig-surv}
    S(t|\mu,\sigma^2,y_0)=\Phi\sqb*{\frac{\mu t+y_0}{\sqrt{\sigma^2t}}}-\exp\p*{-\frac{2y_0\mu}{\sigma^2}}\Phi\sqb*{\frac{\mu t-y_0}{\sqrt{\sigma^2t}}},
\end{equation}
where $\Phi(x)$ is the cumulative distribution function of the standard normal, i.e.,
\begin{equation}
    \Phi(x)=\int_{-\infty}^x\frac{\exp\left(-y^2/2\right)}{\sqrt{2\pi}}\dy,
\end{equation}
and in \eqref{eq:ig-surv} we used the fact that $1-\Phi(-x)=\Phi(x)$, since the standard normal distribution is symmetric around 0.

\subsection{The inverse gaussian is overdetermined if the health process is latent}
There are three parameters in the inverse Gaussian distribution, namely $y_0, \mu$ and $\sigma$. We observe, however, that both the pdf $f(t|y_0,\mu,\sigma^2)$ in \eqref{eq:ig-pdf} and the survival function $S(t|\mu,\sigma^2,y_0)$ in \eqref{eq:ig-surv} only depend on these parameters through $\mu/\sigma$ and $y_0/\sigma$. Hence, there are only two free parameters. In other words, we can without loss of generality fix one parameter, for instance set $\sigma$ equal to 1. This is the conventional way to proceed \citep{leewhitmore2006}. We will use this to make inference.

\subsection{The shape of the hazard rate}
The hazard rate is $\hz(t)=f(t)/S(t)$ \eqref{eq:hfs}. Regardless of the initial value, this converges to the same limiting hazard. If $y_0$ is close to zero, we essentially get a decreasing hazard rate. If $y_0$ is far from zero, this gives an essentially increasing hazard rate. If $y_0$ is somewhat inbetween, we get a hazard rate which first increases and then decreases \citep{ABG}.
\todo[inline]{This is unclear}
\todo[inline]{Add plots of hazard rates here. see page 401 in ABG} 

\subsection{Comparison of hazard rates}
It might be of particular interest to look at the ratio between two hazard rates. We might for example look at it when the drift $\mu$ is the same, but the initial level $y_0$ is different. Then the hazard ratio is strongly decreasing. This feature is the same phenomenon as that observed in frailty models, where the relative hazards often decline \citep{ABG}.
\todo[inline]{Explain better}
It is also of interest to do the converse, that is, look at the hazard ratio when the initial level is the same, but the drift is different. The result here is quite different. The ratio of the hazards has a ``bathtub'' shape, which levels off at a later time \citep{ABG}. Keep in mind here that levelling off means getting to proportional hazards. The hazard function converges to
\begin{equation}
    \lim_{t\to\infty}\hz(t)=\frac{1}{2}\left(\frac{\mu}{\sigma}\right)^2=0.5\mu^2
\end{equation}
We see that the FHT framework with a Wiener process is a highly flexible parametric model for survival analysis. Indeed, much more flexible than Cox regression, since the hazard ratios in Cox are all confined to be constant over time.
\todo[inline]{Add plot here as well; also here see ABG page 402}

\subsection{Regression}
We may introduce effects from covariates by allowing $\mu$ and $y_0$ to depend on covariates $\x$ and $\z$. A simple and much used model \todo{citation needed}is to simply use the identity link function for the drift $\mu$, and to use the logarithm link function for the initial level $y_0$, since it must be positive in our framework,
\begin{equation}\label{eq:y0}
    \mu(\bbeta)=\bbeta^\T\x=\sum_{j=1}^p \beta_jx_j,
\end{equation}
\begin{equation}\label{eq:mu}
    y_0(\bgamma)=\exp(\bgamma^\T\z)\Rightarrow\ln y_0(\bgamma)=\bgamma^\T\z=\sum_{j=1}^d \gamma_jz_j.
\end{equation}
Here $\bbeta\in\R^p$ and $\bgamma\in\R^d$ are vectors of regression coefficients. Note that we may let $\x$ and $\z$ share none, some, or all elements. We will discuss consequences of this later.

Inserting the pdf \eqref{eq:ig-pdf} and the survival function \eqref{eq:ig-surv} into the log-likelihood \eqref{eq:surv-lik}, we get that the log-likelihood of a survival data set with the inverse gaussian FHT model, i.e.,
\begin{align}\label{eq:loglik}
\begin{split}
    l(y_0,\mu,\sigma)=\sum_{i=1}^n&\di\p*{\ln y_0-\frac{1}{2}\ln\p*{2\pi\sigma^2\ti^3}-\frac{\p*{y_0+\mu\ti}^2}{2\sigma^2\ti}} \\
    &+
    (1-\di)\ln\p*{\Phi\p*{\frac{\mu\ti+y_0}{\sqrt{\sigma^2\ti}}}-\exp\p*{-\frac{2y_0\mu}{\sigma^2}}\Phi\p*{\frac{\mu\ti-y_0}{\sqrt{\sigma^2\ti}}}}.
\end{split}
\end{align}

\subsection{Fitting an IG FHT model}
At the moment, the standard for fitting an inverse gaussian FHT model to survival data is to use numerical likelihood maximization \citep{caroni2017}. A few software packages exist for doing this, and one of these for \verb|R| \citep{Rlang} is the \verb|threg| package \citep{threg}. There does not exist any method to fit a \textit{regularized} model at the moment, nor to do automatic variable selection. \textbf{This is the reason for my thesis.}

\todo[inline]{Add regression example}

%\subsection{Example of application}
%Lorem ipsum some example. Just use numerical maximization.

%\subsection{Identification problems}