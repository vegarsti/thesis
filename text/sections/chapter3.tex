\chapter{Statistical boosting}
\subsection{Boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades. (\cite{mayr14a}) The history of boosting started with the question posed in 1989 by Kearns and Valiant, working on computational learning theory, of whether any weak learner could be transformed to become also a strong learner. (\cite{kearnsvaliant}) A weak classifier is in general defined to be one which is only slightly better than random choice. For regression, it is a bit harder to give a specific definition, but a weak regressor is simple and low dimensional, and does not pick up much of the underlying signal. The answer to the original question is yes, and Schapire and Freund showed this with the AdaBoost algorithm, which constructs a binary classifier. (\cite{adaboost}) The algorithm works by iteratively reweighting observations, giving more weight to misclassified observations, and training a new base learner on all observations, using the updated weights. The resulting AdaBoost classifier is a linear combination of these base classifiers. In its original formulation, the classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm.

\section{Statistical boosting}\label{sec:sboost}
In statistics, however, we are interested in models which are interpretable. We want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation*}
    \E(Y|X=x)=f(x).
\end{equation*}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings. We therefore extend our discussion to the more general regression scheme, where the outcome variable $Y$ can be continuous. To evaluate a candidate $\fh(x)$, we need to see how well it estimates $f(x)$. This is typically done by choosing a loss function,
\begin{equation}\label{eq:loss}
    \loss(Y, f(X)),
\end{equation}
and calculating the empirical risk, i.e., the average in-sample error over some observed test data set. A typical loss function for regression is the $L_2$ loss,
\begin{equation*}
    \loss(Y, f(X))=(Y-f(X))^2
\end{equation*}
The empirical risk is then
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n (y_i-\fh(x_i))^2
\end{equation*}
A possible model for $f(x)$ is the generalized additive model (GAM), in which different effects of single predictors are added,
\begin{equation}\label{eq:gam}
    f(x)=\beta_0+\sum_{i=1}^p h_p(x_p).
\end{equation}
In 2000, Friedman showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function. (\cite{friedman2000}) This provided a way of viewing the successful boosting regime through a statistical lens.

\subsection{Gradient boosting}
Gradient boosting, proposed in 2001, is a boosting scheme which fits a GAM \eqref{eq:gam}. (\cite{friedman2001}) In general, we are interested in finding the function $f(\cdot)$ which minimizes the loss \eqref{eq:loss}. From a numerical optimization perspective, this can be seen as
\begin{equation*}
    \hat{f}(x)=\underset{f}{\argmin}\,{\E_{Y,X}[\loss(Y,f(X))]}.
\end{equation*}
Gradient boosting does a gradient descent search in function space to find this $\fh(\cdot)$. While the original AdaBoost algorithm iteratively reweights observations, gradient boosting iteratively fits the base-learner to the negative gradient vector $\u^{[m]}$ of the loss function, evaluated at the previous iteration,
\begin{equation*}
    \u^{[m]}=\p*{-\frac{\pd}{\pd f}\loss(Y,f)\at_{f=\fh(\cdot)^{[m-1]}}}.
\end{equation*}
This is the other key point of gradient boosting. Further, often the base learners are one dimensional. Hence,
\begin{equation*}
    \u^{[m]}=\p*{u_1^{[m]},\cdots,u_p^{[m]}},
\end{equation*}
where each component is
\begin{equation*}
    u_j^{[m]}=-\p*{\frac{\pd}{\pd f_j}\loss(Y,f)\at_{f=\fh(\cdot)^{[m-1]}}}
\end{equation*}
Call $k$ the component which is most negative, i.e.,
\begin{equation*}
    k=\argmin{\u^{[m]}},
\end{equation*}
such that $u_k^{[m]}$ is the component in which the gradient of $f$ is steepest. We can then take a gradient descent step in this direction. This gives rise to the component-wise gradient boosting algorithm.

\subsection{Likelihood-based boosting}
To do!


\subsection*{Gradient boosting (re-do)}
Gradient boosting was proposed in 2001 (\cite{friedman2001}), and further refined by \cite{buhlmann-yu} in 2003. We will here present the gradient boosting framework, first a general algorithm.
Assume we have data $\X\in\R^p$ and $Y\in\R$ with some relation $Y\sim f(\X)$, $f\colon\R^p\to\R$, which we wish to estimate. We have
\begin{equation*}
    Y=f(\X)+\epsilon,
\end{equation*}
where $\epsilon$ is a random variable with expectation zero. We wish to minimize the expected loss of our distribution,
\begin{equation}\label{eq:min-loss}
    \min\E_{Y,\X}[\loss(Y,f(\X))],
\end{equation}
where $\loss$ is some meaningful loss function which measures the difference between $Y$ and $f(\X)$. In particular, a loss function is 0 if $Y$ is exactly equal $f(\X)$, and positive otherwise. To estimate $f$ we typically choose a parameterized model,
\begin{equation}
    f(\X)=\bgamma^Th(\X),
\end{equation}
where $h(\cdot)$ is some function, and $\bgamma$ are parameters to be estimated. For finite data points $\{\X_i,Y_i\}_{i=1}^N$ and chosen $h$, there exists $\beta_m$ which minimizes \eqref{eq:min-loss},
\begin{equation}\label{eq:min-loss-param}
    \bgamma^*=\underset{\bgamma}{\min}\E_{Y,\X}[\loss(Y,\bgamma^Th(\X))],
\end{equation}
but estimating this is not necessarily easy. An algorithm which is often used to find the solution $\bgamma^*$ to \ref{eq:min-loss-param} is steepest gradient descent. This is a greedy iterative algorithm, which at each step improves the previous solution by going in the direction of the gradient.

Let $\bgamma=\sum_{m=0}^M\bbeta_m$. We start with an initial guess $\bbeta_0$, e.g. $\bbeta_0=0$. We then carry out steps $m=1,\dotsc,M$, where we find increments $\bbeta_m$ which improve our existing solution $\bgamma_{m-1}$. We continue until some stopping criterion. At each iteration step in the algorithm, we compute the gradient of the loss with respect to the parameters, evaluated at the current solution,
\begin{equation}
    \mathbf{g}_m=\{g_{jm}\}=\left\{\frac{\partial}{\partial\beta_j}\E_{Y,\X}[\loss(Y,\bgamma_{m}^Th(\X))]\right\}_{j=1}^{p}.
\end{equation}
We then do a so-called line search to find the optimal step length,
\begin{equation}
    \rho_m=\underset{\rho}{\argmin}\E_{Y,X}[\loss(Y,(\bgamma_{m-1}+\rho \g_m)^Th(\X))],
\end{equation}
and choose $\beta_m=\rho_m\g_m$.

\subsubsection*{Gradient descent in function space}
Instead of optimizing a parametric function in parameter space, we can also view the optimization problem \eqref{eq:min-loss} from a non-parametric perspective. We are then optimizing in functional space, i.e., finding
\begin{equation}\label{eq:min-loss-func}
    f^*=\underset{f}{\min}\E_{Y,X}[\loss(Y,f(\X)].
\end{equation}
In function space there are in theory an infinite number of such $f$. But in data sets there are only a finite number. We can also here use steepest gradient descent. Following the numerical optimization paradigm as above, we take the solution $f^*$ to \eqref{eq:min-loss-func} to be
\begin{equation}
    f^*=\sum_{m=0}^Mh.
\end{equation}

\subsubsection{Steepest descent}
Steepest descent is a greedy numerical optimization algorithm. At each iteration step, we go in the direction of the gradient, i.e. where the function increases the most. To avoid overshooting, we perform a line search in that direction, finding the best step length. We then continue iterating until we reach convergence.