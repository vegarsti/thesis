\chapter{Statistical boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}. It has become a staple part of the statistical learning toolbox because it is a flexible tool for estimating interpretable statistical models. Boosting, however, originated as a black box algorithm in the fields of computational learning theory and machine learning, not in statistics.

Computer scientists Michael Kearns and Leslie Valiant, who were working on computational learning theory, posed the following question: Could any weak learner be transformed to become a strong learner? \citep{kearnsvaliant} A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. In the binary classification setting, then, it would only perform slightly better than a coin flip. For regression, a weak learner is for example a linear least squares model of only one variable, and having only a small parameter effect for that variable. Meanwhile, a strong learner should be able to perform in a near-perfect fashion, for example attaining 99\% accuracy on a prediction task. I will first attend to give a summary of the history of boosting, starting with AdaBoost \citep{adaboost}, which proved that the answer to the original question above was yes. For another overview, consult also the literature review article by \citet{mayr14a}.

\section{AdaBoost}
The original AdaBoost, also called Discrete AdaBoost \citep{adaboost} is an iterative algorithm for constructing a binary classifier $F(\cdot)$. It was the first \textit{adaptive} boosting algorithm, as it automatically adjusted its parameters to the data based on its perfomance. In the binary classification problem, we are given a set of observations $(\x_i,y_i)_{i=1,\ldots,n}$, where $x\in\R^p$ and $y\in\{-1,1\}$, i.e., positive or negative; yes or no. We want to find a rule which best separates these observations into the correct buckets, as well as being able to classify new, unseen observations $\x_{\text{new}}$ of the same form. Some observations are hard to classify, whereas some are not. One way to look at binary classification is to imagine the $p$-dimensional space of the observations $\x$, and think of the classifier as finding the line which best splits the observations into their corresponding label. Some observations are not at all close to the boundary, and so they are easily classified. Other observations, however, are close to the boundary. \citet{adaboost} realized that one could assign a weight to each observation. First, assign equal weight to each observation. Then, use a weak learner $h(\cdot)$ to make an initial classifier, minimizing the weighted sum of misclassified points, which initially is a plain sum of the observations. After this initial classification, some points will be correctly classified, and some will be misclassified. What we now do is we change the weights of the observations. We increase the weights of the misclassified ones, and normalize the weights afterwards. This then also results in the correctly classified ones having a reduced weight. Finally, based on the misclassification rate of this classifier, calculate a weight $\alpha$ to give to this classifier. Currently, the classifier is $F_1(\cdot)=\alpha_1h_1(\cdot).$ In the next iteration, make a new weak learner which minimizes the weighted sum of the observations and reweight observations accordingly as before. Again calculate a weight to give to this new classifier, and add it to the previous classifier, such that $F_2(\cdot)=\alpha_1h_1(\cdot)+\alpha_2h_2(\cdot)$. Continue iterating in this fashion until an iteration $m$. The resulting final classifier, the AdaBoost classifier, becomes $\hat{F}(\cdot)=F_m(\cdot)=\sum_{i=1}^m\alpha_ih_i(\cdot)$. It is a linear combination of the weak classifiers, and in essence a weighted majority vote of weak learners given the observations.

The AdaBoost algorithm often carries out highly accurate prediction. In practice, it is often used with stumps: Decision trees with one split. This was especially true in the early years of boosting. For example, \citet{bauer-kohavi} report an average 27\% relative improvement in the misclassification error for AdaBoost using stump trees, compared to the error attained with a single decision tree. They conclude that boosting not only reduces the variance in the prediction error from using different training data sets, but that it also is able to reduce the average difference between the predicted and the true class, i.e., the bias. \citet{breiman1998} supports this analysis. Because of its plug-and-play nature and the fact that it never seemed to overfit (overfitting occurs when the learned classifier degrades in test error because of being too specialized on its training set), Breiman remarked that ``boosting is the best off-the-shelf classifier in the world'' \citep{ESL}.

Overfitting occurs when the out-of-sample error starts to increase. At this point, the model is starting to be too sensitive to the structure of the specific data set it is estimated on. One way of thinking about it is that it is starting to fit to the error terms. Since what we actually care about is the performance on a test set, we want to stop just before the model starts overfitting.

In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, however, we are interested in models which are interpretable.

%See some figure for a schematic overview of the algorithm.

\section{Statistical model fitting}
While originally developed for binary classification, boosting is now used to estimate the unknown quantities in more general statistical models and settings. We therefore extend our discussion to a more general regression scheme. Let $D=\{x^{(i)},y^{(i)}\}_{i=1,\ldots,n}$ be a learning data set. We assume that the samples $i=1,\ldots,n$ are sampled independently from an identical distribution over the joint space $\setX\times\setY$. The input space is a possibly high-dimensional $\setX\in\R^p$ and the output space is a low-dimensional space $\setY$. For the majority of applications, the output space $\setY$ is one-dimensional and continuous, e.g., in the standard regression setting. (In the censored survival data setting, however, it is in a sense two-dimensional, since we have $y^{(i)}=(t_i,d_i)$. But not really, since we are only outputting the time.)

We assume there exists some structure in the data to be found. We choose a model $f(\cdot)$ to model that structure. We can then produce predicted values $\hat{y}^{(i)}=f(x^{(i)})$. To calculate the difference between the predicted outcome and the actual outcome, we need a loss function $\rho$. It measures the difference, or distance, between the true outcome $y^{(i)}$ and the predicted outcome $\hat{y}^{(i)}$. A loss function must be symmetric and convex. Examples of $\rho$ are the absolute loss $|y-\eta(x)|$, which leads to a regression model for the median, and the quadratic loss (the $L_2$ loss), which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Keep in mind that the negative is used, due to the aim of \textit{minimizing} the loss function. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.

The goal of the model fitting scenario is to estimate a function which minimizes the loss over an unseen ``hold-out'' sample, often called the out-of-sample error, the generalization error, or the test error. (Or, in a statistical setting, one might use more traditional model selection criteria.) For a specific data set, we can calculate the empirical risk $R$, which is the sum of the loss function evaluated on all samples in the learning data set $D$,
\begin{equation}\label{eq:empirical-risk-2}
    R(D)=\sum_{i=1}^n\rho(y^{(i)},x^{(i)}).
\end{equation}
Other names for $R$ are in-sample error and training error. Since $D$ arises from a data distribution, $R(D)$ is a realization of a more general loss value. We wish to learn about the general structure of $D$, and as such are we most interested in the expected loss, also known as the generalization or test error,
\begin{equation*}
    \text{Err}_{D}=\E[\rho(Y,f(\X))|D],
\end{equation*}
where $(X,Y)$ is drawn randomly from their joint distribution and the training set $D$ is held fixed. It is infeasible to do effectively in practice
and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_D]=\E_D\left[\rho(Y,f(\X))|D\right],
\end{equation}
i.e., average over many different test sets. As mentioned, in practice we observe a sample data set $D$. For this sample, we can calculate the training error -- the empirical risk. To estimate $\text{Err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. Call this data set for $D_{\text{test}}$ We then estimate our model and its parameters based on the other 80\%, and make an estimate of the generalization error $\text{Err}$ by seeing how our estimated model performs on the hold-out test set. We call the resulting error
\begin{equation*}
    \widehat{\text{Err}}_{\text{test}}=\frac{1}{M}\sum_{i=1}^M \rho(y_i,\hat{f}(\x_i)),
\end{equation*}
for test error. If the observed sample is not large enough, one can calculate a $K$-fold cross-validated test error. In this case, one divides the data set into $K$ parts (folds), and for each fold, one lets it be the hold-out data set, and estimate a model using only the other $K-1$ folds. In this way, one gets $K$ test errors, and so the cross-validated test error is the mean of these. See later for a more detailed description.

\section{Gradient boosting}
\citet{friedman2001} developed an algorithm for fitting an additive model called gradient boosting. He showed that AdaBoost performs this algorithm, for a particular exponential loss function. See \citet{ESL} for a good demonstration of the argument. This provided a way of viewing boosting through a statistical lens, and connected the successful machine learning approach to the world of statistical modelling. To understand the algorithm, we first need to understand the gradient descent algorithm.

\subsection{Gradient descent}
Suppose you are trying to minimize a differentiable multivariate function $G\colon\R^m\to\R$, where $m\in\N$. Gradient descent is a greedy algorithm for finding the minimum of such a function $G$, and one which is quite simple and surprisingly effective. If all partial derivatives of $G$ at a point $\x=(x_1,x_2,\ldots,x_m)$ exist, then the gradient of $G$ at $\x$ is the vector of all its partial derivatives at $\x$, namely
\begin{equation}
    \nabla G(\x)=\left(\frac{\partial G(\x)}{\partial x_1},\frac{\partial G(\x)}{\partial x_2},\ldots,\frac{\partial G(\x)}{\partial x_m}\right).
\end{equation}
The motivation behind the gradient descent algorithm is that in a small interval around a point $\x_0\in\R^m$, $G$ is decreasing the most in the direction of the negative gradient at that point. Therefore, by taking a small step slightly in the direction of the negative gradient, from $\x_0$ to a new value $\x_1$, we end up with a slightly lower function value: The new function value $G(\x_1)$ will be less than $G(\x_0)$. In some versions of the algorithm, the step length $\nu\in(0,1]$ is found by a line search, i.e., by finding the step length which gives the best improvement. In other versions, one simply uses a fixed step length. The gradient descent algorithm repeats this procedure until convergence. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit possibly to a local minimum. For a schematic overview of the algorithm, see Algorithm \ref{algo:grad-desc}.
\begin{algorithm}
\caption{Gradient descent}
\label{algo:grad-desc}
We want to minimize $G(x)$, i.e. solve $\min_{x}G(x)$.
\begin{enumerate}
    \item Start with an initial guess $\x_0$, e.g. $\x_0=\0$. Let $m=1$.
    \item Calculate the direction to step in, $\g_{m-1}=-\nabla G(\x_{m-1})$.
    \item Solve the line search to find the best step length $a_m$,
        \begin{equation*}
            a_m=\argmin_{a}\x_{m-1}+a \g_{m-1}.
        \end{equation*}
    \item The step in iteration $m$ becomes $\h_m=a\cdot\g_{m-1}$.
    \item Let $\x_m=\x_{m-1}+\h_{m-1}$.
    \item Increase $m$, and go to step 2. Repeat until $m=M$.
    \item The resulting minumum point is $\x_M=\x_0+\sum_{m=1}^M\h_m(\x_m)$.
\end{enumerate}
\end{algorithm}
The gradient descent algorithm is surprisingly robust. Even though it may converge to a local minimum, it often seems to find good solutions globally. This is likely related to research which has found that in high-dimensional spaces, most minima are not minima, but in fact, saddlepoints masquerading as local minima \citep{saddlepoints}. This means that training will slow since the gradient will be small at this saddlepoint or plateau. When using a gradient descent method typically one sets a threshold at which the algorithm terminates when the gradient becomes smaller than the threshold. However if powering through the saddlepoint, then the multivariate gradient descent search should be able to continue digging downwards from these points.

\subsection{Description of gradient boosting}
Now, consider the problem of finding a model $f(\cdot)$ which minimizes the empirical risk of a chosen loss function \eqref{eq:empirical-risk-2} on a data set $D=\{x_i,y_i\}_{i=1}^N$,
\begin{equation}
    \argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)}).
\end{equation}
Friedman starts off with suggesting one might take a nonparametric approach. In this case, we consider each function value $f(x)$ to be a parameter, and then seek to minimize the empirical risk \eqref{eq:empirical-risk-2}, as above. In function space, there are infinite such parameters, since the space in which $x$ exists is continuous. However, for our realized data set, there are only a finite number of such parameters, since we have $N$ values of $\hat{f}(x_i),i=1,\ldots,N$. We can then use the gradient descent algorithm as inspiration, and we take the solution $f(\cdot)$ to be a sum
\begin{equation}
    f(\cdot)=\sum_{m=0}^M f_m(\cdot),
\end{equation}
where the first $f_0(\cdot)$ is an initial guess, and the remaining $\{f_m(x)\}_{m=1}^M$ are incremental functions -- steps or boosts -- defined by the optimization method.

To use gradient descent on the empirical risk, we need to compute its negative gradient, which we denote $\u$. There are two ways to arrive at that. One is to look calculate partial derivatives of the empirical risk \eqref{eq:empirical-risk-2}, with respect to each estimated function value $\hat{f}_{m-1}(x_i)$:
\begin{align}
    \u&=-\left(\frac{\partial}{\partial f(x_1)}R(f(x_1)),\ldots,\frac{\partial}{\partial f(x_n)}R(f(x_n))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\sum_{i=1}^N\rho(y_i, f(x_i)),\ldots,\frac{\partial}{\partial f(x_n)}\sum_{i=1}^N\rho(y_i, f(x_i))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\rho(y_1, f(x_1)),\ldots,\frac{\partial}{\partial f(x_n)}\rho(y_n, f(x_n))\right)
\end{align}
We see that each element in this vector $\u$ consists of the partial derivative of the loss function, with respect to each sample. However these partial derivatives are equal except for the $y_i$'s and the $\hat{f}(x_i)$'s. In other words, we can simplify the notation as
\begin{equation}
    \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}(x)}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
\end{equation}
This $\u^{[m-1]}$ is a vector of \textit{generalized residuals}. We could also have arrived at it simply by taking the derivative of the loss function $\rho(y,\hat{f}(x))$ with respect to $\hat{f}$, and made the vector by plugging in each sample.

With the generalized residuals in hand, we should now be able to minimize the loss function by performing gradient descent. However, this nonparametric approach of simply reducing the error of each data point will not work, because we are only looking at the observed data points, and not at neighboring points in $\setX$ space. We must therefore impose smoothness to neighboring points. And as statisticians we in any case wish to have an interpretable model. So we choose a model class
\begin{equation}
    h(\cdot;\bbeta)
\end{equation}
which is parameterized by $\bbeta=(\beta_1,\beta_2,\ldots,\beta_p)$. If $h$ is relatively simple, we call it a base learner. A good example of a base learner is a linear least squares model. These base learners are usually relatively simple, regularized (see definition of regularization) parametric effects of $\beta_j$. Typical examples are such as linear least squares, stumps (trees with one split; see \citet{buhlmann2007} and \citet{ESL}), or splines with a few degrees of freedom. When using an additive model, it is reasonable to let the individual $h_m$'s be simple models, because often algorithms exist for very fast computation of estimates. It is not as easy to get a gain through combining many complex learners.

We start with an initial guess $f_0(\cdot)$, a constant, say. Then we iterate, at each step first calculating the generalized residuals from the previous iteration. At a given step in our gradient descent on the empirical risk, we as mentioned wish to minimize the generalized residuals. We constrain our choice of functions to the parameterized function class $h(x;\bbeta)$. The function that we wish to choose, which minimizes the residuals, is the member of that parameterized class that produces the $\hat{h}$ which is \textit{most parallel} to $u$. This is the $h$ that is most correlated with $\u^{[m-1]}$ over the data distribution. This means that this $\hat{h}_m$ is an approximation of the generalized residuals $\u^{[m-1]}$, or, a projection of the generalized residuals onto the space spanned by the base learner function class. We obtain that $\hat{h}_m$ by solving
\begin{equation}
    \bbeta_m=\argmin_{\bbeta} \sum_{i=1}^N (u_i^{[m-1]}-h(x_i;\bbeta))^2,
\end{equation}
i.e., choose the $h$ which minimizes the residual sum of squares (RSS) on the generalized residuals. We obtain $\hat{h}_m(\cdot;\bbeta_m)$, the function to add into our model. The current model is $\hat{f}_{m-1}$. We do a line search to find which the best step length to use,
\begin{equation*}
    a_m=\argmin_{a}R(\hat{f}_{m-1}+a\cdot\hat{h}_m{\cdot;\bbeta}).
\end{equation*}
The final model is then the sum of these terms. We are using gradient descent to find the parameters in each iteration, i.e., to find each $h_m$. In other words, doing gradient descent in parameter space \citep{friedman2001}. So boosting can be viewed as an optimization procedure in functional space. This concludes the outline of a generic functional gradient descent algorithm. For a schematic overview, see Algorithm \ref{algo:fgd}.
\begin{algorithm}
\caption{Functional gradient descent}
\label{algo:fgd}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)}).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Specify a base learner class $h$.
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector,
        \begin{equation}
            \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\x_i,u_i^{[m-1]})$ using the base learner $h$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i^{[m-1]},h(\x_i;\bbeta))
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Find best step length $a_m$ by a line search:
        \begin{equation*}
            a_m=\argmin_{a}R(\hat{f}_{m-1}+a\cdot\hat{h}_m{\cdot;\bbeta}).
        \end{equation*}
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+a_m\cdot h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 8 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
\subsection{Step length}
In the original generic functional gradient descent algorithm, the step length $a_m$ for each iteration is found by a line search.
Friedman says that fitting the data too closely may be counterproductive, and result in overfitting. To combat the overfitting, one constrains the fitting procedure. This constraint is called regularization. Friedman therefore, later in the paper, proposes to regularize each step in the algorithm by a common learning rate, $0<\nu\leq1$. Another natural way to regularize would have been to control the number of terms in the expansion, i.e., number of iterations, $M$. However, it has often been found that regularization through shrinkage provides superior results. (Copas 1983)\todo{find citation?} 

As we will see, most modern boosting algorithms omit the step of the line search to find $a_m$, but instead always uses a learning rate/step length $\nu$.  
The choice of this step length is not of critical importance as long as it is sufficiently small \citep{schmid-hothorn}, i.e., with sufficient shrinkage, but the convention is to use $\text{sl}=0.1$ \citep{mayr14a}. This reduces the complexity of the algorithm, and makes the number of parameters to estimate lower. There will of course be a tradeoff between the number of iterations $M$ and the size of the step length $\nu$, which is another reason to use the conventional step length each time.

\subsection{Number of iterations}
With a fixed step length (learning rate), the main tuning parameter for gradient boosting is the number of iterations $M$ that are performed before the algorithm is stopped. We denote the resulting parameter $\mstop$. If $\mstop$ is too small, the model will underfit and it cannot fully incorporate the influence of the effects on the response and will consequently have poor performance. On the other hand, too many iterations will result in overfitting, leading to poor generalization.

\subsection{Loss functions}
\todo[inline]{I repeat here (word for word) some of what is said in a previous section. Needs to be fixed.}
Gradient boosting requires only that one uses a convex and differentiable loss function. The loss function measures the difference, or distance, between the true outcome $y$ and the predicted outcome. A loss function must be symmetric, since it doesn't matter which is the prediction and which is the truth, and it must be convex, since the loss for a prediction which is further away from another point at least cannot be smaller. Examples of choices for $\rho$ are the absolute loss $|y-\eta(x)|$, which leads to a regression model for the median, and the quadratic loss (the $L_2$ loss), which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Note that we will use the negative log likelihood as the loss function, due to the aim of \textit{minimizing} the loss function, whereas the log likelihood increases as the model fits better to the data. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.

\subsection{Practical considerations}
When boosting, one must (or should) center and scale the matrix $X$.

\section{$L_2$Boost}
With the generic functional gradient boosting algorithm \eqref{algo:fgd}, it is quite straightforward to derive specific algorithms to use for specific models: It is just a matter of plugging in a chosen loss function. This gives great flexibility.

In the original paper \citep{friedman2001}, he derived such an algorithm for the standard regression setting, which he called $L_2$Boost. $L_2$Boost is a computationally simple variant of boosting, constructed from a functional gradient descent algorithm of the $L_2$ loss function,
\begin{equation*}
    \rho(y, \hat{y})=\frac{1}{2}(y-\hat{y})^2.
\end{equation*}
The reason it is simple is that the generalized residual $u_i$ of an observation $y_i,x_i$, i.e., the negative derivative of the loss function with regard to an estimate $\hat{y}_i=\hat{f}(x_i)$, is
\begin{equation*}
    -\frac{\partial}{\partial\hat{y}}\rho(y_i, \hat{y}_i)=y_i-\hat{y}_i,
\end{equation*}
that is, the so-called residual. The negative gradient vector $\u$ then becomes simply the residual vector,
\begin{equation*}
    \frac{\partial\loss(y,F(\x))}{\partial x_i}=(y-F(x_i)),\quad i=1,\dotsc,n,
\end{equation*}
and hence the boosting steps become repeated refitting of residuals \citep{friedman2001,buhlmann-yu}. With $M=2$ iterations, this had in fact been proposed already by \citep{tukey}, who called it ``twicing''. See Algorithm \ref{algo:L2} for an overview of the algorithm. Note that we here use the algorithm given in \citet{buhlmann-yu}, who do not use a step length, i.e., they let $\nu_m=\nu=1$ for all iterations $m=1,\ldots,M$.
\begin{algorithm}
\caption{$L_2$Boost}
\label{algo:L2}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$. Set the loss function $\rho(y,\hat{f}(x))=\frac{1}{2}(y-\hat{f}(x))^2$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n y_i-\hat{f}(x_i).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Let the base learner class $h$ be the least squares model, i.e.,
        \begin{equation}
            h(\x, \bbeta)=\x^T\bbeta=\sum_{j=1}^p\beta_jx_j
        \end{equation}
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector, i.e., the residuals, with the model evaluated at the previous estimate
        \begin{equation}
            \u^{[m-1]}=\left(y_i-\hat{f}(x_i)\right)_{i=1}^N
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\x_i,u_i^{[m-1]})$ using the base learner $h$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i^{[m-1]},h(\x_i;\bbeta))
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 7 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
They also prove some nice important theoretical results for L2Boost.
\todo[inline]{more on L2Boost!!}
\subsection{$L_2$Boost example}
Lorem ipsum.

\section{High dimensions and component-wise gradient boosting}
In some situations, a data set consists of more predictors $p$ than observations $N$. We often call this the $p>N$ setting, or simply a high-dimensional setting. With such data sets, it will be infeasible to use a base learner $h$ which incorporates all $p$ dimensions of the observation matrix $\X$.
For instance in the $L_2$Boost algorithm, if one uses a least squares base learner which uses all $p$ dimensions, we see that it is infeasible: The matrix which must be inverted is singular when the number of predictors $p$ is larger than the number of observations $N$. For other models, it might be possible to estimate parameters for each predictor, but it would very easily result in overfitting. If, for instance, the data set input $\X$ consists of gene expressions, it is obvious that the response variable $y$ is not dependent on every single gene.

Component-wise gradient boosting is an algorithm which does work in these settings. In fact, Buhlmann believes that it is mainly in the case of high-dimensional predictors that boosting has a substantial advantage over classical approaches \citep{buhlmann2006}.
The component-wise approach was first proposed in the L2boost paper by \citet{buhlmann-yu}, and it has further been refined and explored, see e.g. \citet{buhlmann2006}.
In a gradient boosting algorithm, we start out with an empty model $f_0(\cdot)$, which perhaps only uses a constant, or something of the sort. We have not added any predictors yet. Instead of adding a small effect from all predictors, as above, we could try adding only one variable at a time. This is a typical statistical model selection regime, namely forward stepwise model selection. Here one at each step looks at all predictors separately, and tries adding it to the model. One proceeds only with the predictor which gives the best improvement.

The main idea of component-wise gradient boosting is to do exactly this, except in a stagewise manner, for boosting. While stepwise procedures look at all parameters in a model at the same time, a stagewise does not change the added parameters, but only looks at the next one.
The component-wise gradient boosting algorithm takes the generic functional gradient boosting algorithm \eqref{algo:fgd} and instead of using one base learner which incorporates all predictors, one uses a set $\mathcal{H}$ of base learners, where all base learners are univariate. Typically the structure of these base learner is exactly the same, but there is one base learner for each predictor. So if using a linear least squares model, the set of base learners would be
\begin{equation}
    \mathcal{H}=\{h_1(\x;\beta)=\beta x_1,h_2(\x;\beta)=\beta x_2,\ldots,h_p(\x;\beta)=\beta x_p \}.
\end{equation}
One then fits all of these separately to the generalized residuals $\u$, and selects only one learner, that with the best performance, to be added into the boosted model. The resulting model
\begin{equation}
    f(\cdot)=\sum_{m=1}^Mf_m(\cdot),
\end{equation}
where each $f_m$ is a componentwise learner, can then also be seen as a sum of componentwise effects,
\begin{equation*}
    f(\x)=\sum_{j=1}^p p_j(x_j),
\end{equation*}
where $p_j(\cdot)=\sum_{m=1}^M f(x_j)$. For a schematic overview of the algorithm, see Algorithm \ref{algo:component-gradboost}.
\begin{algorithm}
\caption{Component-wise gradient boosting}
\label{algo:component-gradboost}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Specify a set of base learners $\mathcal{H}=\{h_1(\cdot),\dotsc,h_p(\cdot)\}$, where each $h_j$ is univariate.
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector,
        \begin{equation}
            \u=\left(-\frac{\partial}{\partial \hat{f}}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
        \end{equation}
    \item For each base learner $h_j\in\mathcal{H},j=1,\ldots,p$, estimate $\hat{h}_{j,m}$ by fitting $(\X_i,u_i)$ using the base learner $h_j$
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N(u_i,h_j(\x_i;\bbeta))^2
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Select $h(\cdot;\bbeta_m)=\argmin_j\sum_{i=1}^N(u_i,h_j(\x_i;\bbeta))^2$
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 8 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
\subsection{Variable selection}
If the number of iterations $M$ is not very large compared to the number of variables, the component-wise gradient boosting algorithm will carry out automatic variable selection. What this means is that based on their explanatory power, many of the base learners will never be added into the model, and therefore many of the columns of $\X$ will not be a part of the final model. In many cases, a good $M$ is less than the number of predictors, and so even if every iteration selects a different base learner, the final model will have excluded predictors. Realistically, some predictors will be more explanatory than others, and so they will be selected more than once. Some predictors are simply more correlated with the output than others. Therefore some components will lead to better improvements and those corresponding base learners will thus be more frequently selected. Therefore the component-wise boosting algorithm has inherent variable selection.

\section{Selecting $\mstop$}
The crucial tuning parameter in boosting is the number of iterations, $\mstop$. Stopping early enough performs variable selection and shrinks the parameter estimates toward zero. Left on its own, the parameters in boosting will converge towards the maximum likelihood parameters\todo{SOURCE?}, i.e., maximizing the in-sample error. We are, on the other hand, after all interested in minimizing out-of-sample prediction error (PE). The prediction error for a given data set is a function of the boosting iteration $m$. What we want is therefore a good method for approximating $\PE(m)$. This can be done in a number of ways. Many authors state that the algorithm should be stopped early, but do not go further into the details here. Common model selection criteria such as the Akaike Information Criteria (AIC) may be used, however the AIC is dependant on estimates of the model's degrees of freedom. Methods by \citet{chang2010} try this. This is problematic for several reasons. For $\text{L}_2\text{Boost}$, \citet{buhlmann2007} suggest that $\df(m)=\trace(B_m)$ is a good approximation. Here $B_m$ is the hat matrix resulting from the boosting algorithm. This was, however, shown by \citet{hastie2007} to always underestimate the actual degrees of freedom. \citet{mayr-hofner} propose a sequential stopping rule using subsampling etc. We argue instead that cross-validation, a very common method for selection of tuning parameters in statistics, is a good method to use. It is flexible and easy to implement. It is somewhat computationally demanding, requiring several full runs of the boosting algorithm.

\subsection{Other selection methods}
The number of iterations in the boosting procedure, $M$, is a tuning parameter. It acts as a regularizer. AIC, etc.

\subsection{K-fold cross-validation}
K-fold cross-validation \citep{lachenbruch}, or simply cross-validation, is a general method commonly used for selection of penalty or tuning parameters. We will use it to approximate the prediction error. In cross-validation, the data is split randomly into K rougly equally sized folds. For a given fold $k$, all folds except $k$ act as the training data in estimating the model. We often say that the $k$'th fold is left out. The resulting model is then evaluated on the unseen data, namely fold $k$. This procedure is repeated for all $k=1,\ldots,K$. An estimate for the prediction error is obtained by summing over the test errors from evaluting the left-out fold. Let $\kappa(k)$ be the set of indices for fold $k$. The cross-validated estimate for a given $m$ then becomes
\begin{equation}
    \CV(m)=\sum_{k=1}^K\sum_{i\in\kappa(k)}\rho(\cdot,\cdot).
\end{equation}
For each $m$, we calculate the estimate of the cross-validated prediction error $\CV(m)$. We choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\CV(m).
\end{equation}
Typical values for $K$ are 5 or 10, but in theory one can choose any number. The extreme case is $K=N$, called leave one out cross-validation, where all but one observation is used for training and one evaluates the model on the observation that was left out. In this case, the outcome is deterministic, since there is no randomness when dividing into folds.

\subsection{Stratified cross-validation}
When dividing an already small number of survival data observations into $K$ folds, we might risk getting folds without any observed deaths, or in any case, very few. In stratified cross validation, we do not divide the folds entirely at random, but rather, try to divide the data such that there is an equal amount of censored data in each fold.
As before, let $\kappa(k)$ be the set of indices for fold $k$. Divide the observed data into $K$ folds, as with usual cross validation, to get an index set $\kappa_{\delta=1}(k)$ for a given $k$. Similarly, divide the censored data into $K$ folds, obtaining $\kappa_{\delta=0}(k)$. Finally, $\kappa(k)$ is the union of these sets: $\kappa(k)=\kappa_{\delta=1}(k)\cup\kappa_{\delta=0}(k)$. For ``real-life data sets like ours'', \citet{kohavi} illustrate that 10-fold stratified cross validation performs best.

\subsection{Repeated cross-validation}
The randomness inherent in the cross-validation splits has an effect on the resulting $\mstop$. This is true for boosting in general, but it is true for real-life survival data, especially. In typical survival time data sets one typically has a small effective sample size (number of observed events). We can easily imagine that for two different splits of the data, we can end up with quite different values for $\mstop$.
It has been very effectively demonstrated that the split of the folds has a large impact on the choice of $\mstop$ \citep{seibold}. \citet{seibold} suggest simply repeating the cross-validation scheme. They show that repeating even 5 times effectively averages out the randomness.  In other words, we divide the data into $K$ folds, and repeat this $J$ times. Now let $\kappa(j, k)$ be the $k$'th fold in the $j$'th split. We end up with a new estimate for the prediction error,
\begin{equation}
    \RCV(m)=\sum_{j=1}^J\sum_{k=1}^K\sum_{i\in\kappa(j,k)}\rho(\cdot,\cdot).
\end{equation}
As before, we choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\RCV(m).
\end{equation}
In practice, to ensure we find the minimizing $m$, we let the boosting algorithm run for $m=1$ to $m=M$, where $M$ is a large number that we are sure will result in a overfitted model.

\section{Multidimensional boosting: (Component-wise) boosting of a multivariate loss function}
The above methods consider a loss function depending on one parameter: $\rho(\beta)$. In the boosting steps, one uses a gradient descent step with the loss function differentiated with respect to this one parameter. But this means that we are restricted to boosting models which have only one parameter. In many applications, this will not be sufficient, and will not be flexible enough. We want to be able to boost models of more than one parameter, i.e., multidimensional boosting.
%gamboostlss-paper

\subsection{Cyclical multidimensional boosting}
\citet{schmid} extend the component-wise gradient boosting algorithm \citep{friedman2001} to such a setting where one has a multivariate loss function, a loss function with $K$ parameters. Like before, we take the derivative of the loss function to get our generalized residuals. This time, we take the partial derivative of the multivariate loss function with respect to each variable. This results in $K$ sets of generalized residuals, one for each input dimension. %At this point, it is not obvious what to do, as there are several possible choices for a natural extension of the boosting algorithms we have seen so far. We will present several of these choices.
The algorithm proposed in \citet{schmid} is to do boost each variable in each boosting step, i.e.,    in essence replicates the gradient boosting. See ... for a schematic overview. Note that this algorithm resembles the backfitting strategy by Hastie, Tibshirani (1990). In both strategies, components are updated successively by using estimates of the other components as offset values. In backfitting, a completely new estimate of $f^*$ is determined in every iteration, but in gradient boosting, the estimates are only slightly modified in each iteration. After having obtained an update of the parameters, the algorithm cycles through the scale/nuisance parameters, and maximizes these numerically.

The main tuning parameters here are the stopping iterations $\mathbf{m}_{\text{stop}}=m_{\text{stop},1},\ldots,m_{\text{stop},K}$. As in the one-dimensional gradient boosting algorithm, \citet{schmid} say that it should not run until convergence, but rather find estimates by cross-validation. Similarly, the step length should be small, but is not of minor importance.

In a boosting algorithm developed for the GAMLSS family \citep{gamlss}, \citet{gamboostlss-paper} applies the above algorithm, with nuisance/scale parameter set equal to the empty set, i.e., they drop the entire steps X until X.

\begin{algorithm}
\caption{Multidimensional cyclical component-wise gradient boosting}
\label{algo:multi-cyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{(0)}_1,f^{(0)}_2,\ldots,f^{(0)}_K,$, e.g., by setting it to zero for all components, or by finding the best constant. Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1. If $m>m_{\text{stop},k}$, go to step X. Otherwise compute the negative partial derivative
        $-\frac{\partial\rho}{\partial \hat{f}_k}$ and evaluate at $\hat{f}^{(m-1)}(x_i),i=1,\ldots,N$, yielding the
        negative gradient vector
        \begin{equation}
            \u^{(m-1)}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{(m-1)}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{(m-1)}_k$.
    \item Select the component of $X$ which best fits $\u^{(m-1)}_k$ according to a pre-specified goodness-of-fit, typically RSS.
        Set $\hat{\u}^{(m-1)}_k$ equal to the fitted values of the corresponding best model fitted in step X.
    \item Update $\hat{f}_k{[m-1]}\gets\hat{f}_k^{[m-1]}+\nu\hat{U}_k^{[m-1]},$ where $\nu$ is a pre-specified real-valued step-length factor.
    \item Repeat steps 6 until 9 for $k=2,\ldots,K$. 
    \item Finally, update $\hat{f}^{[m]}\gets\hat{f}^{[m-1]}$.
    \item Set $l=0$.
    \item Increase $l$ by 1.
    \item Plug $\hat{f}^{(m)}$ and $\hat{\sigma}_1^{(m-1)},\ldots,\hat{\sigma}_{l-1}^{(m-1)},\hat{\sigma}_{l+1}^{(m-1)},\hat{\sigma}_{L}^{(m-1)}$ into the empirical risk function $R$ and minimize the empirical risk over $\sigma_l$. Set $\hat{\sigma}_l^{(m)}$ equal that minimizer.
    \item Repeat steps 13 and 14 for $l=2,\ldots,L$.
    \item Repeat steps 4 to 15 until $m=max_k(m_{\text{stop},k})$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}

\subsection{Noncyclical boosting algorithm}
Recently, \citet{thomas2018} point out a problem with the above approach: The different $m_{\text{stop},j}$ parameters are not independent of each other, and hence have to be jointly optimized. The usually applied \textit{grid search} for such parameters scales exponentially with the number of parameters $k$, and this can quickly become computationally very demanding. Therefore \citet{thomas2018} develop an alternative solution in which only one tuning parameter is needed. They call the approach in the above algorithm for \textit{cyclical} multidimensional boosting (one cycles through all parameters in each iteration). \citet{thomas2018} develop an algorithm where instead of cyclical boosting, one chooses only one parameter in each boosting iteration.

Usually, base-learners are selected by comparing their residual sum of squares with respect to the negative gradient vector. \citet{thomas2018} call this the \textit{inner loss}. However, in general these are not comparable across parameters of the loss function.\todo{Add small section with empirical proof?} Because this depends on the scale of the parameters. In a multivariate normal distribution, for example, the partial derivatives for each mean are symmetrical. But the partial derivative for the standard deviation will not be comparable. Therefore, to compare between parameters, a different comparison method is needed. \citet{thomas2018} give two methods here, which they call \textit{inner} and \textit{outer} loss. In both cases of this algorithm, we have the advantage that the optimal number of boosting steps, $m_{\text{stop}}$, is always a scalar value. Finding this tuning parameter can be done fairly quickly with standard cross validation schemes, and most importantly, it scales with with the number of parameters. This is unlike the cyclical algorithm, which needs a multidimensional grid search.

\subsubsection{Inner loss}
One solution is to compare the empirical risks after the update with the best-fitting base-learners that have been selected via the residual sum of squares for each distribution parameter. In other words, for each parameter, calculate the estimated base learner for each base learner. Then, choose the base learner which has the best RSS of the generalized residuals. Finally, calculate the empirical risk for the model, given that you choose each of these base learners separately. Compare the additional gain $\Delta\rho$ across the selected base learners. Then, select only that parameter and that base learner which led to the best improvement, and update the model accordingly.

\subsubsection{Outer loss}
Another option is to use the empirical risk for choosing the base learner within the parameter as well. Choosing base-learners and parameters with respect to two different optimization criteria may not always lead to the best possible update. The empirical loss, i.e., the negative log likelihood of the modeled distribution can be used to compare both. The negative gradients are used to estimate all base-learners. The improvement in the empirical risk is then calculated for each base learner of every distribution parameter, and only the overall best-performing base learner with regard to the outer loss is updated.

\todo[inline]{Here I could perhaps write something about the performance of the noncyclical compared with the cyclical.}

\subsection{Degenerate noncyclical boosting algorithm}
We have a loss function for which we have several partial derivatives, one for each component/parameter. One way to extend the component-wise gradient boosting algorithm for one-dimensional loss functions into several dimensions is to estimate all base learners for all loss function components, and simply choose the best component according to the same criterion as for one-dimensional loss function, namely that with best RSS. This is however not a good idea, because the gradients are \textbf{not comparable} \citep{thomas2018}. Therefore, \citet{thomas2018} give the following algorithm.


\begin{algorithm}
\caption{Multidimensional non-cyclical component-wise gradient boosting}
\label{algo:multi-non-cyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{(0)}_1,f^{(0)}_2,\ldots,f^{(0)}_K,$, e.g., by setting it to zero for all components, or by finding the best constant.% Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1.
    \item Compute the negative partial derivative $-\frac{\partial\rho}{\partial \hat{f}_k}$
        and evaluate at $\hat{f}^{(m-1)}(x_i),i=1,\ldots,N$, yielding negative gradient vector
        \begin{equation}
            \u^{(m-1)}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{(m-1)}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{(m-1)}_k$.
    \item Select the best fitting base learner, $h_{kj}$, either by
        \begin{itemize}
            \item the inner loss, i.e., the RSS of the base-learner fit w.r.t the negative gradient vector
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N(u_k^{(i)}-\hat{h}_{kj}(x^{(i)}))^2
                \end{equation}
            \item the outer loss, i.e., the loss function after the potential update,
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj}(x^{(i)}) \right)
                \end{equation}
        \end{itemize}
    \item Compute the possible improvement of this update regarding the outer loss,
        \begin{equation}
            \Delta\rho_k=\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj^*}(x^{(i)}) \right)
        \end{equation}
    \item Update, depending on the value of the loss reduction, $k^*=\argmin_{k\in1,\ldots,K}\Delta\rho_k$
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}+\nu\cdot\hat{h}_{k^*j^*}(x),
        \end{equation}
        while for all $k\neq k^*$,
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}.
        \end{equation}
    \item Repeat steps 4 to 11 until $m=m_{\text{stop}}$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}