\chapter{Statistical boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}. It has become a staple part of the statistical learning toolbox because it is a flexible tool for estimating interpretable statistical models. Boosting, however, originated as a black box algorithm in the fields of computational learning theory and machine learning, not in statistics.

Computer scientists Michael Kearns and Leslie Valiant, who were working on computational learning theory, posed the following question: Could any weak learner be transformed to become a strong learner? \citep{kearnsvaliant} A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. In the binary classification setting, then, it would only perform slightly better than a coin flip. For regression, a weak learner is for example a linear least squares model of only one variable, and having only a small parameter effect for that variable. Meanwhile, a strong learner should be able to perform in a near-perfect fashion, for example attaining high accuracy on a prediction task. I will first attend to give a summary of the history of boosting, starting with AdaBoost \citep{adaboost}, which proved that the answer to the original question above was yes. For a complete overview, see \citet{mayr14a, mayr14b, mayr17}.

\todo[inline]{the section below is new!}

\section{Setting}
Consider a data set containing the values of an outcome variable $\Y$ and predictor variables $\X_1, \X_2,\ldots,\X_p$. Although $\Y$ will be one-dimensional, we explicitly allow for multidimensional outcome variables. The objective is to model the relationship between $\Y$ and $\X=(\X_1,\X_2,\ldots,\X_p)^T$ and to obtain an ``optimal'' prediction of $\Y$ given $\X$. Usually, this is accomplished by optimizing an objective function $\rho(\Y,f)\in\R$ over a prediction function $f$ (depending on $\X$). Linear refression with a continuous outcome variable $\Y\in\R$ is a well known example of this approach: Here, $\rho$ corresponds to the least squares objective function, i.e, the $L_2$ loss function and $f$ is a parametric, linear, function of $\X$.

In order to address the issue of analyzing high dimensional data sets, a variety of regression techniques have been developed over the past years. Many of these techniques are characterized by a built-in mechanism for ``regularization'', which means that shrinkage of coefficient estimates or selection of relevant predictors is carried out simultaneously with the estimation of the model parameters. Both shrinkage and variable selection will typically improve prediction accuracy: In case of shrinkage, coefficient estimates tend to have a slightly increased bias but a decreased variance, while in case of variable selection, overfitting the data is avoided by selecting only the most informative predictors. Note that regularization is not only useful in the high-dimensional data setting, but also tends to improve prediction accuracy in low-dimensional settings where $p\leq N$, where $N$ is the number of observations in the data set, and $p$ the number of predictors, i.e., columns in $\X$.

\section{AdaBoost}
The original AdaBoost, also called Discrete AdaBoost \citep{adaboost} is an iterative algorithm for constructing a binary classifier $F(\cdot)$. It was the first \textit{adaptive} boosting algorithm, as it automatically adjusted its parameters to the data based on its perfomance. In the binary classification problem, we are given a set of observations $(\x_i,y_i)_{i=1,\ldots,n}$, where $x_i\in\R^p$ and $y_i\in\{-1,1\}$, i.e., positive or negative; yes or no. We want to find a rule which best separates these observations into the correct classes $\{-1,1\}$, as well as being able to classify new, unseen observations $\x_{\text{new}}$ of the same form. Some observations are hard to classify, whereas some are not. One way to look at binary classification is to imagine the $p$-dimensional space of the observations $\x$, and think of the classifier as finding the line which best splits the observations into their corresponding label. Some observations are not at all close to the boundary, and so they are easily classified. The problems start when the observations are close to the boundary. \citet{adaboost} realized that one could assign a weight to each observation. First, assign equal weight to each observation. Then, use a weak learner $h(\cdot)$ to make an initial classifier, minimizing the weighted sum of misclassified points. After this initial classification, some points will be correctly classified, and some will be misclassified. We increase the weights of the misclassified ones, and normalize the weights afterwards. This then also results in the correctly classified ones having a reduced weight. Finally, based on the misclassification rate of this classifier, calculate a weight $\alpha$ to give to this classifier. Currently, the classifier is $F_1(\cdot)=\alpha_1h_1(\cdot).$ In the next iteration, aaply again a weak learner which minimizes the weighted sum of the observations and reweight observations accordingly as before. Again, calculate a weight to give to this new classifier, and add it to the previous classifier, such that $F_2(\cdot)=\alpha_1h_1(\cdot)+\alpha_2h_2(\cdot)$. Continue iterating in this fashion until an iteration $m$. The resulting final classifier, the AdaBoost classifier, becomes $\hat{F}(\cdot)=F_m(\cdot)=\sum_{i=1}^m\alpha_ih_i(\cdot)$. It is a linear combination of the weak classifiers, and in essence a weighted majority vote of weak learners given the observations.

The AdaBoost algorithm often carries out highly accurate prediction. In practice, it is often used with stumps: Decision trees with one split. For example, \citet{bauer-kohavi} report an average 27\% relative improvement in the misclassification error for AdaBoost using stump trees, compared to the error attained with a single decision tree. They conclude that boosting not only reduces the variance in the prediction error from using different training data sets, but that it also is able to reduce the average difference between the predicted and the true class, i.e., the bias. \citet{breiman1998} supports this analysis. Because of its plug-and-play nature and the fact that it never seemed to overfit (overfitting occurs when the learned classifier degrades in test error because of being too specialized on its training set), Breiman remarked that ``boosting is the best off-the-shelf classifier in the world'' \citep{ESL}.

Overfitting occurs when the out-of-sample error starts to increase. At this point, the model is starting to be too sensitive to the structure of the specific data set it is estimated on. One way of thinking about it is that it is starting to fit to the error terms. Since what we actually care about is the performance on a test set, we want to stop just before the model starts overfitting.

In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, however, we are interested in models which are interpretable.

%See some figure for a schematic overview of the algorithm.

\section{Statistical model fitting}
While originally developed for binary classification, boosting is now used to estimate the unknown quantities in more general statistical models and settings. We therefore extend our discussion to a more general regression scheme. Let $D=\{x^{(i)},y^{(i)}\}_{i=1,\ldots,n}$ be a learning data set. We assume that the samples $i=1,\ldots,n$ are generated independently from an identical distribution over the joint space $\setX\times\setY$. The input space is a possibly high-dimensional $\setX\in\R^p$ and the output space is a low-dimensional space $\setY$. For the majority of applications, the output space $\setY$ is one-dimensional.

We assume there exists some structure in the data to be found. We choose a model $f(\cdot)$ to model that structure. We can then produce predicted values $\hat{y}^{(i)}=f(x^{(i)})$. To calculate the difference between the predicted outcome and the actual outcome, we need a loss function $\rho$. It measures the difference, or distance, between the true outcome $y^{(i)}$ and the predicted outcome $\hat{y}^{(i)}$. A loss function must be symmetric and convex. Examples of $\rho$ are the absolute loss $|y-\eta(x)|$, which leads to a regression model for the median, and the quadratic loss (the $L_2$ loss), which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Keep in mind that the negative is used, due to the aim of \textit{minimizing} the loss function. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.

The goal of the model fitting scenario is to estimate a function which minimizes the loss over an unseen ``hold-out'' sample, often called the out-of-sample error, the generalization error, or the test error. For a specific data set, we can calculate the empirical risk $R$, which is the sum of the loss function evaluated on all samples in the learning data set $D$,
\begin{equation}\label{eq:empirical-risk-2}
    R(D)=\sum_{i=1}^n\rho(y^{(i)},x^{(i)}).
\end{equation}
Other names for $R$ are in-sample error and training error. Since $D$ arises from a data distribution, $R(D)$ is a realization of a more general loss value. We wish to learn about the general structure of $D$, and as such are we most interested in the expected loss, also known as the generalization or test error,
\begin{equation*}
    \text{Err}_{D}=\E[\rho(Y,f(\X))|D],
\end{equation*}
where $(X,Y)$ is drawn randomly from their joint distribution and the training set $D$ is held fixed. It is infeasible to do effectively in practice
and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_D]=\E_D\left[\rho(Y,f(\X))|D\right],
\end{equation}
i.e., average over many different test sets. As mentioned, in practice we observe a sample data set $D$. For this sample, we can calculate the training error -- the empirical risk. To estimate $\text{Err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. Call this data set for $D_{\text{test}}$ We then estimate our model and its parameters based on the other 80\%, and make an estimate of the generalization error $\text{Err}$ by seeing how our estimated model performs on the hold-out test set. We call the resulting error
\begin{equation*}
    \widehat{\text{Err}}_{\text{test}}=\frac{1}{M}\sum_{i=1}^M \rho(y_i,\hat{f}(\x_i)),
\end{equation*}
for test error. If the observed sample is not large enough, one can calculate a $K$-fold cross-validated test error. In this case, one divides the data set into $K$ parts (folds), and for each fold, one lets it be the hold-out data set, and estimate a model using only the other $K-1$ folds. In this way, one gets $K$ test errors, and so the cross-validated test error is the mean of these. See later for a more detailed description.

\section{Gradient boosting}
\citet{friedman2001} developed an algorithm for fitting an additive model called gradient boosting. He showed that AdaBoost performs this algorithm, for a particular exponential loss function. See \citet{ESL} for a good demonstration of the argument. This provided a way of viewing boosting through a statistical lens, and connected the successful machine learning approach to the world of statistical modelling. To understand the algorithm, we first need to understand the gradient descent algorithm.

\subsection{Gradient descent}
Suppose you are trying to minimize a differentiable multivariate function $G\colon\R^m\to\R$, where $m\in\N$. Gradient descent is a greedy algorithm for finding the minimum of such a function $G$, and one which is quite simple and surprisingly effective. If all partial derivatives of $G$ at a point $\x=(x_1,x_2,\ldots,x_m)$ exist, then the gradient of $G$ at $\x$ is the vector of all its partial derivatives at $\x$, namely
\begin{equation}
    \nabla G(\x)=\left(\frac{\partial G(\x)}{\partial x_1},\frac{\partial G(\x)}{\partial x_2},\ldots,\frac{\partial G(\x)}{\partial x_m}\right).
\end{equation}
The motivation behind the gradient descent algorithm is that in a small interval around a point $\x_0\in\R^m$, $G$ is decreasing the most in the direction of the negative gradient at that point. Therefore, by taking a small step slightly in the direction of the negative gradient, from $\x_0$ to a new value $\x_1$, we end up with a slightly lower function value: The new function value $G(\x_1)$ will be less than $G(\x_0)$. In some versions of the algorithm, the step length $\nu\in(0,1]$ is found by a line search, i.e., by finding the step length which gives the best improvement. In other versions, one simply uses a fixed step length. The gradient descent algorithm repeats this procedure until convergence. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit possibly to a local minimum. For a schematic overview of the algorithm, see Algorithm \ref{algo:grad-desc}.
\begin{algorithm}
\caption{Gradient descent}
\label{algo:grad-desc}
We want to minimize $G(x)$, i.e. solve $\min_{x}G(x)$.
\begin{enumerate}
    \item Start with an initial guess $\x_0$, e.g. $\x_0=\0$. Let $m=1$.
    \item Calculate the direction to step in, $\g_{m-1}=-\nabla G(\x_{m-1})$.
    \item Solve the line search to find the best step length $a_m$,
        \begin{equation*}
            a_m=\argmin_{a}\x_{m-1}+a \g_{m-1}.
        \end{equation*}
    \item The step in iteration $m$ becomes $\h_m=a\cdot\g_{m-1}$.
    \item Let $\x_m=\x_{m-1}+\h_{m-1}$.
    \item Increase $m$, and go to step 2. Repeat until $m=M$.
    \item The resulting minumum point is $\x_M=\x_0+\sum_{m=1}^M\h_m(\x_m)$.
\end{enumerate}
\end{algorithm}
The gradient descent algorithm is surprisingly robust. Even though it may converge to a local minimum, it often seems to find good solutions globally. This is likely related to research which has found that in high-dimensional spaces, most minima are not minima, but in fact, saddlepoints masquerading as local minima \citep{saddlepoints}. This means that training will slow since the gradient will be small at this saddlepoint or plateau. When using a gradient descent method typically one sets a threshold at which the algorithm terminates when the gradient becomes smaller than the threshold. However if powering through the saddlepoint, then the multivariate gradient descent search should be able to continue digging downwards from these points.

\subsection{Description of gradient boosting}
Now, consider the problem of finding a model $f(\cdot)$ which minimizes the empirical risk of a chosen loss function \eqref{eq:empirical-risk-2} on a data set $D=\{x_i,y_i\}_{i=1}^N$,
\begin{equation}
    \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)}).
\end{equation}
Friedman starts with suggesting a nonparametric approach. In this case, we consider each function value $f(x)$ to be a parameter, and then seek to minimize the empirical risk \eqref{eq:empirical-risk-2}, as above. In function space, there are infinite such parameters, since the space in which $x$ exists is continuous. However, for our realized data set, there are only a finite number of such parameters, since we have $N$ values of $\hat{f}(x_i),i=1,\ldots,N$. We can then use the gradient descent algorithm as inspiration, and we take the solution $f(\cdot)$ to be a sum
\begin{equation}
    f(\cdot)=\sum_{m=0}^M f_m(\cdot),
\end{equation}
where the first $f_0(\cdot)$ is an initial guess, and the remaining $\{f_m(x)\}_{m=1}^M$ are incremental functions -- steps or boosts -- defined by the optimization method.

To use gradient descent on the empirical risk, we need to compute its negative gradient, which we denote $\u$. There are two ways to arrive at that. One is to calculate the partial derivatives of the empirical risk \eqref{eq:empirical-risk-2}, with respect to each estimated function value $\hat{f}_{m-1}(x_i)$:
\begin{align}
    \u&=-\left(\frac{\partial}{\partial f(x_1)}R(f(x_1)),\ldots,\frac{\partial}{\partial f(x_n)}R(f(x_n))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\sum_{i=1}^N\rho(y_i, f(x_i)),\ldots,\frac{\partial}{\partial f(x_n)}\sum_{i=1}^N\rho(y_i, f(x_i))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\rho(y_1, f(x_1)),\ldots,\frac{\partial}{\partial f(x_n)}\rho(y_n, f(x_n))\right)
\end{align}
We see that each element in this vector $\u$ consists of the partial derivative of the loss function, with respect to each sample. However these partial derivatives are equal except for the $y_i$'s and the $\hat{f}(x_i)$'s. In other words, we can simplify the notation as
\begin{equation}
    \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}(x)}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
\end{equation}
This $\u^{[m-1]}$ is a vector of \textit{generalized residuals}. We could also have arrived at it by simply taking the derivative of the loss function $\rho(y,\hat{f}(x))$ with respect to $\hat{f}$, and made the vector by plugging in each sample.

With the generalized residuals in hand, we should now be able to minimize the loss function by performing gradient descent. However, this nonparametric approach of simply reducing the error of each data point will not work, because we are only looking at the observed data points, and not at neighboring points in $\setX$ space. We must therefore impose smoothness to neighboring points. And as statisticians we in any case wish to have an interpretable model. So we choose a model class
\begin{equation}
    h(\cdot;\bbeta)
\end{equation}
which is parameterized by $\bbeta=(\beta_1,\beta_2,\ldots,\beta_p)$. If $h$ is relatively simple, we call it a base learner. A good example of a base learner is a linear least squares model. These base learners are usually relatively simple, regularized (see definition of regularization) parametric effects of $\beta_j$. Typical examples are such as linear least squares, stumps (trees with one split; see \citet{buhlmann2007} and \citet{ESL}), or splines with a few degrees of freedom. When using an additive model\todo{introduce earlier}, it is reasonable to let the individual $h_m$'s be simple models, because often algorithms exist for very fast computation of estimates. It is not as easy to get a gain through combining many complex learners.

We start with an initial guess $f_0(\cdot)$, a constant, say. Then we iterate, let us say, at each step first calculating the generalized residuals from the previous iteration. At a given step in our gradient descent on the empirical risk, we as mentioned wish to minimize the generalized residuals. We constrain our choice of functions to the parameterized function class $h(x;\bbeta)$. The function that we wish to choose, which minimizes the residuals, is the member of that parameterized class that produces the $\hat{h}$ which is \textit{most parallel} to $u$, i.e. the $h$ that is most correlated with $\u^{[m-1]}$ over the data distribution. This means that this $\hat{h}_m$ is an approximation of the generalized residuals $\u^{[m-1]}$, or, a projection of the generalized residuals onto the space spanned by the base learner function class. We obtain that $\hat{h}_m$ by solving
\begin{equation}
    \bbeta_m=\argmin_{\bbeta} \sum_{i=1}^N (u_i^{[m-1]}-h(x_i;\bbeta))^2,
\end{equation}
i.e., choose the $h$ which minimizes the residual sum of squares (RSS) on the generalized residuals. We obtain $\hat{h}_m(\cdot;\bbeta_m)$, the function to add into our model. The current model is $\hat{f}_{m-1}$. We do a line search to find which the best step length to use,
\begin{equation*}
    a_m=\argmin_{a}R(\hat{f}_{m-1}+a\cdot\hat{h}_m{\cdot;\bbeta}).
\end{equation*}
The final model is then the sum of these terms. We are using gradient descent to find the parameters in each iteration, i.e., to find each $h_m$. In other words, doing gradient descent in parameter space \citep{friedman2001}. So boosting can be viewed as an optimization procedure in functional space. This concludes the outline of a generic functional gradient descent algorithm. For a schematic overview, see Algorithm \ref{algo:fgd}.
\begin{algorithm}
\caption{Functional gradient descent}
\label{algo:fgd}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)}).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Specify a base learner class $h$.
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector,
        \begin{equation}
            \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\x_i,u_i^{[m-1]})$ using the base learner $h$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i^{[m-1]},h(\x_i;\bbeta))
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Find best step length $a_m$ by a line search:
        \begin{equation*}
            a_m=\argmin_{a}R(\hat{f}_{m-1}+a\cdot\hat{h}_m{\cdot;\bbeta}).
        \end{equation*}
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+a_m\cdot h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 8 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
\subsection{Step length}
In the original generic functional gradient descent algorithm, the step length $a_m$ for each iteration is found by a line search.
\citet{friedman2001} says that fitting the data too closely may be counterproductive, and result in overfitting. To combat the overfitting, one constrains the fitting procedure. This constraint is called regularization. Friedman therefore, later in the paper, proposes to regularize each step in the algorithm by a common learning rate, $0<\nu\leq1$. Another natural way to regularize would have been to control the number of terms in the expansion, i.e., number of iterations, $M$. However, it has often been found that regularization through shrinkage provides superior results. (Copas 1983)\todo{find citation?} 

As we will see, most modern boosting algorithms omit the step of the line search to find $a_m$, but instead always uses a learning rate/step length $\nu$.  
The choice of this step length is not of critical importance as long as it is sufficiently small \citep{schmid-hothorn}, i.e., with sufficient shrinkage, but the convention is to use $\nu=0.1$ \citep{mayr14a}. This reduces the complexity of the algorithm, and makes the number of parameters to estimate lower. There will of course be a tradeoff between the number of iterations $M$ and the size of the step length $\nu$, which is another reason to use the conventional step length each time.

\subsection{Number of iterations}\label{subsec:iterations}
With a fixed step length (learning rate), the main tuning parameter for gradient boosting is the number of iterations $M$ that are performed before the algorithm is stopped. If $M$ is too small, the model will underfit and it cannot fully incorporate the influence of the effects on the response and will consequently have poor performance. On the other hand, too many iterations will result in overfitting, leading to poor generalization.

\subsection{Loss functions}
\todo[inline]{I repeat here (word for word) some of what is said in a previous section. Needs to be fixed.}
Gradient boosting requires only that one uses a convex and differentiable loss function. The loss function measures the difference, or distance, between the true outcome $y$ and the predicted outcome. A loss function must be symmetric, since it doesn't matter which is the prediction and which is the truth, and it must be convex, since the loss for a prediction which is further away from another point at least cannot be smaller. Examples of choices for $\rho$ are the absolute loss $|y-\eta(x)|$, which leads to a regression model for the median, and the quadratic loss (the $L_2$ loss), which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Note that we will use the negative log likelihood as the loss function, due to the aim of \textit{minimizing} the loss function, whereas the log likelihood increases as the model fits better to the data. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.

\subsection{Practical considerations}
When boosting, one must (or should) center and scale the matrix $X$.

\section{$L_2$Boost}
With the generic functional gradient boosting algorithm \eqref{algo:fgd}, it is quite straightforward to derive specific algorithms to use for specific models: It is just a matter of plugging in a chosen loss function. This gives great flexibility.

In the original paper \citep{friedman2001}, he derived such an algorithm for the standard regression setting, which he called $L_2$Boost. $L_2$Boost is a computationally simple variant of boosting, constructed from a functional gradient descent algorithm of the $L_2$ loss function,
\begin{equation*}
    \rho(y, \hat{y})=\frac{1}{2}(y-\hat{y})^2.
\end{equation*}
The reason it is simple is that the generalized residual $u_i$ of an observation $y_i,x_i$, i.e., the negative derivative of the loss function with regard to an estimate $\hat{y}_i=\hat{f}(x_i)$, is
\begin{equation*}
    -\frac{\partial}{\partial\hat{y}}\rho(y_i, \hat{y}_i)=y_i-\hat{y}_i,
\end{equation*}
that is, the so-called residual. The negative gradient vector $\u$ then becomes simply the residual vector,
\begin{equation*}
    \frac{\partial\loss(y,f(\x))}{\partial x_i}=(y-f(x_i)),\quad i=1,\dotsc,n,
\end{equation*}
and hence the boosting steps become repeated refitting of residuals \citep{friedman2001,buhlmann-yu}. With $M=2$ iterations, this had in fact been proposed already, under the name of ``twicing'' \citep{tukey}. See Algorithm \ref{algo:L2} for an overview of the algorithm. Note that we here use the algorithm given in \citet{buhlmann-yu}, who do not use a step length, i.e., they let $\nu_m=\nu=1$ for all iterations $m=1,\ldots,M$.
\begin{algorithm}
\caption{$L_2$Boost}
\label{algo:L2}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$. Set the loss function $\rho(y,\hat{f}(x))=\frac{1}{2}(y-\hat{f}(x))^2$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n y_i-\hat{f}(x_i).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Let the base learner class $h$ be the least squares model, i.e.,
        \begin{equation}
            h(\x, \bbeta)=\x^T\bbeta=\sum_{j=1}^p\beta_jx_j
        \end{equation}
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector, i.e., the residuals, with the model evaluated at the previous estimate
        \begin{equation}
            \u^{[m-1]}=\left(y_i-\hat{f}(x_i)\right)_{i=1}^N
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\x_i,u_i^{[m-1]})$ using the base learner $h$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i^{[m-1]},h(\x_i;\bbeta))
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 7 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
They also prove some nice important theoretical results for L2Boost.
\todo[inline]{more on L2Boost!!}
\subsection{$L_2$Boost example}
Lorem ipsum.

\section{High dimensions and component-wise gradient boosting}
In some situations, a data set consists of more predictors $p$ than observations $N$. When $p$ is much larger than $N$ ($p>>N$), we talk about high-dimensional settings. With such data sets, it will be infeasible to use a base learner $h$ which incorporates all $p$ dimensions of the observation matrix $\X$.
For instance in the $L_2$Boost algorithm, if one uses a least squares base learner which uses all $p$ dimensions, we see that it is infeasible: The matrix which must be inverted is singular when the number of predictors $p$ is larger than the number of observations $N$. For other models, it might be possible to estimate parameters for each predictor, but it would very easily result in overfitting. If, for instance, the data set input $\X$ consists of gene expressions, it is obvious that the response variable $y$ is not dependent on every single gene.

Component-wise gradient boosting is an algorithm which does work in these settings. In fact, Buhlmann believes that it is mainly in the case of high-dimensional predictors that boosting has a substantial advantage over classical approaches \citep{buhlmann2006}.
The component-wise approach was first proposed in the L2boost paper by \citet{buhlmann-yu}, and it has further been refined and explored, see e.g. \citet{buhlmann2006}.
In a gradient boosting algorithm, we start out with an empty model $f_0(\cdot)$, which perhaps only uses a constant, or something of the sort. We have not added any predictors yet. Instead of adding a small effect from all predictors, as above, we could try adding only one variable at a time. This is a typical statistical model selection regime, namely forward stepwise model selection. Here one at each step looks at all predictors separately, and tries adding it to the model. One proceeds only with the predictor which gives the best improvement.

The main idea of component-wise gradient boosting is to do exactly this, except in a stagewise manner, for boosting. While stepwise procedures look at all parameters in a model at the same time, a stagewise does not change the added parameters, but only looks at the next one.
The component-wise gradient boosting algorithm takes the generic functional gradient boosting algorithm \eqref{algo:fgd} and instead of using one base learner which incorporates all predictors, one uses a set $\mathcal{H}$ of base learners, where all base learners are univariate. Typically the structure of these base learner is exactly the same, but there is one base learner for each predictor. So if using a linear least squares model, the set of base learners would be
\begin{equation}
    \mathcal{H}=\{h_1(\x;\beta)=\beta x_1,h_2(\x;\beta)=\beta x_2,\ldots,h_p(\x;\beta)=\beta x_p \}.
\end{equation}
One then fits all of these separately to the generalized residuals $\u$, and selects only one learner, that with the best performance, to be added into the boosted model. The resulting model
\begin{equation}
    f(\cdot)=\sum_{m=1}^Mf_m(\cdot),
\end{equation}
where each $f_m$ is a componentwise learner, can then also be seen as a sum of componentwise effects,
\begin{equation*}
    f(\x)=\sum_{j=1}^p p_j(x_j),
\end{equation*}
where $p_j(\cdot)=\sum_{m=1}^M f(x_j)$. For a schematic overview of the algorithm, see Algorithm \ref{algo:component-wise}.
\begin{algorithm}
\caption{Component-wise gradient boosting}\label{algo:component-wise}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Specify a set of base learners $\mathcal{H}=\{h_1(\cdot),\dotsc,h_p(\cdot)\}$, where each $h_j$ is univariate.
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector,
        \begin{equation}
            \u=\left(-\frac{\partial}{\partial \hat{f}}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
        \end{equation}
    \item For each base learner $h_j\in\mathcal{H},j=1,\ldots,p$, estimate $\hat{h}_{j,m}$ by fitting $(\X_i,u_i)$ using the base learner $h_j$
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N(u_i,h_j(\x_i;\bbeta))^2
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Select $h(\cdot;\bbeta_m)=\argmin_j\sum_{i=1}^N(u_i,h_j(\x_i;\bbeta))^2$
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 8 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
\subsection{Variable selection}
If the number of iterations $M$ is not very large compared to the number of variables, the component-wise gradient boosting algorithm will carry out automatic variable selection. What this means is that based on their explanatory power, the base learners applied to irrelevant variables will never be added into the model, and therefore many of the columns of $\X$ will not be a part of the final model. In many cases, a good $M$ is less than the number of predictors, and so even if every iteration selects a different base learner, the final model will have excluded predictors. Realistically, some predictors will be more explanatory than others, and so they will be selected more than once. Some predictors are simply more correlated with the output than others. Therefore some components will lead to better improvements and those corresponding base learners will thus be more frequently selected. Therefore the component-wise boosting algorithm has inherent variable selection.

\section{Selecting $\mstop$}
As we mentioned in subsection \ref{subsec:iterations}, the crucial tuning parameter in boosting is the number of iterations, $\mstop$. Stopping early enough performs variable selection and shrinks the parameter estimates toward zero. In the case of $p<N$, with $m\to\infty$, the parameters in boosting will converge towards the maximum likelihood estimates \citep{DeBin2016}, i.e., maximizing the in-sample error. We are, on the other hand, after all interested in minimizing out-of-sample prediction error (PE). The prediction error for a given data set is a function of the boosting iteration $m$. What we want is therefore a good method for approximating $\PE(m)$. This can be done in a number of ways. Many authors state that the algorithm should be stopped early, but do not go further into the details here. Common model selection criteria such as the Akaike Information Criteria (AIC) may be used, however the AIC is dependant on estimates of the model's degrees of freedom. Methods by \citet{chang2010} try this. This is problematic for several reasons. For $\text{L}_2\text{Boost}$, \citet{buhlmann2007} suggest that $\df(m)=\trace(B_m)$ is a good approximation. Here $B_m$ is the hat matrix resulting from the boosting algorithm. This was, however, shown by \citet{hastie2007} to always underestimate the actual degrees of freedom. \citet{mayr-hofner} propose a sequential stopping rule using subsampling. However this is computationally very expensive and not really used in practice. Instead, cross-validation, a very common method for selection of tuning parameters in statistics, is what is used in almost all cases, both in practice and in research.\todo{citation?} Cross-validation is flexible and easy to implement. It is somewhat computationally demanding, because it requires several full runs of the boosting algorithm.

\subsection{Other selection methods}
The number of iterations in the boosting procedure, $M$, is a tuning parameter. It acts as a regularizer. AIC, etc.

\subsection{K-fold cross-validation}\label{subsec:K-fold}
K-fold cross-validation \citep{lachenbruch}, or simply cross-validation, is a general method commonly used for selection of penalty or tuning parameters. We will use it to approximate the prediction error. In cross-validation, the data is split randomly into K rougly equally sized folds. For a given fold $k$, all folds except $k$ act as the training data in estimating the model. We often say that the $k$-th fold is left out. The resulting model is then evaluated on the unseen data, namely the observations belonging to fold $k$. This procedure is repeated for all $k=1,\ldots,K$. An estimate for the prediction error is obtained by averaging over the test errors evaluated in each left-out fold. Let $\kappa(k)$ be the set of indices for fold $k$. The cross-validated estimate for a given $m$ then becomes
\begin{equation}
    \CV(m)=\sum_{k=1}^K\sum_{i\in\kappa(k)}\rho(y_i,\hat{y}_i^{-\kappa(k)}).
\end{equation}
For each $m$, we calculate the estimate of the cross-validated prediction error $\CV(m)$. We choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\CV(m).
\end{equation}
Typical values for $K$ are 5 or 10, but in theory one can choose any number. The extreme case is $K=N$, called leave-one-out cross-validation, where all but one observation is used for training and the model is evaluated on the observation that was left out. In this case, the outcome is deterministic, since there is no randomness when dividing into folds.

\subsection{Stratified cross-validation}
When dividing an already small number of survival data observations into $K$ folds, we might risk getting folds without any observed deaths, or in any case, very few. In stratified cross validation, we do not divide the folds entirely at random, but rather, try to divide the data such that there is an equal amount of censored data in each fold.
As before, let $\kappa(k)$ be the set of indices for fold $k$. Divide the observed data into $K$ folds, as with usual cross validation, to get an index set $\kappa_{\delta=1}(k)$ for a given $k$. Similarly, divide the censored data into $K$ folds, obtaining $\kappa_{\delta=0}(k)$. Finally, $\kappa(k)$ is the union of these sets: $\kappa(k)=\kappa_{\delta=1}(k)\cup\kappa_{\delta=0}(k)$.
For a detailed description of 10-fold cross-validation issues in the presence of censored data, see \citet{kohavi}.

\subsection{Repeated cross-validation}
The randomness inherent in the cross-validation splits has an effect on the resulting $\mstop$. This is true for boosting in general, but it is true for real-life survival data, especially. In typical survival time data sets one typically has a small effective sample size (number of observed events). We can easily imagine that for two different splits of the data, we can end up with quite different values for $\mstop$.
It has been very effectively demonstrated that the split of the folds has a large impact on the choice of $\mstop$ \citep{seibold}. \citet{seibold} suggest simply repeating the cross-validation scheme. They show that repeating even 5 times effectively averages out the randomness.  In other words, we divide the data into $K$ folds, and repeat this $J$ times. Now let $\kappa(j, k)$ be the $k$-th fold in the $j$-th split. We end up with a new estimate for the prediction error,
\begin{equation}
    \RCV(m)=\sum_{j=1}^J\sum_{k=1}^K\sum_{i\in\kappa(j,k)}\rho(y_i,\hat{y}_i^{-\kappa(j,k)}).
\end{equation}
As before, we choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\RCV(m).
\end{equation}
In practice, to ensure we find the minimizing $m$, we let the boosting algorithm run for $m=1$ to $m=M$, where $M$ is a large number that we are sure will result in a overfitted model.

\section{Cyclical component-wise multidimensional boosting}
A limitation of both classical boosting \citep{friedman2001} and $L_1$-penalized estimation such as the lasso method \citep{lasso} is that they are designed for statistical problems involving a one-dimensional prediction/loss function. By only considering such functions, we are restricted to estimating models which only model a single quantity of interest, which is almost always the mean.
In many applications, modelling only one parameter will not be sufficient. We want to be able to estimate more general models, in which more quantities, e.g. the drift and the threshold of the models described in section \ref{sec:FHT}, can be explained by covariates. Typical examples of multidimensional estimation problems are classification with multiple outcome categories and regression models for count data. Another example is estimating models in the GAMLSS family \citep{gamlss}. GAMLSS, which refer to ``generalized additive models for location, scale and shape,'' are a modelling technique that relates not only the mean, but all parameters of the outcome distribution to the available covariates. A gradient boosting algorithm called gamboostLSS was developed for boosting such models \citep{gamboostlss-paper}. The algorithm framework used in gamboostLSS is an example of the multidimensional boosting algorithm first introduced in \citet{schmid}. We will here explain the algorithm used in this paper.

\citet{schmid} extend the component-wise gradient boosting algorithm \citep{friedman2001} to such a setting where one has a prediction function $f$ which takes $K$ parameters:
\begin{equation}
    f(\btheta)=f(\theta_1,\theta_2,\ldots,\theta_K).
\end{equation}
Like in regular statistical boosting, we must be able to obtain derivatives. Therefore, it is assumed that all partial derivatives
\begin{equation}
    \frac{\partial}{\theta_1}f(\btheta),\frac{\partial}{\theta_2}f(\btheta),\ldots,\frac{\partial}{\theta_K}f(\btheta)
\end{equation}
exist. The main idea of the cyclical multidimensional boosting algorithm, although cyclical was a term coined later \citep{thomas2018}, is to have a boosting step for each component $k=1,2,\ldots,K$, in each iteration. In other words, we cycle through all parameter dimensions in each boosting iteration. The initialization of the algorithm is done analogously to the regular boosting method, by setting each parameter to a constant, typically zero or the constant which maximizes the maximum likelihood. We also specify a base learner for each parameter dimension. In a given boosting iteration $m+1$, the algorithm cycles through the different parameter dimensions $k$. In every dimension $k$, we carry out one boosting iteration. This boosting iteration is essentially the same as in the usual component-wise gradient boosting algorithm \eqref{algo:component-wise}. At this point, the estimated parameter vector is
\begin{equation*}
    \hat{\btheta}_{k-1}^{[m+1]}=\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\ldots,\hat{\theta}_{k-1}^{[m+1]},\hat{\theta}_{k}^{[m]},\hat{\theta}_{k+1}^{[m]},\ldots,\hat{\theta}_{K}^{[m]}\right),
\end{equation*}
meaning all parameter dimensions $1,2,\ldots,k-1$ have been updated in the current iteration $m+1$. The following dimensions $k+1,\ldots,K$ have not, and we are now going to update dimension $k$. We calculate a residual for dimension $k$ by calculating the $k$-th partial derivative and inserting the current estimated parameter vector $\hat{\btheta}_{k-1}^{[m+1]}$, and evaluating it at the observations $x_1,x_2,\ldots,x_N$. This yields residual vector
\begin{align*}
\u_k&=(u_{k,1},u_{k,2},\ldots,u_{k,N})\\
&=\left(-\frac{\partial}{\partial \theta_k}f(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\ldots,\hat{\theta}_{k-1}^{[m+1]},\hat{\theta}_{k+1}^{[m]},\ldots,\hat{\theta}_{K}^{[m]})\right)_{i=1}^N.
\end{align*}
Again, like in a regular component-wise boosting algorithm, we fit all component-wise base learners separately to this residual vector $\u_k$. Of these learners, select the best fitting component $j^*$ and corresponding estimated learner $\hat{h}_{j^*}(\cdot)$, and update the parameter in dimension $k$ by the usual
\begin{equation}
    \theta_k^{[m+1]}=\theta_k^{[m]}+\nu\cdot \hat{h}_{j^*}(\cdot).
\end{equation}
A schematic representation of the updating process using this algorithm in a given iteration $m+1$ looks as follows:
\begin{align*}
    \frac{\partial}{\theta_1}\rho\left(\hat{\theta}_1^{[m]},\hat{\theta}_2^{[m]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{1}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{1}^{[m+1]} \\
    \frac{\partial}{\theta_2}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}} \u_{2}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{2}^{[m+1]} \\
    \frac{\partial}{\theta_3}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{3}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{3}^{[m+1]} \\
    \ldots\\
    \frac{\partial}{\theta_{K-1}}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m+1]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{K-1}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{K-1}^{[m+1]} \\
    \frac{\partial}{\theta_K}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m+1]},\ldots,\hat{\theta}_{K-1}^{[m+1]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{K}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{K}^{[m+1]}
\end{align*}
After cycling through all estimation parameters in $\rho$, we find the current optimal nuisance parameters, if the loss function has any nuisance parameters. A nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest. For example, if we are considering a multivariate normal distribution and are only interested in the means, the residual variance would be a nuisance parameter. We find the optimal nuisance parameters by performing numerical minimization of the empirical risk for one scale parameter at a time.
For each nuisance parameter $\sigma_l, l=1,2,\ldots,L$, we plug all other estimated parameters into the empirical risk, and minimize it over the current nuisance parameter $\sigma_l$:
\begin{equation}
    \hat{\sigma_l}^{[m+1]}=\argmin_{\sigma_l} \sum_{i=1}^N \rho(y_i,\hat{f}^{[m+1]},\hat{\sigma}_1^{[m+1]},\hat{\sigma}_2^{[m+1]},\ldots,
    \hat{\sigma}_{l-1}^{[m+1]},\sigma_l,\hat{\sigma}_{l+1}^{[m]},\ldots,\hat{\sigma}_L^{[m]})
\end{equation}
The gamboostLSS algorithm \citep{gamboostLSS} does not include this step this because GAMLSS do not have any nuisance parameters.
\begin{algorithm}
\caption{Multidimensional cyclical component-wise gradient boosting}
\label{algo:multi-cyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{[0]}_1,f^{[0]}_2,\ldots,f^{[0]}_K,$, e.g., by setting them to zero for all components, or by finding the best constant by a numerical maximum likelihood method. Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1. If $m>m_{\text{stop},k}$, go to step X. Otherwise compute the negative partial derivative
        $-\frac{\partial\rho}{\partial \hat{f}_k}$ and evaluate at $\hat{f}^{[m-1]}(x_i),i=1,\ldots,N$, yielding the
        negative gradient vector
        \begin{equation}
            \u^{[m-1]}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{[m-1]}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{[m-1]}_k$.
    \item Select the component of $X$ which best fits $\u^{(m-1)}_k$ according to a pre-specified goodness-of-fit, typically RSS.
        Set $\hat{\u}^{[m-1]}_k$ equal to the fitted values of the corresponding best model fitted in step X.
    \item Update $\hat{f}_k{[m-1]}\gets\hat{f}_k^{[m-1]}+\nu\hat{U}_k^{[m-1]},$ where $\nu$ is a pre-specified real-valued step-length factor.
    \item Repeat steps 6 until 9 for $k=2,\ldots,K$. 
    \item Finally, update $\hat{f}^{[m]}\gets\hat{f}^{[m-1]}$.
    \item Set $l=0$.
    \item Increase $l$ by 1.
    \item Plug $\hat{f}^{(m)}$ and $\hat{\sigma}_1^{[m-1]},\ldots,\hat{\sigma}_{l-1}^{[m-1]},\hat{\sigma}_{l+1}^{[m-1]},\hat{\sigma}_{L}^{[m-1]}$ into the empirical risk function $R$ and minimize the empirical risk over $\sigma_l$. Set $\hat{\sigma}_l^{[m]}$ equal that minimizer.
    \item Repeat steps 13 and 14 for $l=2,\ldots,L$.
    \item Repeat steps 4 to 15 until $m=\max_k(m_{\text{stop},k})$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}

See \eqref{algo:multi-cyclical} for a schematic overview of the algorithm. Note that this algorithm resembles the backfitting strategy by \citet{hastie1986}. In both strategies, components are updated successively by using estimates of the other components as offset values. In backfitting, a completely new estimate of $f^*$ is determined in every iteration, but in gradient boosting, the estimates are only slightly modified in each iteration.

The main tuning parameters in this algorithm are the stopping iterations $\mathbf{m}_{\text{stop}}=m_{\text{stop},1},\ldots,m_{\text{stop},K}$. As in the one-dimensional gradient boosting algorithm, \citet{schmid} say that it should not run until convergence, but rather find estimates by cross-validation. An issue with this algorithm, though, is that for proper tuning it requires a vector $\mathbf{m}_{\text{stop}}$ of stopping iterations, one for each prediction parameter. To properly tune these parameters, it is necessary to perform what is often called a grid search, i.e., do a multidimensional search of the parameters. This should be done by cross-validation, as usual, and the next subsection explains the procedure.

\subsection{Grid search cross-validation}\label{grid-search}
To find a vector of length $K$ of optimal iterations $\mathbf{m}_{\text{stop}}=m_{\text{stop},1},\ldots,m_{\text{stop},K}$, we perform a $K$-dimensional grid search. We must first specify a minimum and maximum number of iterations for each parameter. Call these $m_{\min,k}$ and $m_{\max,k}$, respectively. We then divide this one-dimensional search space into a finite grid with $N_k$ points, such that we obtain
\begin{equation} 
    m_{\min,k}=m_{1,k}<m_{2,k}<\ldots<m_{N_k-1,k}<m_{N_k,k}=m_{\max,k},
\end{equation}
again for each $k=1,2,\ldots,K$. The total search space is the cartesian product of all of these grids. We illustrate with an example. Let $K=3$, and $m_{\min,k}=1$ and $m_{\max,k}=10$ for all $k$, and finally divide each grid into 10 points, i.e., $N_1=N_2=N_3=10$. The total search grid will consist of $N_1\cdot N_2\cdot N_3=10^3=10000$ tuples of configurations of $\mathbf{m}$, enumerated below:
\begin{align*}
    \left(m_{1,1},m_{1,2},m_{1,3}\right) \\
    \left(m_{1,1},m_{1,2},m_{2,3}\right) \\
    \ldots \\
    \left(m_{1,1},m_{1,2},m_{10,3}\right) \\
    \left(m_{1,1},m_{2,2},m_{1,3}\right) \\
    \ldots \\
    \left(m_{1,1},m_{2,2},m_{10,3}\right) \\
    \ldots \\
    \left(m_{10,1},m_{10,2},m_{10,3}\right).
\end{align*}
We want to find the best configuration $\mathbf{m}$, i.e., we want to find the optimum of the hyperplane $CV(\mathbf{m})$. Like in subsection \ref{subsec:K-fold}, we must calculate the estimate of the cross-validated prediction error for each given configuration $\mathbf{m}$, obtaining the prediction error $CV(\mathbf{m})$. We choose $\mathbf{m}_{\text{stop}}$ to be the minimizer of this error,
\begin{equation*}
    \mathbf{m}_{\text{stop}}=\argmin_{\mathbf{m}}\CV(\mathbf{m}).
\end{equation*}
Using boosting, we may obtain estimates of $\CV(\mathbf{m})$ for all $\CV(\mathbf{m})$ by fixing all but one of the parameters and perform a typical boosting run. If we fix all but one of the parameters in the vector $\mathbf{m}=\left(m_{i_1,1},m_{i_2,2},m_{i_3,3}\right)$, where $m_{\min,k}\leq i_k\leq m_{\max,k}$ for all $k=1,2,3$, say, we fix $m_{i_1,1}$ and $m_{i_2,2}$. This is due to the way boosting algorithms work, since for any given iteration $M$, we also automatically obtain all boosted estimates for all iterations less than $M$, if we have saved the boosted parameters for each iteration. Consider again the example. We now let the first two parameters in the example be fixed for each boosting run. While the search grid consist of $N_1\cdot N_2\cdot N_3$ tuples, considering the first two parameters as fixed, we only need to do $N_1\cdot N_2$ boosting runs, and in each run set the maximum number of possible iterations in the boosting algorithm for the third component to be $m_{\max,3}$. This means that we consider all configurations of the first two parameters in $\mathbf{m}$, i.e.,
\begin{align*}
    \left(m_{1,1},m_{1,2}\right) \\
    \left(m_{1,1},m_{2,2}\right) \\
    \ldots \\
    \left(m_{1,1},m_{10,2}\right) \\
    \left(m_{2,1},m_{1,2}\right) \\
    \ldots \\
    \left(m_{2,1},m_{10,2}\right) \\
    \ldots \\
    \left(m_{10,2},m_{10,2}\right),
\end{align*}
and do a boosting run for each such. Like in subsection \ref{subsec:K-fold}, we choose
\begin{equation*}
    \mathbf{m}_{\text{stop}}=\argmin_{\mathbf{m}}\CV(\mathbf{m}),
\end{equation*}
where
\begin{equation*}
    \CV(\mathbf{m})=\sum_{k=1}^K\sum_{i\in\kappa(k)}\rho(y_i,\hat{y}_i^{-\kappa(k)}),
\end{equation*}
i.e., the cross-validated prediction error, as usual.
\section{Noncyclical component-wise multidimensional boosting algorithm}
In the cyclical algorithm seen previously, algorithm \ref{algo:multi-cyclical}, the different $m_{\text{stop},j}$ parameters are not independent of each other, and hence they have to be jointly optimized. As we saw in the previous subsection \ref{grid-search}, the usually applied \textit{grid search} for such parameters scales exponentially with the number of parameters $K$. This can quickly become very demanding computationally. \citet{thomas2018} develop a new algorithm for fitting GAMLSS models, instead of the cyclical one used in gamboostLSS. In this new algorithm, which they call ``noncyclical,'' only one scalar tuning parameter $m_{\text{stop}}$ is needed because only one parameter is chosen in each boosting iteration. Compared to the cyclical algorithm in gamboostLSS \citep{gamboostlss-paper}, this noncyclical algorithm obtains faster variable tuning and equal prediction results on simulation studies carried out \citep{thomas2018}.

\subsection{Gradients are not comparable across parameters}
In the cyclical algorithm, we always boost all parameters in the same iteration. Therefore we do not need to choose between parameters. If we want to avoid having a separate tuning parameter for each parameter that we are boosting, however, it is necessary to choose one parameter to boost in each iteration. To do this we have to choose between parameters, and so we need to be able to find out which parameter would lead to the best increase in performance. We already do this for choosing which component-wise learner to use in each parameter. There, we choose that which has the best residual-sum-of-squares (RSS), with respect to the negative gradient vector. \citet{thomas2018} denote this the \textit{inner loss}.\todo[inline]{Add empirical proof?}However, in general these residual vectors are not comparable across parameters of the loss function, because the parameters have different scales \citep{thomas2018}. In a normal distribution, for example, the partial derivatives for the mean and the partial derivative for the standard deviation will not be comparable. Therefore, to compare between parameters, a different comparison method is needed. We cannot compare the RSS'es, because it will not tell us which parameter will decrease the loss function the most.

For each parameter $\theta_k$, we choose the component-wise base learner which best fits according to the RSS,
\begin{equation}
    \hat{h}_{\theta_k}(\cdot).
\end{equation}
If we incorporate this estimated base learner into the full boosted model, we would get
\begin{equation}
    f^{[m+1]}_{\theta_k}=f^{[m]}+\nu\cdot\hat{h}_{\theta_k}(\cdot).
\end{equation}
We can insert this proposed new model into the loss function to obtain a new empirical risk value,
\begin{equation}
    R(f^{[m+1]}_{\theta_k}).
\end{equation}
We calculate the gain in the loss function, which we denote $\Delta\rho_{\theta_k}$, by
\begin{equation}
    \Delta\rho_{\theta_k}=R(f^{[m]})-R(f^{[m+1]}_{\theta_k}).
\end{equation}
If we now compare the gain in loss function value, $\rho_{\theta_k}$, across each parameter $k=1,2,\ldots,K$, we can find out which parameter leads to the best increase. We choose that one, i.e.,
\begin{equation}
    k^*=\argmin_k\Delta\rho_{\theta_k},
\end{equation}
and incorporate only the best-fitting learner corresponding for that parameter into the full boosting model, i.e.,
\begin{equation}
    f^{[m+1]}\gets f^{[m+1]}_{\theta_k}=f^{[m]}+\nu\cdot\hat{h}_{\theta_k}(\cdot).
\end{equation}
We choose the base learner for each component by comparing their residual sum of squares with respect to the negative gradient vector, which we called the inner loss. In other words, we use the usual procedure of choosing the component-wise learner for each parameter which minimizes the RSS, i.e.,
\begin{equation}
    j^*=\argmin_{j} \sum_{i=1}^N (u_{k,i}-(f^{[m]}(x_i)+\hat{h}(\x_i)))^2.
\end{equation}
This is, however, not the same criterion that is used to choose between parameters. This might be problematic. Therefore, \citet{thomas2018} propose using the loss function $\rho$ for choosing between component-wise learners as well. They call this the ``outer loss.'' In that case, we instead choose the component-wise learner for each parameter which minimizes the outer loss function, i.e., 
\begin{equation}
    j^*=\argmin_{j} \sum_{i=1}^N \rho(y_i,f(x_i)).
\end{equation}
The individual component-wise learners are still estimated by their usual method, i.e., calculating the negative gradient of the generalized residuals and using the base learner to estimate models. E.g., by linear least squares if using simple linear regression base learners. The improvement in the empirical risk, $\Delta\rho_{\theta_k}$, is then calculated for each base learner of every distribution parameter, and only the overall best-performing base learner with regard to the outer loss is updated.

In both cases of this algorithm, we have the advantage that the optimal number of boosting steps, $m_{\text{stop}}$, is always a scalar value. Finding this tuning parameter can be done fairly quickly with standard cross validation schemes, and most importantly, it scales with with the number of parameters. This is unlike the cyclical algorithm, which needs a multidimensional grid search.

\begin{algorithm}
\caption{Multidimensional noncyclical component-wise gradient boosting}
\label{algo:multi-noncyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{(0)}_1,f^{(0)}_2,\ldots,f^{(0)}_K,$, e.g., by setting it to zero for all components, or by finding the best constant.% Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1.
    \item Compute the negative partial derivative $-\frac{\partial\rho}{\partial \hat{f}_k}$
        and evaluate at $\hat{f}^{(m-1)}(x_i),i=1,\ldots,N$, yielding negative gradient vector
        \begin{equation}
            \u^{(m-1)}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{(m-1)}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{(m-1)}_k$.
    \item Select the best fitting base learner, $h_{kj}$, either by
        \begin{itemize}
            \item the inner loss, i.e., the RSS of the base-learner fit w.r.t the negative gradient vector
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N(u_k^{(i)}-\hat{h}_{kj}(x^{(i)}))^2
                \end{equation}
            \item the outer loss, i.e., the loss function after the potential update,
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj}(x^{(i)}) \right)
                \end{equation}
        \end{itemize}
    \item Compute the possible improvement of this update regarding the outer loss,
        \begin{equation}
            \Delta\rho_k=\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj^*}(x^{(i)}) \right)
        \end{equation}
    \item Update, depending on the value of the loss reduction, $k^*=\argmin_{k\in1,\ldots,K}\Delta\rho_k$
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}+\nu\cdot\hat{h}_{k^*j^*}(x),
        \end{equation}
        while for all $k\neq k^*$,
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}.
        \end{equation}
    \item Repeat steps 4 to 11 until $m=m_{\text{stop}}$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}