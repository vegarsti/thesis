\chapter{Statistical boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}. It has become a staple part of the statistical learning toolbox because it is a flexible tool for estimating interpretable statistical models. Boosting, however, originated as a black box algorithm in the fields of computational learning theory and machine learning, not in statistics.

Computer scientists Michael Kearns and Leslie Valiant, who were working on computational learning theory, posed the following question: Could any weak learner be transformed to become a strong learner? \citep{kearnsvaliant} A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. In the binary classification setting, then, it would only perform slightly better than a coin flip. For regression, a weak learner is for example a linear least squares model of only one variable, and having only a small parameter effect for that variable. Meanwhile, a strong learner should be able to perform in a near-perfect fashion, for example attaining high accuracy on a prediction task. I will first attend to give a summary of the history of boosting, starting with AdaBoost \citep{adaboost}, which proved that the answer to the original question above was yes. For a complete overview, see \citet{mayr14a, mayr14b, mayr17}.

\section{AdaBoost: From machine learning to statistical boosting}
The original AdaBoost, also called Discrete AdaBoost \citep{adaboost} is an iterative algorithm for constructing a binary classifier $F(\cdot)$. It was the first \textit{adaptive} boosting algorithm, as it automatically adjusted its parameters to the data based on its perfomance. In the binary classification problem, we are given a set of observations $(\x_i,y_i)_{i=1,\ldots,n}$, where $x_i\in\R^p$ and $y_i\in\{-1,1\}$, i.e., positive or negative; yes or no. We want to find a rule which best separates these observations into the correct classes $\{-1,1\}$, as well as being able to classify new, unseen observations $\x_{\text{new}}$ of the same form. Some observations are hard to classify, whereas some are not. One way to look at binary classification is to imagine the $p$-dimensional space of the observations $\x$, and think of the classifier as finding the line which best splits the observations into their corresponding label. Some observations are not at all close to the boundary, and so they are easily classified. The problems start when the observations are close to the boundary. \citet{adaboost} realized that one could assign a weight to each observation. First, assign equal weight to each observation. Then, use a weak learner $h(\cdot)$ to make an initial classifier, minimizing the weighted sum of misclassified points. After this initial classification, some points will be correctly classified, and some will be misclassified. We increase the weights of the misclassified ones, and normalize the weights afterwards. This then also results in the correctly classified ones having a reduced weight. Finally, based on the misclassification rate of this classifier, calculate a weight $\alpha$ to give to this classifier. Currently, the classifier is $F_1(\cdot)=\alpha_1h_1(\cdot).$ In the next iteration, aaply again a weak learner which minimizes the weighted sum of the observations and reweight observations accordingly as before. Again, calculate a weight to give to this new classifier, and add it to the previous classifier, such that $F_2(\cdot)=\alpha_1h_1(\cdot)+\alpha_2h_2(\cdot)$. Continue iterating in this fashion until an iteration $m$. The resulting final classifier, the AdaBoost classifier, becomes $\hat{F}(\cdot)=F_m(\cdot)=\sum_{i=1}^m\alpha_ih_i(\cdot)$. It is a linear combination of the weak classifiers, and in essence a weighted majority vote of weak learners given the observations.

The AdaBoost algorithm often carries out highly accurate prediction. In practice, it is often used with stumps: Decision trees with one split. For example, \citet{bauer-kohavi} report an average 27\% relative improvement in the misclassification error for AdaBoost using stump trees, compared to the error attained with a single decision tree. They conclude that boosting not only reduces the variance in the prediction error from using different training data sets, but that it also is able to reduce the average difference between the predicted and the true class, i.e., the bias. \citet{breiman1998} supports this analysis. Because of its plug-and-play nature and the fact that it never seemed to overfit (overfitting occurs when the learned classifier degrades in test error because of being too specialized on its training set), Breiman remarked that ``boosting is the best off-the-shelf classifier in the world'' \citep{ESL}.

Overfitting occurs when the out-of-sample error starts to increase. At this point, the model is starting to be too sensitive to the structure of the specific data set it is estimated on. One way of thinking about it is that it is starting to fit to the error terms. Since what we actually care about is the performance on a test set, we want to stop just before the model starts overfitting.

In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, however, we are interested in models which are interpretable.

While originally developed for binary classification, boosting is now used to estimate the unknown quantities in more general statistical models and settings. We therefore extend our discussion to a more general statistical regression scheme.

%See some figure for a schematic overview of the algorithm.

\section{General model structure and setting} %Statistical model fitting and setting
The aim of statistical boosting algorithms is to estimate and select the effects in structured additive regression models. Consider a data set
\begin{equation}
    D=\{\X^{(i)},\Y^{(i)}\}_{i=1,\ldots,n}
\end{equation}
containing the values of an outcome variable $\Y$ and predictor variables $\X_1, \X_2,\ldots,\X_p$, forming covariate matrix $\X=(\X_1, \X_2,\ldots,\X_p)$. We assume that the samples $i=1,\ldots,n$ are generated independently from an identical distribution over the joint space $\setX\times\setY$. The input space of $\X$ is a possibly high-dimensional $\setX\in\R^p$ and the output space is a low-dimensional space $\setY$. For the majority of applications, the output space $\setY$ is one-dimensional, but we will explicitly allow for multidimensional outcome variables. Our objective is to model the relationship between $\Y$ and $\X$ and to obtain an ``optimal'' prediction of $\Y$ given $\X$. 

An important model class is the generalized additive model (GAM) \citep{gam-book}. In a GAM, the conditional distribution of the response variable is assumed to follow an exponential family distribution. Then, the expected response $\Y|\X$ is modeled given the observed value $\x$ using a link function $g$ as
\begin{equation}
    g(\mathbb{E}(\Y|\X=\x))=f(\x).
\end{equation}
We will not restrict ourselves to GAMs, but we will do something very similar. We will consider outcomes from a distribution
\begin{equation}
    \phi(\Y|\X=\x,\theta),
\end{equation}
which we will also at times refer to as a prediction function. We will model the conditional distribution of the parameter $\theta$ used in the prediction function, i.e.,
\begin{equation}
    \mathbb{E}(\theta|\X=\x)=f(\x).
\end{equation}
This parameter $\theta$ will typically correspond to the mean, but varying the distribution of course gives a lot of flexibility. In both these cases, we will call the function $f(\cdot)$ an additive predictor, which consists of the additive effects of the single predictors,
\begin{equation}\label{eq:gam}
    f(\x)=\beta_0+f_1(x_1)+\ldots+f_p(x_p),
\end{equation}
where $\beta_0$ is a common intercept and the functions $f_j(x_j),j=1,\ldots,p$ are the partial effects of the variables $x_j$. The generic notation $f_j(x_j)$ may be different types of predictor effects such as classical linear effects $x_j\beta_j$, smooth non-linear effects constructed via regression splines, spatial effects or random effects of the explanatory variable $x_j$, and so on. In statistical boosting algorithms, we typically use component-wise effects, meaning that the different partial effects are estimated by separate base-learners $h_1(\cdot),\ldots,h_p(\cdot)$. Read more about this in section \ref{sec:component} on component-wise boosting. The component-wise effects will be built up by additive estimation of base-learners.

We evaluate the additive predictor using a loss function $\rho(y,f(\cdot))$, which is a measure of the discrepancy between the observed outcome $\y$ and the additive predictor $f(\cdot)$. Very often, the loss $\rho$ is derived from the negative log likelihood of the distribution of $\setY$. We denote this distribution $\psi(y)$. In case of GAMs, this will be the distribution of the corresponding exponential family. So the loss function is
\begin{equation}
    \rho(\y,f(\x))=-\log{\psi(\y|f(\x)}.
\end{equation}
Note that we will use the negative log likelihood as the loss function, due to the aim of \textit{minimizing} the loss function, whereas the log likelihood increases as the model fits better to the data.

\subsection{Example of a model and corresponding loss function}
An example might be to model a normal linear regression. We assume data $D=(\X_i,Y_i)_{i=1}^N$ generated from a normal distribution $N(\mu,1)$, where $\mu$ corresponds to $\theta$ above and is a functional which depends on covariates, i.e.,
\begin{equation}
    \mu(\bbeta,\X)=g(\bbeta^T\X),
\end{equation}
and where $g(\cdot)$ is a link function. For a continuous response $\Y$, we let the link function $g$ be the identity link, such that
\begin{equation}
    \mu(\bbeta,\X)=\bbeta^T\X=\beta_0+\sum_{j=1}^p\beta_jx_j.
\end{equation}
Although $\mu$ is a functional which depends on covariate matrix $\X$ and parameter vector $\bbeta$, we will denote it $\mu$ whenever possible to lighten the notation. For a normally distributed observation, the likelihood is the pdf,
\begin{equation}
    .f(y|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{-\frac{(y-\mu)^2}{2\sigma^2}},
\end{equation}
and we derive the loss function $\rho$ accordingly, yielding
\begin{align*}
    \rho(y,x)&=-\log{f(y|\mu)}\\
    &=\log{(\sqrt{2\pi\sigma^2})}+\frac{(y-\mu)^2}{2\sigma^2} \\
    &\propto(y-\mu)^2,
\end{align*}
which is the familiar $L_2$ loss function. Note that since we will only perform regression on $\mu$, the loss function need not depend on $\sigma^2$, so we must only preserve the relevant proportionality.

\subsection{Optimization}
Obtaining the optimal prediction is usually accomplished by minimizing $\rho$. The goal of the model fitting scenario is to estimate a function which minimizes the loss over an unseen ``hold-out'' sample, often called the out-of-sample error, the generalization error, or the test error. For a specific data set, we can calculate the empirical risk $R$, which is the sum of the loss function evaluated on all samples in the learning data set $D$,
\begin{equation}\label{eq:empirical-risk-2}
    R(D)=\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)})).
\end{equation}
Other names for $R$ are in-sample error and training error. Since $D$ arises from a data distribution, $R(D)$ is a realization of a more general loss value. We wish to learn about the general structure of $D$, and as such are we most interested in the expected loss, also known as the generalization or test error,
\begin{equation*}
    \text{Err}_{D}=\E[\rho(Y,f(\X))|D],
\end{equation*}
where $(X,Y)$ is drawn randomly from their joint distribution and the training set $D$ is held fixed. It is infeasible to do effectively in practice
and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_D]=\E_D\left[\rho(Y,f(\X))|D\right],
\end{equation}
i.e., average over many different test sets. As mentioned, in practice we observe a sample data set $D$. For this sample, we can calculate the training error -- the empirical risk. To estimate $\text{Err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. Call this data set for $D_{\text{test}}$ We then estimate our model and its parameters based on the other 80\%, and make an estimate of the generalization error $\text{Err}$ by seeing how our estimated model performs on the hold-out test set. We call the resulting error
\begin{equation*}
    \widehat{\text{Err}}_{\text{test}}=\frac{1}{M}\sum_{i=1}^M \rho(y_i,\hat{f}(\x_i)),
\end{equation*}
for test error. If the observed sample is not large enough, one can calculate a $K$-fold cross-validated test error. In this case, one divides the data set into $K$ parts (folds), and for each fold, one lets it be the hold-out data set, and estimate a model using only the other $K-1$ folds. In this way, one gets $K$ test errors, and so the cross-validated test error is the mean of these. See later for a more detailed description.

\section{Gradient boosting}
\citet{friedman2001} developed an iterative algorithm for fitting an additive model \eqref{eq:gam} called gradient boosting. He showed that AdaBoost performs this algorithm for a particular exponential loss function, which provided a connection between what had previously been in the machine learning domain, with the statistical domain. See \citet{ESL} for a good demonstration of this AdaBoost argument. This provided a way of viewing boosting through a statistical lens, and connected the successful machine learning approach to the world of statistical modelling. To understand gradient boosting, we first need to understand the gradient descent algorithm.

\subsection{Gradient descent}
Suppose we are trying to minimize a differentiable multivariate function $G\colon\R^m\to\R$, where $m\in\N$. Gradient descent is a greedy algorithm for finding the minimum of such a function $G$, and one which is quite simple and surprisingly effective. If all partial derivatives of $G$ at a point $\x=(x_1,x_2,\ldots,x_m)$ exist, then the gradient of $G$ at $\x$ is the vector of all its partial derivatives at $\x$, namely
\begin{equation}
    \nabla G(\x)=\left(\frac{\partial G(\x)}{\partial x_1},\frac{\partial G(\x)}{\partial x_2},\ldots,\frac{\partial G(\x)}{\partial x_m}\right).
\end{equation}
The motivation behind the gradient descent algorithm is that in a small interval around a point $\x_0\in\R^m$, $G$ is decreasing the most in the direction of the negative gradient at that point. Therefore, by taking a small step slightly in the direction of the negative gradient, from $\x_0$ to a new value $\x_1$, we end up with a slightly lower function value: The new function value $G(\x_1)$ will be less than $G(\x_0)$. In some versions of the algorithm, the step length $\nu\in(0,1]$ is found by a line search, i.e., by finding the step length which gives the best improvement. In other versions, one simply uses a fixed step length. The gradient descent algorithm repeats this procedure until convergence. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit possibly to a local minimum. For a schematic overview of the algorithm, see Algorithm \ref{algo:grad-desc}.
\begin{algorithm}
\caption{Gradient descent}
\label{algo:grad-desc}
We want to minimize $G(\x)$, i.e. solve $\min_{\x}G(\x)$.
\begin{enumerate}
    \item Start with an initial guess $\x_0$, e.g. $\x_0=\0$. Let $m=1$.
    \item Calculate the direction to step in, $\g_{m-1}=-\nabla G(\x_{m-1})$.
    \item Solve the line search to find the best step length $a_m$,
        \begin{equation*}
            a_m=\argmin_{a}\x_{m-1}+a \g_{m-1}.
        \end{equation*}
    \item The step in iteration $m$ becomes $\h_m=a\cdot\g_{m-1}$.
    \item Let $\x_m=\x_{m-1}+\h_{m-1}$.
    \item Increase $m$, and go to step 2. Repeat until $m=M$.
    \item The resulting minimum point is $\x_M=\x_0+\sum_{m=1}^M\h_m(\x_m)$.
\end{enumerate}
\end{algorithm}
The gradient descent algorithm is surprisingly robust. Even though it may converge to a local minimum, it often seems to find good solutions globally. This is likely related to research which has found that in high-dimensional spaces, most minima are not minima, but in fact, saddlepoints masquerading as local minima \citep{saddlepoints}. This means that training will slow since the gradient will be small at this saddlepoint or plateau. When using a gradient descent method typically one sets a threshold at which the algorithm terminates when the gradient becomes smaller than the threshold. However if powering through the saddlepoint, then the multivariate gradient descent search should be able to continue digging downwards from these points.

\subsection{Description of gradient boosting}
Now, consider the problem of finding the additive predictor \eqref{eq:gam} which minimizes the empirical risk of a chosen loss function on a data set $D=\{x_i,y_i\}_{i=1}^N$:
\begin{equation}
    \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(\y^{(i)},f(\x^{(i)}).
\end{equation}
The gradient boosting algorithm is gradient descent in the functional parameter space spanned by the base learners \citep{friedman2001}.
Friedman starts with suggesting a nonparametric approach. In this case, we consider each function value $f(x)$ to be a parameter, and then seek to minimize the empirical risk \eqref{eq:empirical-risk-2}, as above. In function space, there are an infinite number of such parameters, since the space in which $x$ exists is continuous. However, for our realized data set, there are only a finite number of such parameters, since we have $N$ values of $\hat{f}(x_i),i=1,\ldots,N$. We can then use the gradient descent algorithm as inspiration, and we take the solution $f(\cdot)$ to be a sum
\begin{equation}
    f(\cdot)=f^{[0]}(\cdot)+\sum_{m=1}^M f^{[m]}(\cdot),
\end{equation}
where the first $f^{[0]}(\cdot)$ is an initial guess, and the remaining $\{f^{[m]}(\cdot)\}_{m=1}^M$ are incremental functions -- steps, or boosts -- defined by the optimization method.

To use gradient descent on our objective function, the empirical risk, we need to compute its negative gradient, which we denote $\u$. There are two ways to arrive at it. One is to calculate the partial derivatives of the empirical risk \eqref{eq:empirical-risk-2}, with respect to each estimated function value $\hat{f}^{[m-1]}(x_i)$:
\begin{align}
    \u&=-\left(\frac{\partial}{\partial f(x_1)}R(f(\x)),\ldots,\frac{\partial}{\partial f(x_n)}R(f(x_n))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\sum_{i=1}^N\rho(y_i, f(x_i)),\ldots,\frac{\partial}{\partial f(x_n)}\sum_{i=1}^N\rho(y_i, f(x_i))\right) \\
    &=-\left(\frac{\partial}{\partial f(x_1)}\rho(y_1, f(x_1)),\ldots,\frac{\partial}{\partial f(x_n)}\rho(y_n, f(x_n))\right)
\end{align}
We see that each element in this vector $\u$ consists of the partial derivative of the loss function, with respect to each sample. However these partial derivatives are equal except for the $y_i$'s and the $\hat{f}(x_i)$'s. In other words, we can simplify the notation as
\begin{equation}
    \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}(x)}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}_{m-1}}\right)_{i=1}^N
\end{equation}
This $\u^{[m-1]}$ is a vector of \textit{generalized residuals}. We could also have arrived at it by simply taking the derivative of the loss function $\rho(y,\hat{f}(x))$ with respect to $\hat{f}$, and made the vector by plugging in each sample.

With the generalized residuals in hand, we should now be able to minimize the loss function by performing gradient descent. However, this nonparametric approach of simply reducing the error of each data point will not generalize, because we are only looking at the observed data points, and not at neighboring points in $\setX$ space. We must therefore impose smoothness to neighboring points. But most importantly, we in any case wish to have an interpretable model. So we choose a base learner
\begin{equation}
    h(\cdot),
\end{equation}
like discussed previously. These base learners are usually relatively simple parametric effects of $\beta$. Again, typical examples are linear least squares, stumps (trees with one split; see \citet{buhlmann2007} and \citet{ESL}), and splines with a few degrees of freedom. The reason we use simple models as base learners is that often algorithms exist for very fast computation of estimates, and there is also less to gain through combining complex learners.

We start with an initial guess $f_0(\cdot)$, say, a constant. Then we iterate, let us say, at each step $m$ ($m>0$) first calculating the generalized residuals from the previous iteration,
\begin{equation}
    \u^{[m-1]}=\left(-\frac{\partial}{\partial \hat{f}(x)}\rho(y_i, \hat{f}(x))\big\rvert_{\hat{f}=\hat{f}^{[m-1]}}\right)_{i=1}^N,
\end{equation}
where we insert the model from the previous step, $\hat{f}^{[m-1]}$. We wish to improve on these generalized residuals as much as possible, therefore we use a gradient descent step, but constrained to the base learner $h(\cdot)$. The function that we wish to choose, which minimizes the residuals, is the member of the base learner class that produces the $\hat{h}^{[m]}$ which is \textit{most parallel} to $\u^{[m-1]}$, i.e. the $h$ that is most correlated with $\u^{[m-1]}$ over the data distribution. This means that this $\hat{h}^{[m]}$ is an approximation of the generalized residuals $\u^{[m-1]}$, or, a projection of the generalized residuals onto the space spanned by the base learner function class. We obtain that $\hat{h}_m$ by fitting the base learner $h(\cdot)$ to the generalized residuals. The method of fitting depends on the basel earner. If, for example, the base learner is ordinary least squares, then this will be $\hat{h}^{[m]}=\left(\u^{[m-1]}\right)^T\hat{\bbeta}^{[m]}$, where
\begin{equation}
    \hat{\bbeta}^{[m]}=\left(\left(\u^{[m-1]}\right)^T\u^{[m-1]}\right)^{-1}\left(\u^{[m-1]}\right)^T\y.
\end{equation}
Having estimated the base learner, we do a line search to find the appropriate step length to use in order to minimize the loss function the most,
\begin{equation}
    a^{[m]}=\argmin_{a}R(\hat{f}^{[m-1]}+a^{[m]}\cdot\hat{h}^{[m]}{\cdot}.
\end{equation}
We add the estimated learner times the step length to the current model, obtaining
\begin{equation}
    f^{[m]}(\cdot)\gets f^{[m-1]}(\cdot)+a^{[m]}\hat{h}^{[m]}(\cdot).
\end{equation}
We iterate this procedure until some stopping criterion.

We are using gradient descent to find the parameters in each iteration, i.e., to find each . In other words, doing gradient descent in parameter space . So boosting can be viewed as an optimization procedure in functional space. For a schematic overview, see Algorithm \ref{algo:fgd}.
\begin{algorithm}
\caption{Gradient boosting, or, generic Functional Gradient Descent (FGD)}
\label{algo:fgd}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n\rho(y^{(i)},f(x^{(i)}).
        \end{equation}
    \item Set iteration counter $m$ to 0. Initialize the additive predictor by setting $f_0(\cdot)$ to a constant $c$. One option is to find the the best constant my numerical maximization, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Specify a base learner $h$.
    \item Increase $m$ by 1.
    \item Compute the generalized residuals (the negative gradient vector) of the previous iteration,
        \begin{equation}
            \u^{[m-1]}=\left(-\frac{\partial}{\partial f}\rho(y_i, f(x_i))\big\rvert_{f=\hat{f}^{[m-1]}}\right)_{i=1}^N
        \end{equation}
    \item Fit base learner $h$ to the generalized residuals $\u$ to obtain a fitted version $\hat{h}^{[m]}$.
    \item Find best step length for $a^{[m]}$ by a line search:
        \begin{equation*}
            a^{[m]}=\argmin_{a}R\left(\hat{f}^{[m-1]}+a\cdot\hat{h}^{[m]}{\cdot}\right).
        \end{equation*}
    \item Update $f^{[m]}(\cdot)\gets f^{[m-1]}(\cdot)+a^{[m]}\cdot \hat{h}^{[m]}(\cdot)$.
    \item Repeat steps 4 to 8 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}^{[M]}(\cdot)=\sum_{m=0}^{M}f^{[m]}(\cdot)$.
\end{enumerate}
\end{algorithm}

\subsection{Step length}
In the original generic functional gradient descent algorithm, the step length $a_m$ for each iteration is found by a line search.
\citet{friedman2001} says that fitting the data too closely may be counterproductive, and result in overfitting. To combat the overfitting, one constrains the fitting procedure. This constraint is called regularization. Friedman therefore, later in the paper, proposes to regularize each step in the algorithm by a common learning rate, $0<\nu\leq1$. Another natural way to regularize would have been to control the number of terms in the expansion, i.e., number of iterations, $M$. However, it has often been found that regularization through shrinkage provides superior results. (Copas 1983)\todo{find citation?} 

As we will see, most modern boosting algorithms omit the step of the line search to find $a_m$, but instead always uses a learning rate/step length $\nu$.  
The choice of this step length is not of critical importance as long as it is sufficiently small \citep{schmid-hothorn}, i.e., with sufficient shrinkage, but the convention is to use $\nu=0.1$ \citep{mayr14a}. This reduces the complexity of the algorithm, and makes the number of parameters to estimate lower. There will of course be a tradeoff between the number of iterations $M$ and the size of the step length $\nu$, which is another reason to use the conventional step length each time.

\subsection{Number of iterations}\label{subsec:iterations}
With a fixed step length (learning rate), the main tuning parameter for gradient boosting is the number of iterations $M$ that are performed before the algorithm is stopped. If $M$ is too small, the model will underfit and it cannot fully incorporate the influence of the effects on the response and will consequently have poor performance. On the other hand, too many iterations will result in overfitting, leading to poor generalization.

\subsection{Practical considerations}
When boosting, one must (or should) center and scale the matrix $X$.

\section{likelihood-based boosting}
Lorem ipsum. \citep{DeBin2016}

\section{$L_2$Boost}
With the generic functional gradient boosting algorithm \eqref{algo:fgd}, it is quite straightforward to derive specific algorithms to use for specific models: It is just a matter of plugging in a chosen loss function. This gives great flexibility.

In the original paper \citep{friedman2001}, he derived such an algorithm for the standard regression setting, which he called $L_2$Boost. $L_2$Boost is a computationally simple variant of boosting, constructed from a functional gradient descent algorithm of the $L_2$ loss function,
\begin{equation*}
    \rho(y, \hat{y})=\frac{1}{2}(y-\hat{y})^2.
\end{equation*}
The reason it is simple is that the generalized residual $u_i$ of an observation $y_i,x_i$, i.e., the negative derivative of the loss function with regard to an estimate $\hat{y}_i=\hat{f}(x_i)$, is
\begin{equation*}
    -\frac{\partial}{\partial\hat{y}}\rho(y_i, \hat{y}_i)=y_i-\hat{y}_i,
\end{equation*}
that is, the so-called residual. The negative gradient vector $\u$ then becomes simply the residual vector,
\begin{equation*}
    \frac{\partial\loss(y,f(\x))}{\partial x_i}=(y-f(x_i)),\quad i=1,\dotsc,n,
\end{equation*}
and hence the boosting steps become repeated refitting of residuals \citep{friedman2001,buhlmann-yu}. With $M=2$ iterations, this had in fact been proposed already, under the name of ``twicing'' \citep{tukey}. See Algorithm \ref{algo:L2} for an overview of the algorithm. Note that we here use the algorithm given in \citet{buhlmann-yu}, who do not use a step length, i.e., they let $\nu_m=\nu=1$ for all iterations $m=1,\ldots,M$.
\begin{algorithm}
\caption{$L_2$Boost}
\label{algo:L2}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$. Set the loss function $\rho(y,\hat{f}(x))=\frac{1}{2}(y-\hat{f}(x))^2$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f)=\argmin_{f}\sum_{i=1}^n y_i-\hat{f}(x_i).
        \end{equation}
    \item Set $m=0$. Initialize $f_0(\x)$, e.g., by setting it to zero for all components, or by finding the best constant, i.e.,
        \begin{equation}
            f_0(\cdot)=\argmin_c R(c).
        \end{equation}
    \item Let the base learner class $h$ be the least squares model, i.e.,
        \begin{equation}
            h(\x, \bbeta)=\x^T\bbeta=\sum_{j=1}^p\beta_jx_j
        \end{equation}
    \item Increase $m$ by 1.
    \item Compute the negative gradient vector, i.e., the residuals, with the model evaluated at the previous estimate
        \begin{equation}
            \u^{[m-1]}=\left(y_i-\hat{f}(x_i)\right)_{i=1}^N
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\x_i,u_i^{[m-1]})$ using the base learner $h$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i^{[m-1]},h(\x_i;\bbeta))
        \end{equation*}
        This estimation can be viewed as an approximation of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the base learner.
    \item Update $f_m(\cdot)=f_{m-1}(\cdot)+h(\cdot;\bbeta_m)$.
    \item Repeat steps 4 to 7 (inclusive) until $m=M$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}
They also prove some nice important theoretical results for L2Boost.
\todo[inline]{more on L2Boost!!}
\subsection{$L_2$Boost example}
Lorem ipsum.

\section{High dimensions and component-wise gradient boosting}\label{sec:component}
% add this to component-wise
In modern biomedical statistics, it is crucial to be able to handle high-dimensional data. In some situations, a data set consists of more predictors $p$ than observations $N$. When $p$ is much larger than $N$ ($p>>N$), we talk about high-dimensional settings. In order to address the issue of analyzing high-dimensional data sets, a variety of regression techniques have been developed over the past years. Many of these techniques are characterized by a built-in mechanism for ``regularization'', which means that shrinkage of coefficient estimates or selection of relevant predictors is carried out simultaneously with the estimation of the model parameters. Both shrinkage and variable selection will typically improve prediction accuracy: In case of shrinkage, coefficient estimates tend to have a slightly increased bias but a decreased variance, while in case of variable selection, overfitting the data is avoided by selecting only the most informative predictors. For instance in the $L_2$Boost algorithm, if one uses a least squares base learner which uses all $p$ dimensions, we see that it is infeasible: The matrix which must be inverted is singular when the number of predictors $p$ is larger than the number of observations $N$. For other models, it might be possible to estimate parameters for each predictor, but it would very easily result in overfitting. If, for instance, the data set input $\X$ consists of gene expressions, it is obvious that the response variable $y$ is not dependent on every single gene.
%Note that regularization is not only useful in the high-dimensional data setting, but also tends to improve prediction accuracy in low-dimensional settings where $p\leq N$, where $N$ is the number of observations in the data set, and $p$ the number of predictors, i.e., columns in $\X$. With such data sets, it will be infeasible to use a base learner $h$ which incorporates all $p$ dimensions of the observation matrix $\X$. 

\subsection{Stagewise, not stepwise}
Component-wise gradient boosting is an algorithm which works very well in these settings. In fact, Buhlmann believes that it is mainly in the case of high-dimensional predictors that boosting has a substantial advantage over classical approaches \citep{buhlmann2006}.
The component-wise approach was first proposed in the $L_2$Boost paper \citep{buhlmann-yu}, and is very much an active field of research \citep{buhlmann2006, mayr14a, mayr14b, mayr17}.
In the gradient boosting algorithm described in algorithm \eqref{algo:fgd}, we start out with an additive predictor $f^{[0]}(\cdot)$ which only consists of a constant. We have not added any effects of covariates yet. Instead of adding a small effect from all predictors, the component-wise approach is to add only one variable at a time. This is similar to the typical statistical model selection regime of forward stepwise model selection. In forward stepwise, we will iterate in the following manner. We start with an empty set of predictors, or covariates, and look at each separately. Looking at each separately means adding it to the set of predictors and estimating a model with those predictors. Then we add, to the set of predictors, that predictor which gives the best improvement to the objective function. We repeat this in each step, but we estimate the entire model in each step. The main idea of component-wise gradient boosting is to do this, except in a stagewise manner. This means that we do not change the added parameters, but we only estimate the next one.

The structure of the component-wise boosting algorithm is very much the same as the generic functional gradient boosting algorithm \eqref{algo:fgd}, but with some additional steps. Instead of using one base learner which incorporates all predictors, one uses a set $\mathcal{H}$ of base learners, where all base learners are univariate. Typically these base learner have the same structure, but each uses its own covariate. For example, if we use a linear least squares model as base learners, the set of base learners would be
\begin{equation}
    \mathcal{H}=\{h_1(\x;\beta_1)=\beta_1 x_1,h_2(\x;\beta_2)=\beta_2 x_2,\ldots,h_p(\x;\beta_p)=\beta_p x_p \}.
\end{equation}


The initialization is the same as in the FGD algorithm. We first initialize the additive predictor $f^{[0]}(\cdot)$ to a constant $\beta_0$, and then specify a base learner. Now we specify a base learner for each component, as explained above. We then iterate. In an iteration $m$, we first calculate the generalized residuals by calculating the negative gradient where we insert the additive predictor from the previous step,
\begin{equation}
    \u^{[m-1]}=\left(-\frac{\partial}{\partial f}\rho(y_i, f(x))\big\rvert_{f=\hat{f}^{[m-1]}}\right)_{i=1}^N.
\end{equation}
Note that this is exactly like in the generic FGD algorithm. While the generic FGD algorithm here only estimated a single base learner, in the component-wise we now estimate all base learners separately, obtaining
\begin{equation}
    \hat{h}_1^{[m]}(\cdot),\hat{h}_2^{[m]}(\cdot),\ldots,\hat{h}_p^{[m]}(\cdot),
\end{equation}
These estimated learners can be viewed as approximations of the negative gradient vector, and as the projection of the negative gradient vector onto the space spanned by the component-wise base learner. We select the best-fitting base-learner $h_{j^{[m]}}^{[m]}$ based on the residual sum of squares of the base-learner fit
\begin{equation}
    j^{[m]}=\argmin_{j\in\{1,2,\ldots,p\}}\sum_{i=1}^N \left(u_i-\hat{h}_j^{[m]}\right)^2.
\end{equation}
We add this best-fitting base-learner to the current model, with a pre-specified step length of $\nu$. Hence the model after iteration $m$ is
\begin{equation}
    f^{[m]}(\cdot)\gets f^{[m-1]}(\cdot)+\nu\cdot\hat{h}_{j^{[m]}}.
\end{equation}
We iterate until the iteration number $m$ becomes the pre-specified stopping iteration $m_{\text{stop}}$. The final model then becomes
\begin{equation}
    \hat{f}=f^{[m_{\text{stop}}]}(\cdot)=\beta_0 + \sum_{m=1}^{m_{\text{stop}}}\nu\cdot\hat{h}_{j^{[m]}}^{[m]}(\cdot).
\end{equation}
Note that any base-learner $h_j$ can be selected at multiple iterations. The partial effect of the variable $x_j$ is the sum of the estimated corresponding base learner in all iterations where it was selected, i.e.,
\begin{equation*}
    f_j(x_j)=\sum_{m=1}^{m_{\text{stop}}}\nu\cdot\hat{h}_j^{[m]}(x_j)\indicator\left(j^{[m]}=j\right),
\end{equation*}
hence the resulting additive predictor is a sum of component-wise predictors in the GAM form of
\begin{equation}
    f(\x)=\beta_0+\sum_{j=1}^p f_j(x_j).
\end{equation}
For a schematic overview of the algorithm, see Algorithm \ref{algo:component-wise}.
\begin{algorithm}
\caption{Component-wise gradient boosting}\label{algo:component-wise}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,f(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set iteration counter $m$ to 0. Initialize the additive predictor to an offset by setting $f_0(\cdot)$ to a constant $\beta_0$. One option is to find the the best constant by numerical maximization, i.e.,
        \begin{equation}
            \beta_0=\argmin_c R(c).
        \end{equation}
    \item Specify a set of base learners $\mathcal{H}=\{h_1(\cdot),\dotsc,h_p(\cdot)\}$, where each $h_j$ is univariate and takes column $j$ of $\X$.
    \item\label{first-step} Increase $m$ by 1.
    \item Compute the negative gradient vector, i.e., the generalized residuals after the previous iteration of the boosted model,
        \begin{equation}
            \u^{[m-1]}=\left(-\frac{\partial}{\partial f}\rho(y_i, f(x))\big\rvert_{f=\hat{f}^{[m-1]}}\right)_{i=1}^N.
        \end{equation}
    \item For each base learner $h_j\in\mathcal{H},j=1,\ldots,p$, estimate $\hat{h}_{j}^{[m]}$ by fitting $(\X_i,u_i)$ using the base learner $h_j(\cdot)$. We obtain
        \begin{equation}
            \hat{h}_1^{[m]}(\cdot),\hat{h}_2^{[m]}(\cdot),\ldots,\hat{h}_p^{[m]}(\cdot).
        \end{equation}
    \item Select the best-fitting component $j^{[m]}$, i.e., with lowest RSS,
        \begin{equation}
            j^{[m]}=\argmin_{j\in\{1,2,\ldots,p\}}\sum_{i=1}^N \left(u_i-\hat{h}_j^{[m]}\right)^2.
        \end{equation}
    \item\label{last-step} Update the current model with the best-fitting model from the current iteration
        \begin{equation}
            \hat{f}^{[m]}(\cdot)\gets \hat{f}^{[m-1]}(\cdot)+\nu\cdot \hat{h}_{j^{[m]}}^{[m]}(\cdot).
        \end{equation}
    \item Repeat steps \ref{first-step} to \ref{last-step} (inclusive) until $m=m_{\text{stop}}$.
    \item Return the final boosted additive predictor
        \begin{equation}
            \hat{f}(\cdot)=\hat{f}^{[m_{\text{stop}}]}(\cdot)=\beta_0+\sum_{m=1}^{m_{\text{stop}}}+\hat{h}_{j^{[m]}}^{[m]}(\cdot)
        \end{equation}
\end{enumerate}
\end{algorithm}

\section{Boosting performs data-driven variable selection}
If the number of iterations $M$ is not very large compared to the number of variables, the component-wise gradient boosting algorithm will carry out automatic variable selection. What this means is that based on their explanatory power, the base learners applied to irrelevant variables will never be added into the model, and therefore many of the columns of $\X$ will not be a part of the final model. Some predictors will have more explanatory power, or signal, than others, and so they will be selected more than once. This is because some predictors are more correlated with the output than others. Therefore some components will lead to better improvements and those corresponding base learners will thus be more frequently selected. Therefore the component-wise boosting algorithm has inherent variable selection.

\section{Selecting $\mstop$}
As we mentioned in subsection \ref{subsec:iterations}, the crucial tuning parameter in boosting is the number of iterations, $\mstop$. Stopping early enough performs variable selection and shrinks the parameter estimates toward zero. In the case of $p<N$, with $m\to\infty$, the parameters in boosting will converge towards the maximum likelihood estimates \citep{DeBin2016}, i.e., maximizing the in-sample error. We are, on the other hand, after all interested in minimizing out-of-sample prediction error (PE). The prediction error for a given data set is a function of the boosting iteration $m$. What we want is therefore a good method for approximating $\PE(m)$. This can be done in a number of ways. Many authors state that the algorithm should be stopped early, but do not go further into the details here. Common model selection criteria such as the Akaike Information Criteria (AIC) may be used, however the AIC is dependant on estimates of the model's degrees of freedom. Methods by \citet{chang2010} try this. This is problematic for several reasons. For $\text{L}_2\text{Boost}$, \citet{buhlmann2007} suggest that $\df(m)=\trace(B_m)$ is a good approximation. Here $B_m$ is the hat matrix resulting from the boosting algorithm. This was, however, shown by \citet{hastie2007} to always underestimate the actual degrees of freedom. \citet{mayr-hofner} propose a sequential stopping rule using subsampling. However this is computationally very expensive and not really used in practice. Instead, cross-validation, a very common method for selection of tuning parameters in statistics, is what is used in almost all cases, both in practice and in research.\todo{citation?} Cross-validation is flexible and easy to implement. It is somewhat computationally demanding, because it requires several full runs of the boosting algorithm.

\subsection{Other selection methods}
The number of iterations in the boosting procedure, $M$, is a tuning parameter. It acts as a regularizer. AIC, etc.

\subsection{K-fold cross-validation}\label{subsec:K-fold}
K-fold cross-validation \citep{lachenbruch}, or simply cross-validation, is a general method commonly used for selection of penalty or tuning parameters. We will use it to approximate the prediction error. In cross-validation, the data is split randomly into K rougly equally sized folds. For a given fold $k$, all folds except $k$ act as the training data in estimating the model. We often say that the $k$-th fold is left out. The resulting model is then evaluated on the unseen data, namely the observations belonging to fold $k$. This procedure is repeated for all $k=1,\ldots,K$. An estimate for the prediction error is obtained by averaging over the test errors evaluated in each left-out fold. Let $\kappa(k)$ be the set of indices for fold $k$. The cross-validated estimate for a given $m$ then becomes
\begin{equation}
    \CV(m)=\sum_{k=1}^K\sum_{i\in\kappa(k)}\rho(y_i,\hat{y}_i^{-\kappa(k)}).
\end{equation}
For each $m$, we calculate the estimate of the cross-validated prediction error $\CV(m)$. We choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\CV(m).
\end{equation}
Typical values for $K$ are 5 or 10, but in theory one can choose any number. The extreme case is $K=N$, called leave-one-out cross-validation, where all but one observation is used for training and the model is evaluated on the observation that was left out. In this case, the outcome is deterministic, since there is no randomness when dividing into folds.

\subsection{Stratified cross-validation}
When dividing an already small number of survival data observations into $K$ folds, we might risk getting folds without any observed deaths, or in any case, very few. In stratified cross validation, we do not divide the folds entirely at random, but rather, try to divide the data such that there is an equal amount of censored data in each fold.
As before, let $\kappa(k)$ be the set of indices for fold $k$. Divide the observed data into $K$ folds, as with usual cross validation, to get an index set $\kappa_{\delta=1}(k)$ for a given $k$. Similarly, divide the censored data into $K$ folds, obtaining $\kappa_{\delta=0}(k)$. Finally, $\kappa(k)$ is the union of these sets: $\kappa(k)=\kappa_{\delta=1}(k)\cup\kappa_{\delta=0}(k)$.
For a detailed description of 10-fold cross-validation issues in the presence of censored data, see \citet{kohavi}.

\subsection{Repeated cross-validation}
The randomness inherent in the cross-validation splits has an effect on the resulting $\mstop$. This is true for boosting in general, but it is true for real-life survival data, especially. In typical survival time data sets one typically has a small effective sample size (number of observed events). We can easily imagine that for two different splits of the data, we can end up with quite different values for $\mstop$.
It has been very effectively demonstrated that the split of the folds has a large impact on the choice of $\mstop$ \citep{seibold}. \citet{seibold} suggest simply repeating the cross-validation scheme. They show that repeating even 5 times effectively averages out the randomness.  In other words, we divide the data into $K$ folds, and repeat this $J$ times. Now let $\kappa(j, k)$ be the $k$-th fold in the $j$-th split. We end up with a new estimate for the prediction error,
\begin{equation}
    \RCV(m)=\sum_{j=1}^J\sum_{k=1}^K\sum_{i\in\kappa(j,k)}\rho(y_i,\hat{y}_i^{-\kappa(j,k)}).
\end{equation}
As before, we choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\RCV(m).
\end{equation}
In practice, to ensure we find the minimizing $m$, we let the boosting algorithm run for $m=1$ to $m=M$, where $M$ is a large number that we are sure will result in a overfitted model.

\section{Multidimensional boosting: Cyclical component-wise}
A limitation of all the classical boosting methods we have described earlier, as well as of $L_1$-penalized estimation such as the lasso method \citep{lasso}, is that they are designed for statistical problems involving a one-dimensional prediction function. By only considering such functions, we are restricted to estimating models which only model a single quantity of interest, which is almost always the mean.
In many applications, modelling only one parameter will not be sufficient. We want to be able to estimate more general models, in which more quantities, e.g. the drift and the threshold of the models described in section \ref{sec:FHT}, can be explained by covariates. Typical examples of multidimensional estimation problems are classification with multiple outcome categories and regression models for count data. Another example is estimating models in the GAMLSS family \citep{gamlss}. GAMLSS, which refer to ``generalized additive models for location, scale and shape,'' are a modelling technique that relates not only the mean, but all parameters of the outcome distribution to the available covariates. GAMLSS are an extension of GAM models \citep{gam-book}. A gradient boosting algorithm called gamboostLSS was developed for boosting such models \citep{gamboostlss-paper}. The algorithm framework used in gamboostLSS is an example of the multidimensional boosting algorithm first introduced in \citet{schmid}. We will here explain the algorithm used in this paper.

\citet{schmid} extend the component-wise gradient boosting algorithm \citep{friedman2001} to such a setting where one has a prediction function $f$ which takes $K$ parameters:
\begin{equation}
    f(\btheta)=f(\theta_1,\theta_2,\ldots,\theta_K).
\end{equation}
Like in regular statistical boosting, we must be able to obtain derivatives. Therefore, it is assumed that all partial derivatives
\begin{equation}
    \frac{\partial}{\theta_1}f(\btheta),\frac{\partial}{\theta_2}f(\btheta),\ldots,\frac{\partial}{\theta_K}f(\btheta)
\end{equation}
exist. The main idea of the cyclical multidimensional boosting algorithm, although cyclical was a term coined later \citep{thomas2018}, is to have a boosting step for each component $k=1,2,\ldots,K$, in each iteration. In other words, we cycle through all parameter dimensions in each boosting iteration. The initialization of the algorithm is done analogously to the regular boosting method, by setting each parameter to a constant, typically zero or the constant which maximizes the maximum likelihood. We also specify a base learner for each parameter dimension. In a given boosting iteration $m+1$, the algorithm cycles through the different parameter dimensions $k$. In every dimension $k$, we carry out one boosting iteration. This boosting iteration is essentially the same as in the usual component-wise gradient boosting algorithm \eqref{algo:component-wise}. At this point, the estimated parameter vector is
\begin{equation*}
    \hat{\btheta}_{k-1}^{[m+1]}=\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\ldots,\hat{\theta}_{k-1}^{[m+1]},\hat{\theta}_{k}^{[m]},\hat{\theta}_{k+1}^{[m]},\ldots,\hat{\theta}_{K}^{[m]}\right),
\end{equation*}
meaning all parameter dimensions $1,2,\ldots,k-1$ have been updated in the current iteration $m+1$. The following dimensions $k+1,\ldots,K$ have not, and we are now going to update dimension $k$. We calculate a residual for dimension $k$ by calculating the $k$-th partial derivative and inserting the current estimated parameter vector $\hat{\btheta}_{k-1}^{[m+1]}$, and evaluating it at the observations $x_1,x_2,\ldots,x_N$. This yields residual vector
\begin{align*}
\u_k&=(u_{k,1},u_{k,2},\ldots,u_{k,N})\\
&=\left(-\frac{\partial}{\partial \theta_k}f(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\ldots,\hat{\theta}_{k-1}^{[m+1]},\hat{\theta}_{k+1}^{[m]},\ldots,\hat{\theta}_{K}^{[m]})\right)_{i=1}^N.
\end{align*}
Again, like in a regular component-wise boosting algorithm, we fit all component-wise base learners separately to this residual vector $\u_k$. Of these learners, select the best fitting component $j^*$ and corresponding estimated learner $\hat{h}_{j^*}(\cdot)$, and update the parameter in dimension $k$ by the usual
\begin{equation}
    \theta_k^{[m+1]}=\theta_k^{[m]}+\nu\cdot \hat{h}_{j^*}(\cdot).
\end{equation}
A schematic representation of the updating process using this algorithm in a given iteration $m+1$ looks as follows:
\begin{align*}
    \frac{\partial}{\theta_1}\rho\left(\hat{\theta}_1^{[m]},\hat{\theta}_2^{[m]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{1}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{1}^{[m+1]} \\
    \frac{\partial}{\theta_2}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}} \u_{2}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{2}^{[m+1]} \\
    \frac{\partial}{\theta_3}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{3}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{3}^{[m+1]} \\
    \ldots\\
    \frac{\partial}{\theta_{K-1}}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m+1]},\ldots,\hat{\theta}_{K-1}^{[m]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{K-1}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{K-1}^{[m+1]} \\
    \frac{\partial}{\theta_K}\rho\left(\hat{\theta}_1^{[m+1]},\hat{\theta}_2^{[m+1]},\hat{\theta}_3^{[m+1]},\ldots,\hat{\theta}_{K-1}^{[m+1]},\hat{\theta}_K^{[m]}\right)
    \xlongrightarrow{\text{calculate}}\u_{K}
    \xlongrightarrow{\text{fit and update}}\hat{\theta}_{K}^{[m+1]}
\end{align*}
After cycling through all estimation parameters in $\rho$, we find the current optimal nuisance parameters, if the loss function has any nuisance parameters. A nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest. For example, if we are considering a multivariate normal distribution and are only interested in the means, the residual variance would be a nuisance parameter. We find the optimal nuisance parameters by performing numerical minimization of the empirical risk for one scale parameter at a time.
For each nuisance parameter $\sigma_l, l=1,2,\ldots,L$, we plug all other estimated parameters into the empirical risk, and minimize it over the current nuisance parameter $\sigma_l$:
\begin{equation}
    \hat{\sigma_l}^{[m+1]}=\argmin_{\sigma_l} \sum_{i=1}^N \rho(y_i,\hat{f}^{[m+1]},\hat{\sigma}_1^{[m+1]},\hat{\sigma}_2^{[m+1]},\ldots,
    \hat{\sigma}_{l-1}^{[m+1]},\sigma_l,\hat{\sigma}_{l+1}^{[m]},\ldots,\hat{\sigma}_L^{[m]})
\end{equation}
The gamboostLSS algorithm \citep{gamboostlss-paper} does not include this step this because GAMLSS do not have any nuisance parameters.
\begin{algorithm}
\caption{Multidimensional cyclical component-wise gradient boosting}
\label{algo:multi-cyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{[0]}_1,f^{[0]}_2,\ldots,f^{[0]}_K,$, e.g., by setting them to zero for all components, or by finding the best constant by a numerical maximum likelihood method. Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1. If $m>m_{\text{stop},k}$, go to step X. Otherwise compute the negative partial derivative
        $-\frac{\partial\rho}{\partial \hat{f}_k}$ and evaluate at $\hat{f}^{[m-1]}(x_i),i=1,\ldots,N$, yielding the
        negative gradient vector
        \begin{equation}
            \u^{[m-1]}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{[m-1]}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{[m-1]}_k$.
    \item Select the component of $X$ which best fits $\u^{(m-1)}_k$ according to a pre-specified goodness-of-fit, typically RSS.
        Set $\hat{\u}^{[m-1]}_k$ equal to the fitted values of the corresponding best model fitted in step X.
    \item Update $\hat{f}_k{[m-1]}\gets\hat{f}_k^{[m-1]}+\nu\hat{U}_k^{[m-1]},$ where $\nu$ is a pre-specified real-valued step-length factor.
    \item Repeat steps 6 until 9 for $k=2,\ldots,K$. 
    \item Finally, update $\hat{f}^{[m]}\gets\hat{f}^{[m-1]}$.
    \item Set $l=0$.
    \item Increase $l$ by 1.
    \item Plug $\hat{f}^{(m)}$ and $\hat{\sigma}_1^{[m-1]},\ldots,\hat{\sigma}_{l-1}^{[m-1]},\hat{\sigma}_{l+1}^{[m-1]},\hat{\sigma}_{L}^{[m-1]}$ into the empirical risk function $R$ and minimize the empirical risk over $\sigma_l$. Set $\hat{\sigma}_l^{[m]}$ equal that minimizer.
    \item Repeat steps 13 and 14 for $l=2,\ldots,L$.
    \item Repeat steps 4 to 15 until $m=\max_k(m_{\text{stop},k})$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}

See \eqref{algo:multi-cyclical} for a schematic overview of the algorithm. Note that this algorithm resembles the backfitting strategy by \citet{hastie1986}. In both strategies, components are updated successively by using estimates of the other components as offset values. In backfitting, a completely new estimate of $f^*$ is determined in every iteration, but in gradient boosting, the estimates are only slightly modified in each iteration.

The main tuning parameters in this algorithm are the stopping iterations $\mathbf{m}_{\text{stop}}=m_{\text{stop},1},\ldots,m_{\text{stop},K}$. As in the one-dimensional gradient boosting algorithm, \citet{schmid} say that it should not run until convergence, but rather find estimates by cross-validation. An issue with this algorithm, though, is that for proper tuning it requires a vector $\mathbf{m}_{\text{stop}}$ of stopping iterations, one for each prediction parameter. To properly tune these parameters, it is necessary to perform what is often called a grid search, i.e., do a multidimensional search of the parameters. This should be done by cross-validation, as usual, and the next subsection explains the procedure.

\subsection{Grid search cross-validation}\label{grid-search}
To find a vector of length $K$ of optimal iterations $\mathbf{m}_{\text{stop}}=m_{\text{stop},1},\ldots,m_{\text{stop},K}$, we perform a $K$-dimensional grid search. We must first specify a minimum and maximum number of iterations for each parameter. Call these $m_{\min,k}$ and $m_{\max,k}$, respectively. We then divide this one-dimensional search space into a finite grid with $N_k$ points, such that we obtain
\begin{equation} 
    m_{\min,k}=m_{1,k}<m_{2,k}<\ldots<m_{N_k-1,k}<m_{N_k,k}=m_{\max,k},
\end{equation}
again for each $k=1,2,\ldots,K$. The total search space is the cartesian product of all of these grids. We illustrate with an example. Let $K=3$, and $m_{\min,k}=1$ and $m_{\max,k}=10$ for all $k$, and finally divide each grid into 10 points, i.e., $N_1=N_2=N_3=10$. The total search grid will consist of $N_1\cdot N_2\cdot N_3=10^3=10000$ tuples of configurations of $\mathbf{m}$, enumerated below:
\begin{align*}
    \left(m_{1,1},m_{1,2},m_{1,3}\right) \\
    \left(m_{1,1},m_{1,2},m_{2,3}\right) \\
    \ldots \\
    \left(m_{1,1},m_{1,2},m_{10,3}\right) \\
    \left(m_{1,1},m_{2,2},m_{1,3}\right) \\
    \ldots \\
    \left(m_{1,1},m_{2,2},m_{10,3}\right) \\
    \ldots \\
    \left(m_{10,1},m_{10,2},m_{10,3}\right).
\end{align*}
We want to find the best configuration $\mathbf{m}$, i.e., we want to find the optimum of the hyperplane $CV(\mathbf{m})$. Like in subsection \ref{subsec:K-fold}, we must calculate the estimate of the cross-validated prediction error for each given configuration $\mathbf{m}$, obtaining the prediction error $CV(\mathbf{m})$. We choose $\mathbf{m}_{\text{stop}}$ to be the minimizer of this error,
\begin{equation*}
    \mathbf{m}_{\text{stop}}=\argmin_{\mathbf{m}}\CV(\mathbf{m}).
\end{equation*}
Using boosting, we may obtain estimates of $\CV(\mathbf{m})$ for all $\CV(\mathbf{m})$ by fixing all but one of the parameters and perform a typical boosting run. If we fix all but one of the parameters in the vector $\mathbf{m}=\left(m_{i_1,1},m_{i_2,2},m_{i_3,3}\right)$, where $m_{\min,k}\leq i_k\leq m_{\max,k}$ for all $k=1,2,3$, say, we fix $m_{i_1,1}$ and $m_{i_2,2}$. This is due to the way boosting algorithms work, since for any given iteration $M$, we also automatically obtain all boosted estimates for all iterations less than $M$, if we have saved the boosted parameters for each iteration. Consider again the example. We now let the first two parameters in the example be fixed for each boosting run. While the search grid consist of $N_1\cdot N_2\cdot N_3$ tuples, considering the first two parameters as fixed, we only need to do $N_1\cdot N_2$ boosting runs, and in each run set the maximum number of possible iterations in the boosting algorithm for the third component to be $m_{\max,3}$. This means that we consider all configurations of the first two parameters in $\mathbf{m}$, i.e.,
\begin{align*}
    \left(m_{1,1},m_{1,2}\right) \\
    \left(m_{1,1},m_{2,2}\right) \\
    \ldots \\
    \left(m_{1,1},m_{10,2}\right) \\
    \left(m_{2,1},m_{1,2}\right) \\
    \ldots \\
    \left(m_{2,1},m_{10,2}\right) \\
    \ldots \\
    \left(m_{10,2},m_{10,2}\right),
\end{align*}
and do a boosting run for each such. Like in subsection \ref{subsec:K-fold}, we choose
\begin{equation*}
    \mathbf{m}_{\text{stop}}=\argmin_{\mathbf{m}}\CV(\mathbf{m}),
\end{equation*}
where
\begin{equation*}
    \CV(\mathbf{m})=\sum_{k=1}^K\sum_{i\in\kappa(k)}\rho(y_i,\hat{y}_i^{-\kappa(k)}),
\end{equation*}
i.e., the cross-validated prediction error, as usual.
\section{Noncyclical component-wise multidimensional boosting algorithm}
In the cyclical algorithm seen previously, algorithm \ref{algo:multi-cyclical}, the different $m_{\text{stop},j}$ parameters are not independent of each other, and hence they have to be jointly optimized. As we saw in the previous subsection \ref{grid-search}, the usually applied \textit{grid search} for such parameters scales exponentially with the number of parameters $K$. This can quickly become very demanding computationally. \citet{thomas2018} develop a new algorithm for fitting GAMLSS models, instead of the cyclical one used in gamboostLSS. In this new algorithm, which they call ``noncyclical,'' only one scalar tuning parameter $m_{\text{stop}}$ is needed because only one parameter is chosen in each boosting iteration. Compared to the cyclical algorithm in gamboostLSS \citep{gamboostlss-paper}, this noncyclical algorithm obtains faster variable tuning and equal prediction results on simulation studies carried out \citep{thomas2018}.

\subsection{Gradients are not comparable across parameters}
In the cyclical algorithm, we always boost all parameters in the same iteration. Therefore we do not need to choose between parameters. If we want to avoid having a separate tuning parameter for each parameter that we are boosting, however, it is necessary to choose one parameter to boost in each iteration. To do this we have to choose between parameters, and so we need to be able to find out which parameter would lead to the best increase in performance. We already do this for choosing which component-wise learner to use in each parameter. There, we choose that which has the best residual-sum-of-squares (RSS), with respect to the negative gradient vector. \citet{thomas2018} denote this the \textit{inner loss}.\todo[inline]{Add empirical proof?}However, in general these residual vectors are not comparable across parameters of the loss function, because the parameters have different scales \citep{thomas2018}. In a normal distribution, for example, the partial derivatives for the mean and the partial derivative for the standard deviation will not be comparable. Therefore, to compare between parameters, a different comparison method is needed. We cannot compare the RSS'es, because it will not tell us which parameter will decrease the loss function the most.

For each parameter $\theta_k$, we choose the component-wise base learner which best fits according to the RSS,
\begin{equation}
    \hat{h}_{\theta_k}(\cdot).
\end{equation}
If we incorporate this estimated base learner into the full boosted model, we would get
\begin{equation}
    f^{[m+1]}_{\theta_k}=f^{[m]}+\nu\cdot\hat{h}_{\theta_k}(\cdot).
\end{equation}
We can insert this proposed new model into the loss function to obtain a new empirical risk value,
\begin{equation}
    R(f^{[m+1]}_{\theta_k}).
\end{equation}
We calculate the gain in the loss function, which we denote $\Delta\rho_{\theta_k}$, by
\begin{equation}
    \Delta\rho_{\theta_k}=R(f^{[m]})-R(f^{[m+1]}_{\theta_k}).
\end{equation}
If we now compare the gain in loss function value, $\rho_{\theta_k}$, across each parameter $k=1,2,\ldots,K$, we can find out which parameter leads to the best increase. We choose that one, i.e.,
\begin{equation}
    k^*=\argmin_k\Delta\rho_{\theta_k},
\end{equation}
and incorporate only the best-fitting learner corresponding for that parameter into the full boosting model, i.e.,
\begin{equation}
    f^{[m+1]}\gets f^{[m+1]}_{\theta_k}=f^{[m]}+\nu\cdot\hat{h}_{\theta_k}(\cdot).
\end{equation}
We choose the base learner for each component by comparing their residual sum of squares with respect to the negative gradient vector, which we called the inner loss. In other words, we use the usual procedure of choosing the component-wise learner for each parameter which minimizes the RSS, i.e.,
\begin{equation}
    j^*=\argmin_{j} \sum_{i=1}^N (u_{k,i}-(f^{[m]}(x_i)+\hat{h}(\x_i)))^2.
\end{equation}
This is, however, not the same criterion that is used to choose between parameters. This might be problematic. Therefore, \citet{thomas2018} propose using the loss function $\rho$ for choosing between component-wise learners as well. They call this the ``outer loss.'' In that case, we instead choose the component-wise learner for each parameter which minimizes the outer loss function, i.e., 
\begin{equation}
    j^*=\argmin_{j} \sum_{i=1}^N \rho(y_i,f(x_i)).
\end{equation}
The individual component-wise learners are still estimated by their usual method, i.e., calculating the negative gradient of the generalized residuals and using the base learner to estimate models. E.g., by linear least squares if using simple linear regression base learners. The improvement in the empirical risk, $\Delta\rho_{\theta_k}$, is then calculated for each base learner of every distribution parameter, and only the overall best-performing base learner with regard to the outer loss is updated.

In both cases of this algorithm, we have the advantage that the optimal number of boosting steps, $m_{\text{stop}}$, is always a scalar value. Finding this tuning parameter can be done fairly quickly with standard cross validation schemes, and most importantly, it scales with with the number of parameters. This is unlike the cyclical algorithm, which needs a multidimensional grid search.

\begin{algorithm}
\caption{Multidimensional noncyclical component-wise gradient boosting}
\label{algo:multi-noncyclical}
\begin{enumerate}
    \item Start with a data set $D=\{x_i, y_i\}_{i=1}^N$ and a chosen loss function $\rho(y,\hat{f}(x))$, for which we wish to
        minimize the empirical risk, i.e., the loss function evaluated on the samples,
        \begin{equation}
            \hat{f}=\argmin_{f}R(f).
        \end{equation}
    \item Set $m=0$. Initialize $f^{(0)}_1,f^{(0)}_2,\ldots,f^{(0)}_K,$, e.g., by setting it to zero for all components, or by finding the best constant.% Initialize scale (nuisance) parametes $\sigma$.
    \item Specify a base learner $h_k$ for each dimension $k=1,\ldots,K$.
    \item Increase $m$ by 1.
    \item Set $k=0$.
    \item Increase $k$ by 1.
    \item Compute the negative partial derivative $-\frac{\partial\rho}{\partial \hat{f}_k}$
        and evaluate at $\hat{f}^{(m-1)}(x_i),i=1,\ldots,N$, yielding negative gradient vector
        \begin{equation}
            \u^{(m-1)}_k=\left(-\frac{\partial}{\partial \hat{f}_k}\rho(y_i, \hat{f}^{(m-1)}(x_i))\right)_{i=1}^N
        \end{equation}
    \item Fit the negative gradient vector to each of the $p$ components of $X$ (i.e. to each base learner) separately, using the base learners specified in step X. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $\u^{(m-1)}_k$.
    \item Select the best fitting base learner, $h_{kj}$, either by
        \begin{itemize}
            \item the inner loss, i.e., the RSS of the base-learner fit w.r.t the negative gradient vector
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N(u_k^{(i)}-\hat{h}_{kj}(x^{(i)}))^2
                \end{equation}
            \item the outer loss, i.e., the loss function after the potential update,
                \begin{equation}
                    j^*=\argmin_{j\in 1,\ldots,J_k}\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj}(x^{(i)}) \right)
                \end{equation}
        \end{itemize}
    \item Compute the possible improvement of this update regarding the outer loss,
        \begin{equation}
            \Delta\rho_k=\sum_{i=1}^N\rho\left(y^{(i)}, \hat{f}^{(m-1)}(x^{(i)}) + \nu \cdot \hat{h}_{kj^*}(x^{(i)}) \right)
        \end{equation}
    \item Update, depending on the value of the loss reduction, $k^*=\argmin_{k\in1,\ldots,K}\Delta\rho_k$
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}+\nu\cdot\hat{h}_{k^*j^*}(x),
        \end{equation}
        while for all $k\neq k^*$,
        \begin{equation}
            \hat{f}^{(m)}_{k^*}=\hat{f}^{(m-1)}_{k^*}.
        \end{equation}
    \item Repeat steps 4 to 11 until $m=m_{\text{stop}}$.
    \item Return $\hat{f}(\cdot)=\hat{f}_M(\cdot)=\sum_{m=0}^Mf_m(\cdot)$.
\end{enumerate}
\end{algorithm}