\chapter{Statistical boosting}

\section{Statistical learning theory}\label{sec:learning-theory}
Assume we have a joint distribution $(\X, Y)$, $X\in\R^p$ and $Y\in\R$, and $Y=F(\X)+\eps$, $F(\x)=\E(Y|\X=\x)$ We wish to estimate the underlying $F(\X)$. For an estimate $\hat{F}(\cdot)$, we measure the loss, or the difference, with a loss function
\begin{equation*}
    \loss(Y, \hat{F}(\X)).
\end{equation*}
A common loss function for regression is the squared loss, also known as the $L_2$ norm,
\begin{equation*}
    \loss(Y, \hat{F}(\X))=(Y-\hat{F}(\X))^2.
\end{equation*}
For a $\hat{F}(X)$, we wish to estimate the expected loss, also known as the generalization or test error,
\begin{equation*}
    \text{Err}_{\tau}=\E[\loss(Y,\hat{F}(\X))|\tau],
\end{equation*}
where $(X,Y)$ is drawn randomly from their joint distribution and the training set $\tau$ is held fixed. It is infeasible to do effectively in practice \todo{because?} and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_\tau]=\E_\tau\left([\loss(Y,\hat{F}(\X))|\tau]\right),
\end{equation}
i.e., average over many different test sets.
In practice, we observe a sample $(\x_i,y_i)_{i=1}^N$. For this sample, we can calculate the training error,
\begin{equation}\label{eq:empirical-risk}
    \err(F)=\frac{1}{N}\sum_{i=1}^N\loss(y_i, \hat{F}(\x_i)),
\end{equation}
also known as the empirical risk. To estimate $\text{err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. We then train/fit/estimate based on the other 80\%, and estimate $\text{Err}$ by
\begin{equation*}
    \hat{\text{Err}}=\frac{1}{M}\sum_{i=1}^ML(y_i,\hat{F}(\x_i)),
\end{equation*}
where $(x_i,y_i)$ here are from the test set.

\section{The history of boosting}\label{history}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}). Boosting originated in the fields of computational learning theory and machine learning. In 1989 Kearns and Valiant, working on computational learning theory, posed a question: Could any weak learner be transformed to become also a strong learner \citep{kearnsvaliant}. A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. Freund and Schapire invented the AdaBoost algorithm in 2006 for binary classification \citep{adaboost}. It was evidence that the answer to the original question was positive. The AdaBoost algorithm performs iterative reweighting of original observations. For each iteration, it gives more weight to misclassified observations, and then trains a new weak learner based on all weighted observations. It then adds the new weak learner to the final classifier. The resulting AdaBoost classifier is then a linear combination of these weak classifiers, i.e., a weighted majority vote. In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. In statistics, however, we are interested in models which are interpretable.

\section{Statistical boosting}\label{sec:sboost}
In statistics, we are not just interested in prediction accuracy. We also want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation}\label{eq:exp-f}
    \E(Y|\X=\x)=F(\x).
\end{equation}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings, and we therefore extend our discussion to a more general regression scheme where the outcome variable $Y$ can be continuous. We are interested in interpreting the effects of the different covariates of $\X$ on the function $F(\cdot)$. A model for $F(\X)$ which is amenable to such interpretation is the generalized additive model (GAM),
\begin{equation}\label{eq:gam}
    F(\x)=\alpha+\sum_{j=1}^pf_j(x_j),
\end{equation}
where $\alpha\in\R$ is an offset and $x_j$ is the j-th component of $\x$. $F(\x)$ is a sum of component-wise functions $f_j$, and as such a GAM contains interpretable additive predictors. In 2000, Friedman et al. showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function \citep{friedman2000}. This provided a way of viewing the successful boosting regime through a statistical lens. A year later, Friedman himself made a powerful insight for boosting. However, we must first discuss how we find an approximate solution for $F(\X)$ in \eqref{eq:exp-f}.

\section{Finding a solution}
We wish to estimate/approximate/find $F(\X)$ in \eqref{eq:exp-f}, so we are interested in solving
\begin{equation*}
    \argmin_{F}\E\left[\loss(Y,F(\X))\right],
\end{equation*}
where $\loss$ is an appropriate loss function. But as discussed in chapter \ref{sec:learning-theory} on \nameref{ch:learning-theory}, we have in practice observed a dataset $\{\x_i,y_i\}_{i=1}^N$, drawn from the joint distribution $(\X,Y)$. Therefore we substitute the loss expected with the expected risk \eqref{eq:empirical-risk}, and so we want a solution to
\begin{equation}\label{eq:argmin-risk}
    \argmin_{F}\err(F).
\end{equation}
Finding such an $F$ is not easy. We will now discuss a general optimization algorithm often used for this purpose.

\subsection{Gradient descent}
Gradient descent is an optimization algorithm for a differentiable multivariate function $F$. The motivation behind the gradient descent algorithm is that in a small interval around a point $\x$, $F$ is increasing in the direction of the negative gradient at $\x$. Therefore, by moving slightly in that direction, $F$ will increase. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit to a local optimum. More formally, the algorithm is
\begin{enumerate}
    \item Start with an initial guess $\x_0$, e.g. $\x_0=\0$. Let $m=1$.
    \item Calculate the gradient $\g_{m-1}=-\nabla F(\x_{m-1})$.
    \item Let $\h_m=\nu \g_{m-1}$, where $\nu$ is a small step length.
    \item Let $\x_m=\x_{m-1}+\h_{m-1}$
    \item Increase $m$, and go to step 2. Repeat until $m=M$.
    \item The resulting final guess is $\x_M=\x_0+\sum_{m=1}^M\h_m(\x_m)$
\end{enumerate}
Back to our goal of finding an $F$ to minimize \eqref{eq:argmin-risk}. Often we choose a parameterized model $F(\X;\bbeta)$. Finding the optimal $\bbeta$ analytically might be infeasible. The gradient descent algorithm can then be used. In this case, we fix $\X$ and let $F(\X)$ in the algorithm be $F(\bbeta;\X)$. Thus we use gradient descent to find an optimal $\bbeta$. We would then say we are doing gradient descent in parameter space. We are now ready to reveal Friedman's useful insight.

\section{Gradient boosting: Functional gradient descent}
There is another possible way to use gradient descent, and that is the important insight by Friedman in 2001 \citep{friedman2001}. He showed that instead of doing gradient descent in parameter space, one could do gradient descent in function space. Briefly, we first describe a naive way of doing this. Consider the function value at each $\x$ directly as a parameter, and use gradient descent directly on these parameters. However, this does not generalize to unobserved values $\X$, and we are after all interested in the population minimizer of \eqref{eq:exp-f}. We can instead assume a parameterized form for $F$, e.g.,
\begin{equation}\label{eq:gradboost}
    F(\X;\{\bbeta\}_{m=1}^M)=\sum_{m=1}^M\nu H(\X;\bbeta_m),
\end{equation}
where $H(\X;\bbeta)$ is also a function on the GAM form \eqref{eq:gam}, but typically simpler, i.e., a base learner as discussed previously. We would like to minimize a data based estimate of the loss, i.e. the empirical risk, and so would choose $\{\bbeta_m\}$ as the minimizers of 
\begin{equation*}
    \argmin_{\{\bbeta_m\}_{m=1}^M}\err(H(\x;\{\bbeta_m\})).
\end{equation*}
However, estimating these simultaneously may be infeasible. We can then use a greedy stagewise approach, where we at each step $m$ choose the $\bbeta_m$ which gives the best improvement, while not changing any of the previous $\{\bbeta\}_{k=1}^{m-1}$. Hence at each step $m$ the current solution is
\begin{equation*}
    F_{m}=F_{m-1}+\nu H(\x;\bbeta_m),
\end{equation*}
where the parameters $\bbeta_m$ are those in $H$ minimizing the empirical risk when added to the fixed part $F_{m-1}$:
\begin{equation*}
    \bbeta_m=\argmin_{\bbeta}\err(H(\x;\bbeta_k)+H(\x;\bbeta)).
\end{equation*}
The final model is then the sum of these terms, like in \eqref{eq:gradboost}. To find $\bbeta_m$ in each step here, we use gradient descent. We have outlined a generic functional gradient descent algorithm. It can be stated more formally as follows.
\begin{enumerate}
    \item Initialize $F_0(\x)$, e.g., by setting it to zero for all components. Select a base learner $H$.
    \item Compute the negative gradient vector,
        \begin{equation*}
            U_i=-\frac{\partial \loss(y_i,F_{m-1}(\cdot))}{\partial F_{m-1}(\cdot)},\quad i=1,\dotsc,N.
        \end{equation*}
    \item Estimate $\hat{H}_m$ by fitting $(\X_i,U_i)$ using the base learner $H$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i,H(\x_i;\bbeta))
        \end{equation*}
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu H(\cdot;\bbeta_m)$.
    \item Repeat steps from 2 until $m=M$.
\end{enumerate}
Note that while we call this functional gradient descent, this is exactly the gradient boosting algorithm.
\section{L2Boost}
In 2003, Buhlmann and Yu improve upon Friedman's work, and develop the L2Boost algorithm for which they also prove some important theoretical results \citep{buhlmann-yu}. It is a special case of the generic functional gradient descent (FGD) algorithm, where we choose the squared error loss to be the loss function,
\begin{equation*}
    \loss(y,F(\x))=\frac{1}{2}\left(y-F(\x)\right)^2.
\end{equation*}
The negative gradient vector of the loss then becomes the residual vector,
\begin{equation*}
    \frac{\partial\loss(y,F(\x))}{\partial x_i}=(y-F(x_i)),\quad i=1,\dotsc,n,
\end{equation*}
and hence the boosting steps become repeated refitting of residuals \citep{friedman2001,buhlmann-yu}. With $\nu=1$ and $M=2$, this had been proposed already in \citeyear{tukey} by \citeauthor{tukey}, who called it ``twicing'' \citep{tukey}.
\begin{enumerate}
    \item Initialize $F_0(\x)$, e.g., by setting it to zero for all components. Select a base learner $H$, such as ordinary least squares, stumps, etc.
    \item Compute the residuals
        \begin{equation*}
            U_i=(y_i,F_{m-1}(x_i)),\quad i=1,\dotsc,n
        \end{equation*}
    \item Estimate $\hat{H}_m$ by fitting $(\X_i,U_i)_{i=1}^N$ using the base learner $H$:
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i,H(\x_i;\bbeta))
        \end{equation*}
        Note that $\hat{H}(\cdot;\bbeta_m)$ is then an estimate of the negative gradient vector.
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu H(\cdot;\bbeta_m)$.
    \item Repeat steps from 2 until $m=M$..
\end{enumerate}
\section{Component-wise gradient boosting}
In high-dimensional settings, it might often be infeasible, if not impossible, to use a base learner $H$ which incorporates all $p$ dimensions. Indeed, using least squares base learners, it is impossible, since the matrix which must be inverted is singular when $p>N$. Component-wise gradient boosting is a technique/algorithm which does work in these settings. It was developed by Yu and Buhlmann \citep{buhlmann-yu}, and it has further been refined and explored, e.g. in \cite{buhlmann2006}. The idea of the algorithm is to select $p$ base learners. Each of these is only a function of the corresponding component of the data $\X$, i.e.,
\begin{equation*}
    h_j(\x)=h_j(x_j).
\end{equation*}
In each iteration, we fit all these learners separately, and choose only the one which gives the best improvement to be added in the final model. The resulting model $F_m(\cdot)$ is then a sum of componentwise effects,
\begin{equation*}
    F_m(\X)=\sum_{j=1}^pf_j(x_j),
\end{equation*}
where
\begin{equation*}
    f_j(x_j)=\sum_{m=1}^M\1_{mj}h_j(x_j;\bbeta_m),
\end{equation*}
where $\1_{mj}$ is an indicator function which is 1 if component $j$ was selected at iteration $m$ and 0 if not.
Hence this model is a GAM \eqref{eq:gam}. Crucially, if we stop sufficiently early, we will typically perform variable selection. It is likely that some base learners have never been added to the final model, and as such those components in $\X$ are not added. We now give a presentation of the algorithm.
\begin{enumerate}
    \item Start with an initial guess, e.g. $F_0=\0$.\\
    Specify a set of base learners $h_1(\cdot),\dotsc,h_p(\cdot)$.
    \item Compute the negative gradient vector $\U$.
    \item Fit $(\X_i,U_i)_{i=1}^N$ separately to every base learner to get $\hat{h}_1(x_1),\dotsc,\hat{h}_p(x_p)$.
    \item Select the component $k$ which best fits the negative gradient vector.
        \begin{equation*}
            k=\argmin_{j\in[1,p]}\sum_{i=1}^N(u_i-\hat{h}_j(\x_i))^2
        \end{equation*}
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu h_k(x_k)$
\end{enumerate}
In fact, Buhlmann believes that it is mainly in the case of high-dimensional predictors that boosting has a substantial advantage over classical approaches \citep{buhlmann2006}.

\section{The importance of stopping early}
The number of iterations in the boosting procedure, $M$, is a tuning parameter. It acts as a regularizer.