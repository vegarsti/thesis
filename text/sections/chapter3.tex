\chapter{Statistical boosting}
\subsection{Boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades. (\cite{mayr14a}) The history of boosting started with the question posed in 1989 by Kearns and Valiant, working on computational learning theory, of whether any weak learner could be transformed to become also a strong learner. (\cite{kearnsvaliant}) A weak classifier is in general defined to be one which is only slightly better than random choice. For regression, it is a bit harder to give a specific definition, but a weak regressor is simple and low dimensional, and does not pick up much of the underlying signal. The answer to the original question is yes, and Schapire and Freund showed this with the AdaBoost algorithm, which constructs a binary classifier. (\cite{adaboost}) The algorithm works by iteratively reweighting observations, giving more weight to misclassified observations, and training a new base learner on all observations, using the updated weights. The resulting AdaBoost classifier is a linear combination of these base classifiers. In its original formulation, the classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm.

\section{Statistical boosting}\label{sec:sboost}
In statistics, however, we are interested in models which are interpretable. We want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation*}
    \E(Y|X=x)=f(x).
\end{equation*}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings. We therefore extend our discussion to the more general regression scheme, where the outcome variable $Y$ can be continuous. To evaluate a candidate $\fh(x)$, we need to see how well it estimates $f(x)$. This is typically done by choosing a loss function,
\begin{equation}\label{eq:loss}
    \loss(Y, F(X)),
\end{equation}
and calculating the empirical risk, i.e., the average in-sample error over some observed test data set. A typical loss function for regression is the $L_2$ loss,
\begin{equation*}
    \loss(Y, f(X))=(Y-f(X))^2
\end{equation*}
The empirical risk is then
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n (y_i-\fh(x_i))^2
\end{equation*}
A possible model for $f(x)$ is the generalized additive model (GAM), in which different effects of single predictors are added,
\begin{equation}\label{eq:gam}
    f(x)=\beta_0+\sum_{i=1}^p h_p(x_p).
\end{equation}
In 2000, Friedman showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function. (\cite{friedman2000}) This provided a way of viewing the successful boosting regime through a statistical lens.

\subsection{Gradient boosting}
Gradient boosting was proposed in 2001 (\cite{friedman2001}), and further refined in 2003 (\cite{buhlmann-yu}). We will here explain the gradient boosting framework, starting with the motivation in the gradient descent algorithm.

\subsubsection{Gradient descent}
Gradient descent, or steepest descent, is a greedy iterative algorithm for numerically maximizing an objective function $F$. At each iteration step it improves on the previous solution by going in the direction which increases the objective function the most. This is by definition \todo{??} the direction of the gradient. For our purposes, the objective function is the negative loss function, since we are \textit{minimizing} the loss. To avoid going too far, i.e., beyond the closest local optima, we choose a small step size $\nu$, and it has been shown empirically that $\nu=0.1$ is a good choice (\cite{buhlmann-yu}, \cite{buhlmann2006}). The gradient descent search stops when reaching a maximum, possibly a local maximum. The result of the search after $M$ steps is a sequence of improvements, or boosts, on the objective function.

\subsubsection{Setting}
Assume we have data $\X\in\R^p$ and $Y\in\R$ with some relation $Y=F(\X)$, $F\colon\R^p\to\R$, which we wish to estimate. We have the relationship
\begin{equation*}
    Y=F(\X)+\varepsilon,
\end{equation*}
where $\varepsilon$ is a random variable with expectation zero. We wish to find a $\hat{F}(\cdot)$ which minimizes the expected loss between it and the distribution $Y=F(\cdot)$,
\begin{equation}\label{eq:min-loss}
    F^*=\min\E_{Y,\X}\left[\loss(Y,\hat{F}(\X))\right]=\min\E_Y\left\{\E_\X\left[\loss(Y,\hat{F}(\x))|\X=\x\right]\right\},
\end{equation}
where $\loss$ is some meaningful loss function which measures the difference between $Y$ and $F(\X)$. In particular, a loss function is 0 if $Y$ is exactly equal $F(\X)$, and positive otherwise. To estimate $F$ we typically choose a parameterized model,
\begin{equation}
    F(\X)=H(\X;\bgamma),
\end{equation}
where $H:\R^p\to\R$ is some function of parameters $\bgamma$, which are to be estimated. In other words, we are trying to solve
\begin{equation}\label{eq:}
    \bgamma^*=\argmin_{\bgamma}\E_Y\left\{\E_\X\left[\loss(Y,H(\x;\bgamma))|\X=\x\right]\right\}.
\end{equation}
In practice, for finite observed data points $\{\x_i,y_i\}_{i=1}^N$, we must estimate the expected loss \eqref{eq:min-loss} by the empirical risk,
\begin{equation}\label{eq:emp-risk}
    \hat{L}(y,h(x;\bgamma))=\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma)).
\end{equation}
For finite data points and a chosen such model $H$, there exists a $\bgamma^*$ which minimizes \eqref{eq:min-loss}\todo{citation needed?},
\begin{equation}\label{eq:min-loss-param}
    \bgamma^*=\min_{\bgamma} R(H;\bgamma)=\min_{\bgamma}\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma))
\end{equation}
but estimating this is not necessarily easy. An algorithm which is often used to find an approximate solution $\hat{\bgamma}$ to \ref{eq:min-loss-param} is gradient descent in parameter space.

\subsubsection{Gradient descent in parameter space}
At iteration step $m$ we find an improvement $\bbeta_m$, and the current solution at this step is $\bgamma_m=\sum_{j=0}^m\bbeta_j$. We start with an initial guess $\bbeta_0$, say $\bbeta_0=0$. We then carry out steps $m=1,\dotsc,M$, where we find increments $\bbeta_m$ which improve our existing solution $\bgamma_{m-1}$, using gradient descent on the parameters. Hence at each iteration, we compute the gradient of the loss with respect to the parameters, evaluated at the current solution,
\begin{equation}
    \mathbf{g}_m=\{g_{jm}\}=\left\{\frac{\partial}{\partial\gamma_{m-i,j}}\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma_{m-1}))\right\}_{j=1}^{p}.
\end{equation}
We choose $\bbeta_m=\nu\g_m$, for step size $\nu$, and iterate.

\subsubsection*{Gradient descent in function space}
We have until now viewed the function estimating problem as the problem of minimizing a function by optimizing its parameters, i.e., doing gradient descent in parameter space. We can instead view the optimization problem \eqref{eq:min-loss} from a non-parametric perspective, by considering $F(\x)$ at each point $x$ as a parameter. We seek to find
\begin{equation}\label{eq:min-loss-func}
    F^*=\argmin_{F}\E_Y\left\{\E_\X\left[\loss(Y,F(\x))|\X=\x\right]\right\}
\end{equation}
In function space there are in theory an infinite number of such $F$. But in data sets there are only a finite number of possibilities $F(x_i)$. We can also here use gradient descent.

\subsubsection{Additive basis expansion}
However, we would like to estimate function values at other values of $\x$ than that which we have observed. We can then assume a parameterized form such as
\begin{equation}
    F(\x;\left\{\bbeta_m\right\}_{m=1}^M)=\sum_{m=1}\bbeta_mH(\x;\bbeta_m)
\end{equation}
Minimizing this may be infeasible. In such situations one can try a stagewise approach, at each iteration $m$ choosing $\H(\x;\bbeta)$ such that it gives the best improvement. 
Following the numerical optimization paradigm as above, we take the solution $F^*$ to \eqref{eq:min-loss-func} to be the sum of a sequence of improvements, or boosts,
\begin{equation}
    F^*=\sum_{m=0}^MH_m.
\end{equation}