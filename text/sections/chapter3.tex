\chapter{Statistical boosting}
\subsection{Boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades. (\cite{mayr14a}) The history of boosting started with the question posed in 1989 by Kearns and Valiant, working on computational learning theory, of whether any weak learner could be transformed to become also a strong learner. (\cite{kearnsvaliant}) A weak classifier is in general defined to be one which is only slightly better than random choice. For regression, it is a bit harder to give a specific definition, but a weak regressor is simple and low dimensional, and does not pick up much of the underlying signal. The answer to the original question is yes, and Schapire and Freund showed this with the AdaBoost algorithm, which constructs a binary classifier. (\cite{adaboost}) The algorithm works by iteratively reweighting observations, giving more weight to misclassified observations, and training a new base learner on all observations, using the updated weights. The resulting AdaBoost classifier is a linear combination of these base classifiers, i.e., a weighted majority vote. In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm.

\section{Statistical boosting}\label{sec:sboost}
In statistics, however, we are interested in models which are interpretable. We want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation*}
    \E(Y|X=x)=f(x).
\end{equation*}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings. We therefore extend our discussion to the more general regression scheme, where the outcome variable $Y$ can be continuous. To evaluate a candidate $\fh(x)$, we need to see how well it estimates $f(x)$. This is typically done by choosing a loss function,
\begin{equation}\label{eq:loss}
    \loss(Y, F(X)),
\end{equation}
and calculating the empirical risk, i.e., the average in-sample error over some observed test data set. A typical loss function for regression is the $L_2$ loss,
\begin{equation*}
    \loss(Y, f(X))=(Y-f(X))^2
\end{equation*}
The empirical risk is then
\begin{equation*}
    \frac{1}{n}\sum_{i=1}^n (y_i-\fh(x_i))^2
\end{equation*}
A possible model for $f(x)$ is the generalized additive model (GAM), in which different effects of single predictors are added,
\begin{equation}\label{eq:gam}
    f(x)=\beta_0+\sum_{i=1}^p h_p(x_p).
\end{equation}
In 2000, Friedman showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function. (\cite{friedman2000}) This provided a way of viewing the successful boosting regime through a statistical lens.

\subsection{Gradient boosting}
Gradient boosting was proposed in 2001 (\cite{friedman2001}), and further refined in 2003 (\cite{buhlmann-yu}). We will here explain the gradient boosting framework, starting with the motivation in the gradient descent algorithm.

\subsubsection{Gradient descent}
Gradient descent, or steepest descent, is a greedy iterative algorithm for numerically maximizing a differentiable objective function $F$. At each iteration step it improves on the previous solution by going in the direction which increases the objective function the most. This is by definition the direction of the gradient. For our purposes, the objective function is the negative loss function, since we are \textit{minimizing} the loss. To avoid going too far, i.e., beyond the closest local optima, we choose a small step size $\nu$, and it has been shown empirically that $\nu=0.1$ is a good choice (\cite{buhlmann-yu}, \cite{buhlmann2006}). The gradient descent search stops when reaching a maximum, possibly a local maximum. The result of the search after $M$ steps is a sequence of improvements, or boosts, on the objective function.

\subsubsection{Setting}
Assume we have data $\X\in\R^p$ and $Y\in\R$ with some relation $Y=F(\X)$, $F\colon\R^p\to\R$, which we wish to estimate. We have the relationship
\begin{equation*}
    Y=F(\X)+\varepsilon,
\end{equation*}
where $\varepsilon$ is a random variable with expectation zero. We wish to find a $\hat{F}(\cdot)$ which minimizes the expected loss between it and the distribution $Y=F(\cdot)$,
\begin{equation}\label{eq:min-loss}
    F^*=\min\E_{Y,\X}\left[\loss(Y,\hat{F}(\X))\right]=\min\E_Y\left\{\E_\X\left[\loss(Y,\hat{F}(\x))|\X=\x\right]\right\},
\end{equation}
where $\loss$ is some meaningful loss function which measures the difference between $Y$ and $F(\X)$. In particular, a loss function is 0 if $Y$ is exactly equal $F(\X)$, and positive otherwise. To estimate $F$ we typically choose a parameterized model,
\begin{equation}
    F(\X)=H(\X;\bgamma),
\end{equation}
where $H:\R^p\to\R$ is some function of parameters $\bgamma$, which are to be estimated. In other words, we are trying to solve
\begin{equation}\label{eq:}
    \bgamma^*=\argmin_{\bgamma}\E_Y\left\{\E_\X\left[\loss(Y,H(\x;\bgamma))|\X=\x\right]\right\}.
\end{equation}
In practice, for finite observed data points $\{\x_i,y_i\}_{i=1}^N$, we must estimate the expected loss \eqref{eq:min-loss} by the empirical risk,
\begin{equation}\label{eq:emp-risk}
    \hat{L}(y,h(x;\bgamma))=\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma)).
\end{equation}
For finite data points and a chosen such model $H$, there exists a $\bgamma^*$ which minimizes \eqref{eq:min-loss}\todo{citation needed?},
\begin{equation}\label{eq:min-loss-param}
    \bgamma^*=\min_{\bgamma} R(H;\bgamma)=\min_{\bgamma}\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma))
\end{equation}
but estimating this is not necessarily easy. An algorithm which is often used to find an approximate solution $\hat{\bgamma}$ to \ref{eq:min-loss-param} is gradient descent in parameter space.

\subsubsection{Gradient descent in parameter space}
At iteration step $m$ we find an improvement $\bbeta_m$, and the current solution at this step is $\bgamma_m=\sum_{j=0}^m\bbeta_j$. We start with an initial guess $\bbeta_0$, say $\bbeta_0=\bar{y}$. We then carry out steps $m=1,\dotsc,M$, where we find increments $\bbeta_m$ which improve our existing solution $\bgamma_{m-1}$, using gradient descent on the parameters. Hence at each iteration, we compute the gradient of the loss with respect to the parameters, evaluated at the current solution,
\begin{equation}
    \mathbf{g}_m=\{g_{jm}\}=\left\{\frac{\partial}{\partial\gamma_{m-i,j}}\frac{1}{N}\sum_{i=1}^NL(y_i,H(\x_i;\bgamma_{m-1}))\right\}_{j=1}^{p}.
\end{equation}
We choose $\bbeta_m=\nu\g_m$, for step size $\nu$, and iterate.

\subsubsection*{Gradient descent in function space}
We have until now viewed the function estimating problem as the problem of minimizing a function by optimizing its parameters, i.e., doing gradient descent in parameter space. We can instead view the optimization problem \eqref{eq:min-loss} from a non-parametric perspective, by considering $F(\x)$ at each point $\x$ as a parameter.
But doing this directly is not helpful, for we wish to estimate the underlying function, which takes values different from the observed $\x$'s. We therefore assume a parameterized form such as
\begin{equation}
    F(\cdot;\left\{\bbeta_m\right\}_{m=1}^M)=\sum_{m=1}^M\nu H(\cdot;\bbeta_m),
\end{equation}
where $H$ is a function parameterized by $\bbeta_m$ and again $\nu$ is a (typically small) step size. Hence $F$ is an additive basis expansion, where $H(\cdot;\bbeta_m)$ is the family of basis functions. Minimizing this may be infeasible, since it involves simultaneously optimizing several functions and several parameters. In such situations one can try a stagewise approach, at each iteration $m$ choosing $H(\x;\bbeta)$ such that it gives the best improvement, while not changing the previous $m-1$ functions. Following the numerical optimization paradigm as above, we take our approximate solution $\hat{F}$ to be the sum of a sequence of improvements, or boosts,
\begin{equation}
    \hat{F}_M=\sum_{m=0}^M\nu H_m(\cdot;\bbeta_m).
\end{equation}

\subsection{L$_2$Boost with OLS regression}
Let us look at a more concrete example. Assume data $(\X_i,Y_i)_{i=1}^N$, where $\X_i\in\R^2$. We use the $L_2$ loss function,
\begin{equation}\label{l2}
    L(x,y)=\frac{1}{2}\left(y-f(x)\right)^2,
\end{equation}
such that the derivative of $L$ is simply the residual $(y-f(x))$. We also use basis functions $h(\X;\hat{\bbeta}_m)=\alpha + \bbeta_m^T\X$, i.e., ordinary linear regression. At each step the $\hat{\bbeta}$ are the regular least squares minimizers. With the $L_2$ loss, the gradient boosting algorithm then becomes as follows. Start with an initial guess $h_0(\X,Y)=h(\X,Y;\hat{\bbeta_0})$, i.e., one ordinary least squares regression fit. Then $F_0(\X)=h_0(\X)$. Then, for $m=1,\dotsc,M$, calculate the gradient of the loss function, i.e., the residuals,
\begin{equation}\label{residuals}
    U_i=Y_i-F_{m-1}(\X_i).
\end{equation}
Then fit the base learner $h$ to the residuals, i.e.,
\begin{equation}
    h_m(\X_i,U_i)=\X_i(\X_i^T\X_i)^{-1}\X_i^TU_i,
\end{equation}
and add it to the final model,
\begin{equation}
    F_m=\sum_{j=0}^m h_m.
\end{equation}

\subsection{Component-wise gradient boosting}
If $N < p$, it may be infeasible to use base learners which use all $p$ dimensions. In particular, in OLS regression, we would need to calculate a matrix-vector product, which would be impossible, because it would give a singular matrix. One way to solve this is to use base learners which only incorporate one dimension at a time. \cite{buhlmann-yu} developed this algorithm using component-wise smoothing splines as base learners. They concluded that in high dimensions the $L2$Boost algorithm performs as well as boosted trees. At each iteration, one then finds the best base learner for each dimension, and adds the best of these to the final model. Hence at each iteration, only one (regularized) predictor is added. A clear advantage this is that if one stops boosting early enough, this will often do variable selection (!), meaning many predictors will not be included in the final model $F$.



\section{Statistical learning theory}
Assume we have a joint distribution $(\X, Y)$, $X\in\R^p$ and $Y\in\R$, and $Y=F(\X)+\eps$, $F(\x)=\E(Y|\X=\x)$ We wish to estimate the underlying $F(\X)$. For an estimate $\hat{F}(\cdot)$, we measure the loss, or the difference, with a loss function
\begin{equation}
    \loss(Y, \hat{F}(\X)).
\end{equation}
A common loss function for regression is the squared loss, also known as the $L_2$ norm,
\begin{equation}
    \loss(Y, \hat{F}(\X))=(Y-\hat{F}(\X))^2.
\end{equation}
For a $\hat{F}(X)$, we wish to estimate the expected loss, also known as the generalization or test error,
\begin{equation}
    \text{Err}_{\tau}=\E[\loss(Y,\hat{F}(\X))|\tau],
\end{equation}
where $(X,Y)$ is drawn randomly from their join distribution and the training set $\tau$ is held fixed. This is infeasible to do effectively in practice (because?) and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_\tau]=\E_\tau\left([\loss(Y,\hat{F}(\X))|\tau]\right).
\end{equation}
In practice, we observe a sample $(\x_i,y_i)_{i=1}^N$. For this sample, we can calculate the training error,
\begin{equation}
    \overline{\text{err}}=\frac{1}{N}\sum_{i=1}^N\loss(y_i, \hat{F}(\x_i)),
\end{equation}
also known as the empirical risk. To estimate $\text{err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. We then train/fit/estimate based on the other 80\%, and estimate $\text{Err}$ by
\begin{equation}
    \hat{\text{Err}}=\frac{1}{M}\sum_{i=1}^ML(y_i,\hat{F}(\x_i)),
\end{equation}
where $(x_i,y_i)$ here are from the test set.

\section{Boosting}
We will now discuss boosting which is one of the most promisiing methodological approaches developed in the latest ... years.

AdaBoost...

\subsection{Statistical view of boosting}
In addition to hopefully finding an $\hat{F}(\cdot)$ which minimizes the test/generalization error $\text{Err}$, we are interested in interpreting the effects of the different covariates of $\X$ on the fitted function $\hat{F}(\cdot)$. A model which is amenable to such interpretation is the generalized additive model (GAM),
\begin{equation}
    F(\x)=\alpha+\sum_{j=1}^pf_j(x_j),
\end{equation}
where $x_j$ is the j-th component of $\x$. We see this is a component-wise functon for each component, or the sum of component-wise $f$'s.

We are interested in finding the best $f$,
\begin{equation}
    F^*=\argmin_{F}\text{Err}(f).
\end{equation}

\subsection{Gradient descent}
An optimization algorithm for a differentiable multivariate function $F$. The motivation behind gradient descent is that in a small interval around a point $\x$, $F$ is increasing in the direction of the negative gradient at $\x$. Therefore, by moving slightly in that direction, $F$ will increase. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit to a local optimum. More formally, the algorithm is
\begin{enumerate}
    \item Initialize $x_0$ with an initial guess, e.g. $x_0=0$. Let $m=1$.
%    \item For $m=1,\dotsc,M$, until convergence,
    \item Calculate $-\nabla F({x_{m-1}})=\g_m(\x_{m-1})$.
    \item Let $\x_m=\x_{m-1}+\nu\g(\x_{m-1})$, where $\nu$ is a small step length.
    \item Increase $m$, and go to step 2. Repeat until convergence.
    \item Resulting final guess is $\hat{\x}=\x_0+\nu\sum_{m=1}^M\g_m(\x_m)$
\end{enumerate}

\subsection{Gradient boosting}
The gradient boosting can very well be used to find optimal parameters of a function $H(\X;\bbeta)$, such that in the gradient descent algorithm we use $F(\bbeta)=H(\dotsc;\bbeta)$ and find an optimal $\hat{\bbeta}$. We would then say we are doing gradient descent in parameter space. This is quite possible and a good idea if the optimal parameters of $H$ are hard to find. There is another possible way to use gradient descent, and this algorithm and the important insight therein was worked out by Friedman in 2001. (\cite{friedman2001}) He argued that one could instead do gradient descent in function space. A naive way of doing this is to consider the function value at each $\x$ directly as a parameter. However this does not generalize to unobserved values $\X$. We can instead assume a parameterized form, e.g.,
\begin{equation}\label{eq:gradboost}
    F(\X;\{\bbeta\}_{m=1}^M)=\sum_{m=1}^M\nu H(\X;\bbeta_m).
\end{equation}
We would like to minimize a data based estimate of the expected loss (the empirical risk), and so would choose $\{\bbeta_m\}$ as
\begin{equation}
    \{\bbeta_m\}_{m=1}^M=\argmin_{\bbeta_m^\prime}\sum_{i=1}^N\loss\left(y_i,\nu\sum_{m=1}^Mh(\x;\bbeta_m^\prime)\right).
\end{equation}
However, estimating these simultaneously may be infeasible. We then choose a greedy stagewise approach, at each step $m$ choosing that $\bbeta_m$ which gives the best improvement while not changing any of the previous $\{\bbeta\}_{k=1}^{m-1}$,
\begin{equation}
    \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^NL\left(y_i,\nu\left[\sum_{k=1}^{m-1}h(\x;\bbeta_k)+h(\x;\bbeta)\right]\right),
\end{equation}
and
\begin{equation}
    F_{m}=F_{m-1}+\nu h(\x;\bbeta_m).
\end{equation}
The final model is then the sum of these terms, like in \eqref{eq:gradboost}. To find $\bbeta_m$ in each step here, we might use gradient descent. This, then, is gradient boosting. A generic functional gradient descent is as follows.
\begin{enumerate}
    \item Initialize $F_0(\x)$, e.g., by setting it to zero for all components.
    \item Compute the negative gradient vector,
        \begin{equation}
            U_i=-\frac{\partial \loss(y_i,F_{m-1}(\cdot))}{\partial F_{m-1}(\cdot)}, i=1,\dotsc,N.
        \end{equation}
    \item Estimate $\hat{h}_m$ by fitting $(\X_i,U_i)$ using a base learner $h$:
        \begin{equation}
            \bbeta_m=\argmin{\bbeta}\sum_{i=1}^N\loss(u_i,h(\x_i;\bbeta))
        \end{equation}
        $\hat{h}(\cdot;\bbeta_m)$ is then an estimate of the negative gradient vector (!).
    \item Repeat above steps.
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu h(\cdot;\bbeta_m)$.
\end{enumerate}

\subsection{L2Boost}
L2Boost is an algorithm which was developed by Buhlmann and Yu in 2006 (\cite{buhlmann-yu}), and it is a special case of the generic functional gradient descent (FGD) algorithm. In it, we use the squared error loss as the loss function in the FGD algorithm,
\begin{equation}
    L(y,\hat{F}(\x))=\frac{1}{2}\left(y-\hat{F}(\x)\right)^2
\end{equation}
The negative gradient vector then becomes the residual vector, and hence the boosting steps become repeated refitting of residuals. (\cite{friedman2001}, \cite{buhlmann-yu}). With $\nu=1$ and $M=2$, this had been proposed in 1977 by Tukey, as ``twicing''. (\cite{tukey}).

We will outline the algorithm here.

Gradient boosting is functional gradient descent.

\subsection{Component-wise gradient boosting}
In high-dimensional settings, it might often be infeasible, if not impossible, to use a base learner $h$ which incorporates all $p$ dimensions. Indeed, using least squares base learners, it is impossible, since the matrix which must be inverted is singular when $p>N$. Component-wise gradient boosting is a technique/algorithm which does work in these settings. It was developed by Yu and Buhlmann in the same paper (\cite{buhlmann-yu}), and has further been refined and explored. The idea of the algorithm is to select $p$ base learners. Each of these is only a function of the corresponding component of the data $\X$,
\begin{equation}
    h_j(x_j).
\end{equation}
In each iteration, we fit all these learners separately, and choose only the one which gives the best improvement to be added in the final model. Tje resulting model $F_m(\cdot)$ is then a sum of componentwise effects,
\begin{equation}
    F_m(\X)=\sum_{j=1}^pf_j(x_j),
\end{equation}
where
\begin{equation}
    f_j(x_j)=\sum_{m=1}^Mh_j(x_j;\bbeta_m).
\end{equation}
This model is a GAM. Crucially, if we stop sufficiently early, we will typically perform variable selection. It is likely that some base learners have never been added to the final model, and as such those components in $\X$ are not added. We now give a presentation of the algorithm.
\begin{enumerate}
    \item Initialize. $m=0$, $F_0=\0, e.g.$. Specify a set of base learners $h_1(x_1),\dotsc,h_p(x_p)$.
    \item Compute the negative gradient vector $u$.
    \item Fit $u$ separately to every base learner.
    \item Select component $k$ which best fits the negative gradient vector.
        \begin{equation}
            k=\argmin_{j\in[1,p]}\sum_{i=1}^N(u_i-h_j(x_i))^2
        \end{equation}
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu h_k(x_k)$
\end{enumerate}
In fact, ... and ... argue that boosting is appropriate not in low or medium dimensions, but in high dimensions.