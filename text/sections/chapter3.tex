\chapter{Statistical boosting}

\section{Statistical learning theory}\label{sec:learning-theory}
Assume we have a joint distribution $(\X, Y)$, $X\in\R^p$ and $Y\in\R$, and $Y=F(\X)+\eps$, $F(\x)=\E(Y|\X=\x)$ We wish to estimate the underlying $F(\X)$. For an estimate $\hat{F}(\cdot)$, we measure the loss, or the difference, with a loss function
\begin{equation*}
    \loss(Y, \hat{F}(\X)).
\end{equation*}
A common loss function for regression is the squared loss, also known as the $L_2$ norm,
\begin{equation*}
    \loss(Y, \hat{F}(\X))=(Y-\hat{F}(\X))^2.
\end{equation*}
For a $\hat{F}(X)$, we wish to estimate the expected loss, also known as the generalization or test error,
\begin{equation*}
    \text{Err}_{\tau}=\E[\loss(Y,\hat{F}(\X))|\tau],
\end{equation*}
where $(X,Y)$ is drawn randomly from their joint distribution and the training set $\tau$ is held fixed. It is infeasible to do effectively in practice %\todo{because?}
and hence we must instead estimate the expected prediction error,
\begin{equation}\label{eq:err}
    \text{Err}=\E[\text{Err}_\tau]=\E_\tau\left([\loss(Y,\hat{F}(\X))|\tau]\right),
\end{equation}
i.e., average over many different test sets.
In practice, we observe a sample $(\x_i,y_i)_{i=1}^N$. For this sample, we can calculate the training error,
\begin{equation}\label{eq:empirical-risk}
    \err(F)=\frac{1}{N}\sum_{i=1}^N\loss(y_i, \hat{F}(\x_i)),
\end{equation}
also known as the empirical risk. To estimate $\text{err}$ \eqref{eq:err}, one can do two things. First, if the observed sample is large enough, one can choose a portion of this, say 20\%, to be used as a hold-out test set. We then train/fit/estimate based on the other 80\%, and estimate $\text{Err}$ by
\begin{equation*}
    \hat{\text{Err}}=\frac{1}{M}\sum_{i=1}^ML(y_i,\hat{F}(\x_i)),
\end{equation*}
where $(x_i,y_i)$ here are from the test set.

\section{The history of boosting}\label{history}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}. Boosting originated in the fields of computational learning theory and machine learning. \cite{kearnsvaliant}, working on computational learning theory, posed a question: Could any weak learner be transformed to become a strong learner? A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. \cite{adaboost} then invented the AdaBoost algorithm for constructing a binary classifier. It was evidence that the answer to the original question was positive. The AdaBoost algorithm performs iterative reweighting of original observations. For each iteration, it gives more weight to misclassified observations, and then trains a new weak learner based on all weighted observations. It then adds the new weak learner to the final classifier. The resulting AdaBoost classifier is then a linear combination of these weak classifiers, i.e., a weighted majority vote. In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. In statistics, however, we are interested in models which are interpretable.
%\todo[inline]{``something to connect the sentences''???? how! new section on AdaBoost? also rephrase weak learner part!}

\section{Statistical boosting}\label{sec:sboost}
While AdaBoost often has good predictive performance, in its original formulation, it is a black box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, we often want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation}\label{eq:exp-f}
    \E(Y|\X=\x)=F(\x).
\end{equation}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings, and we therefore extend our discussion to a more general regression scheme where the outcome variable $Y$ can be continuous. We are interested in interpreting how the different covariates of $\X$ affect $\E(\cdot)$. A choice for $F(\X)$ which is amenable to such interpretation is the generalized additive model (GAM),
\begin{equation}\label{eq:gam}
    F(\x)=\alpha+\sum_{j=1}^pf_j(x_j),
\end{equation}
where $\alpha\in\R$ is an offset and $x_j$ is the j-th component of $\x$. $F(\x)$ is a sum of component-wise functions $f_j$, and as such a GAM contains interpretable additive predictors. \cite{friedman2000} showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function. This provided a way of viewing boosting through a statistical lens. \cite{friedman2001} further investigated the topic, providing further insight into boosting. However, we must first discuss how we find an approximate solution for $F(\X)$ in \eqref{eq:exp-f}.

\section{Finding a solution}
We wish to find $F(\X)$ in \eqref{eq:exp-f}, so we are interested in solving
\begin{equation*}
    \argmin_{F}\E\left[\loss(Y,F(\X))\right],
\end{equation*}
where $\loss$ is an appropriate loss function. But as discussed in chapter \ref{sec:learning-theory}, we have in practice observed a dataset $\{\x_i,y_i\}_{i=1}^N$, drawn from the joint distribution of $(\X,Y)$. Therefore we substitute the expected loss with the expected risk \eqref{eq:empirical-risk}, and so we want a solution to
\begin{equation}\label{eq:argmin-risk}
    \argmin_{F}\err(F).
\end{equation}
Finding $F$ is not easy. We will now discuss a general optimization algorithm used in boosting.

\section{Gradient descent}
Gradient descent is an optimization algorithm for minimizing a differentiable multivariate function $F$. The motivation behind the gradient descent algorithm is that in a small interval around a point $\x$, $F$ is decreasing in the direction of the negative gradient at $\x$. Therefore, by moving slightly in that direction, $F$ will decrease. Indeed, with a sufficiently small step length, gradient descent will always converge, albeit to a local minimum. For a schematic overview of the algorithm, see Algorithm \ref{algo:grad-desc}.
\begin{algorithm}
\caption{Gradient descent}
\label{algo:grad-desc}
\begin{enumerate}
    \item Start with an initial guess $\x_0$, e.g. $\x_0=\0$. Let $m=1$.
    \item Calculate the direction to step in, $\g_{m-1}=-\nabla F(\x_{m-1})$.
    \item Let $\h_m=\nu \g_{m-1}$, where $\nu$ is the best step length, found by
        \begin{equation*}
            \nu=\argmin_{\nu}\x_{m-1}+\nu \g_{m-1}
        \end{equation*}
    \item Let $\x_m=\x_{m-1}+\h_{m-1}$
    \item Increase $m$, and go to step 2. Repeat until $m=M$.
    \item The resulting final guess is $\x_M=\x_0+\sum_{m=1}^M\h_m(\x_m)$
\end{enumerate}
\end{algorithm}
Back to our goal of finding an $F$ to minimize \eqref{eq:argmin-risk}. Often we choose a parameterized model $F(\X;\bbeta)$. Finding the optimal $\bbeta$ analytically might be infeasible. The gradient descent algorithm can then be used. In this case, we fix $\X$ and let $F(\X)$ in the algorithm be $F(\bbeta;\X)$. Thus we use gradient descent to find an optimal $\bbeta$. We would then say we are doing gradient descent in parameter space. We are now ready to reveal Friedman's useful insight.

\section{Gradient boosting: Functional gradient descent}
There is another possible way to use gradient descent, and that is the important insight by Friedman in 2001 \citep{friedman2001}. He showed that boosting can be viewed as an optimization procedure in functional space. Briefly, we first describe a naive way of doing this. Consider the function value at each $\x$ directly as a parameter, and use gradient descent directly on these parameters. However, this does not generalize to unobserved values $\X$, and we are after all interested in the population minimizer of \eqref{eq:exp-f}. We can instead assume a parameterized form for $F$, e.g.,
%\todo[inline]{needs work!}
\begin{equation}\label{eq:gradboost}
    F(\X;\{\bbeta\}_{m=1}^M)=\sum_{m=1}^M\nu H(\X;\bbeta_m),
\end{equation}
where $H(\X;\bbeta)$ is also a function on the GAM form \eqref{eq:gam}, but typically simpler, i.e., a base learner as discussed previously. We would like to minimize a data based estimate of the loss, i.e. the empirical risk, and so would choose $\{\bbeta_m\}$ as the minimizers of 
\begin{equation*}
    \argmin_{\{\bbeta_m\}_{m=1}^M}\err(H(\x;\{\bbeta_m\})).
\end{equation*}
However, estimating these simultaneously may be infeasible. We can then use a greedy stagewise approach, where we at each step $m$ choose the $\bbeta_m$ which gives the best improvement, while not changing any of the previous $\{\bbeta\}_{k=1}^{m-1}$. Hence at each step $m$ the current solution is
\begin{equation*}
    F_{m}=F_{m-1}+\nu H(\x;\bbeta_m),
\end{equation*}
where the parameters $\bbeta_m$ are those in $H$ minimizing the empirical risk when added to the fixed part $F_{m-1}$:
\begin{equation*}
    \bbeta_m=\argmin_{\bbeta}\err(H(\x;\bbeta_k)+H(\x;\bbeta)).
\end{equation*}
The final model is then the sum of these terms, like in \eqref{eq:gradboost}. To find $\bbeta_m$ in each step here, we use gradient descent. We have outlined a generic functional gradient descent algorithm. For a schematic overview of this, see Algorithm \ref{algo:fgd}.
\begin{algorithm}
\caption{Functional gradient descent}
\label{algo:fgd}
\begin{enumerate}
    \item Initialize $F_0(\x)$, e.g., by setting it to zero for all components. Select a base learner $H$.
    \item Compute the negative gradient vector,
        \begin{equation*}
            U_i=-\frac{\partial \loss(y_i,F_{m-1}(\cdot))}{\partial F_{m-1}(\cdot)},\quad i=1,\dotsc,N.
        \end{equation*}
    \item Estimate $\hat{H}_m$ by fitting $(\X_i,U_i)$ using the base learner $H$ (like in the previous algorithm):
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i,H(\x_i;\bbeta))
        \end{equation*}
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu H(\cdot;\bbeta_m)$.
    \item Repeat steps from 2 until $m=M$.
\end{enumerate}
\end{algorithm}
Note that while we call this functional gradient descent (FGD), this is exactly the gradient boosting algorithm.
\section{L2Boost}
Based on \cite{friedman2001}'s results, \cite{buhlmann-yu} developed the L2Boost algorithm. It is a special case of the generic functional gradient descent (FGD) algorithm, where we choose the squared error loss to be the loss function,
\begin{equation*}
    \loss(y,F(\x))=\frac{1}{2}\left(y-F(\x)\right)^2.
\end{equation*}
The negative gradient vector of the loss then becomes the residual vector,
\begin{equation*}
    \frac{\partial\loss(y,F(\x))}{\partial x_i}=(y-F(x_i)),\quad i=1,\dotsc,n,
\end{equation*}
and hence the boosting steps become repeated refitting of residuals \citep{friedman2001,buhlmann-yu}. With $\nu=1$ and $M=2$, this had been proposed already by \citep{tukey}, who called it ``twicing''. See Algorithm \ref{algo:l2} for an overview.

\begin{algorithm}
\caption{L2Boost}
\label{algo:l2}
\begin{enumerate}
    \item Initialize $F_0(\x)$, e.g., by setting it to zero for all components. Select a base learner $H$, such as ordinary least squares, stumps, etc.
    \item Compute the residuals
        \begin{equation*}
            U_i=(y_i,F_{m-1}(x_i)),\quad i=1,\dotsc,n
        \end{equation*}
    \item Estimate $\hat{H}_m$ by fitting $(\X_i,U_i)_{i=1}^N$ using the base learner $H$:
        \begin{equation*}
            \bbeta_m=\argmin_{\bbeta}\sum_{i=1}^N\loss(u_i,H(\x_i;\bbeta))
        \end{equation*}
        Note that $\hat{H}(\cdot;\bbeta_m)$ is then an estimate of the negative gradient vector.
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu H(\cdot;\bbeta_m)$.
    \item Repeat steps from 2 until $m=M$..
\end{enumerate}
\end{algorithm}

They also prove some important theoretical results for L2Boost.
%\todo[inline]{more on L2Boost!!}
\section{Component-wise gradient boosting}
In high-dimensional settings, it might often be infeasible, if not impossible, to use a base learner $H$ which incorporates all $p$ dimensions. Indeed, using least squares base learners, it is impossible, since the matrix which must be inverted is singular when $p>N$. Component-wise gradient boosting is a technique/algorithm which does work in these settings. First developed by \cite{buhlmann-yu}, and it has further been refined and explored, see e.g. \cite{buhlmann2006}. The idea of the algorithm is to select a set of base learners, the most important property of which being that they are univariate: Each base learner is only a function of one component $x_j$ of the data $\X$, i.e.,
\begin{equation*}
    h_j(\x)=h_j(x_j).
\end{equation*}
In each iteration, we fit the learners separately, and choose only the one which gives the best improvement to be added in the final model. The resulting model $F_m(\cdot)$ is then a sum of componentwise effects,
\begin{equation*}
    F_m(\X)=\sum_{j=1}^pf_j(x_j),
\end{equation*}
where
\begin{equation*}
    f_j(x_j)=\sum_{m=1}^M\1_{mj}h_j(x_j;\bbeta_m),
\end{equation*}
%\todo{better notation??!}
where $\1_{mj}$ is an indicator function which is 1 if component $j$ was selected at iteration $m$ and 0 if not.
Hence this model is a GAM \eqref{eq:gam}. Crucially, if we stop sufficiently early, we will typically perform variable selection. It is likely that some base learners have never been added to the final model, and as such those components in $\X$ are not added. For a schematic overview of the algorithm, see Algorithm \ref{algo:component-gradboost}.
%\todo[inline]{more about variable selection!!}
\begin{algorithm}
\caption{Component-wise gradient boosting}
\label{algo:component-gradboost}
\begin{enumerate}
    \item Start with an initial guess, e.g. $F_0=\0$.\\
    Specify a set of base learners $h_1(\cdot),\dotsc,h_p(\cdot)$.
    \item Compute the negative gradient vector $\U$.
    \item Fit $(\X_i,U_i)_{i=1}^N$ separately to every base learner to get $\hat{h}_1(x_1),\dotsc,\hat{h}_p(x_p)$.
    \item Select the component $k$ which best fits the negative gradient vector.
        \begin{equation*}
            k=\argmin_{j\in[1,p]}\sum_{i=1}^N(u_i-\hat{h}_j(\x_i))^2
        \end{equation*}
    \item Update $F_m(\cdot)=F_{m-1}(\cdot)+\nu h_k(x_k)$
\end{enumerate}
\end{algorithm}
In fact, Buhlmann believes that it is mainly in the case of high-dimensional predictors that boosting has a substantial advantage over classical approaches \citep{buhlmann2006}.

\section{Multidimensional boosting: Component-wise boosting of a multivariate loss function}
The above methods consider a loss function depending on one parameter: $\loss(\beta)$. In the boosting steps, one uses a gradient descent step with the loss function differentiated with respect to this one parameter.
There are however many models which use loss functions of several variables. \cite{schmid} extend the gradient boosting algorithm \citep{friedman2001} to such a setting. In this method, we take the partial derivative of the multivariate loss function with respect to each variable. They propose doing a boosting step in each dimension at a time, and repeat this for all dimensions. We also find an $\mstop$ for each dimension. The algorithm proposed in \cite{schmid} does an additional cycle through of nuisance parameters after having cycled through all dimensions, in each boosting step. The algorithm is as follows.
\begin{algorithm}
\caption{K-dimensional component-wise gradient boosting}
\label{algo:multidim-boost}
\begin{enumerate}
    \item Initialize the $n$-dimensional vectors $f_1^{[0]},\ldots,f_K^{[0]}$, with offset values, e.g. with $f_1^{[0]}=\0,\ldots,f_K^{[0]}=\0$. Alternatively, one can use the maximum likelihood estimates as offset values.
    \item For each component $k=1,\ldots,K$, specify a base learner to use for each of the $p$ components. The base learner takes one input variable and has one output variable. Examples include.
    \item Set $m=0$.
    \item Increase $m$ by 1.
    \begin{enumerate}
        \item Set $k=0$.
        \item Increase $k$ by 1. If $m>m_{\text{stop},k}$, proceed to step 4 f). If not, compute the negative partial derivative $-\frac{\partial\rho}{\partial f_k}$ and evaluate at $\hat{f}^{[m-1]}(X_i)=\left(\hat{f}_1^{[m-1]}(X_i),\ldots,\hat{f}_K^{[m-1]}(X_i)\right),i=1,\ldots,n$. This yields the negative gradient vector $U_k^{[m-1]}=\left(U_{i,k}^{[m-1]}\right)_{i=1,\ldots,n}
        \newline:=\left(-\frac{\partial}{\partial f_k}\rho\left(Y_i,\hat{f}^{[m-1]}(X_i)\right)\right)_{i=1,\ldots,n}$.
        \item Fit the negative gradient vector $U_k^{[m-1]}$ to each of the $p$ components of $\X$ separately (i.e. to each predictor variable) using the base learners specified in step 2. This yields $p$ vectors of predicted values, where each vector is an estimate of the negative gradient vector $U_k^{[m-1]}$.
        \item Select the component of $\X$ which best fits $U_k{[m-1]}$ best according to a pre-specified goodness-of-fit criterion. For continuous variables, the $R^2$ measure should (?) be used. Set $\hat{U}_k^{[m-1]}$ equal to the fitted values of the corresponding best model fitted in the previous step.
        \item Update $\hat{f}_k{[m-1]}\gets\hat{f}_k^{[m-1]}+\nu\hat{U}_k^{[m-1]},$ where $\nu$ is a real-valued step-length factor.
        \item For $k=2,\ldots,K$, repeat steps. Finally, update $\hat{f}^{[m]}\gets\hat{f}^{[m-1]}$.
    \end{enumerate}
    \item Iterate step 4 until $m>\max(m_{\text{stop},k})$ for all $k\in\{1,\ldots,K\}$.
\end{enumerate}
\end{algorithm}



\section{The importance of stopping early}
The number of iterations in the boosting procedure, $M$, is a tuning parameter. It acts as a regularizer.


\section{Selecting $\mstop$}
The crucial tuning parameter in boosting is the number of iterations, $\mstop$. Stopping early enough performs variable selection and shrinks the parameter estimates toward zero. Left on its own, the parameters in boosting will converge towards the maximum likelihood parameters \todo{SOURCE?}, i.e., maximizing the in-sample error. We are, on the other hand, after all interested in minimizing out-of-sample prediction error (PE). The prediction error for a given data set is a function of the boosting iteration $m$. What we want is therefore a good method for approximating $\PE(m)$. This can be done in a number of ways. Many authors state that the algorithm should be stopped early, but do not go further into the details here. Common model selection criteria such as the Akaike Information Criteria (AIC) may be used, however the AIC is dependant on estimates of the model's degrees of freedom. Methods by \citet{chang2010} try this. This is problematic for several reasons. For $\text{L}_2\text{Boost}$, \citet{buhlmann2007} suggest that $\df(m)=\trace(B_m)$ is a good approximation. Here $B_m$ is the hat matrix resulting from the boosting algorithm. This was, however, shown by \citet{hastie2007} to always underestimate the actual degrees of freedom. \citet{mayr-hofner} propose a sequential stopping rule using subsampling etc. We argue instead that cross-validation, a very common method for selection of tuning parameters in statistics, is a good method to use. It is flexible and easy to implement. It is somewhat computationally demanding, requiring several full runs of the boosting algorithm.

\subsection{K-fold cross-validation}
K-fold cross-validation \citep{lachenbruch}, or simply cross-validation, is a general method commonly used for selection of penalty or tuning parameters. We will use it to approximate the prediction error. In cross-validation, the data is split randomly into K rougly equally sized folds. For a given fold $k$, all folds except $k$ act as the training data in estimating the model. We often say that the $k$'th fold is left out. The resulting model is then evaluated on the unseen data, namely fold $k$. This procedure is repeated for all $k=1,\ldots,K$. An estimate for the prediction error is obtained by summing over the test errors from evaluting the left-out fold. Let $\kappa(k)$ be the set of indices for fold $k$. The cross-validated estimate for a given $m$ then becomes
\begin{equation}
    \CV(m)=\sum_{k=1}^K\sum_{i\in\kappa(k)}\loss\left((t_i,\delta_i),\theta_m;\x_i\right).
\end{equation}
For each $m$, we calculate the estimate of the cross-validated prediction error $\CV(m)$. We choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\CV(m).
\end{equation}
Typical values for $K$ are 5 or 10, but in theory one can choose any number. The extreme case is $K=N$, called leave one out cross-validation, where all but one observation is used for training and one evaluates the model on the observation that was left out. In this case, the outcome is deterministic, since there is no randomness when dividing into folds.

\subsection{Stratified cross-validation}
When dividing an already small number of survival data observations into $K$ folds, we might risk getting folds without any observed deaths, or in any case, very few. In stratified cross validation, we do not divide the folds entirely at random, but rather, try to divide the data such that there is an equal amount of censored data in each fold.
As before, let $\kappa(k)$ be the set of indices for fold $k$. Divide the observed data into $K$ folds, as with usual cross validation, to get an index set $\kappa_{\delta=1}(k)$ for a given $k$. Similarly, divide the censored data into $K$ folds, obtaining $\kappa_{\delta=0}(k)$. Finally, $\kappa(k)$ is the union of these sets: $\kappa(k)=\kappa_{\delta=1}(k)\cup\kappa_{\delta=0}(k)$. For ``real-life data sets like ours'', \cite{kohavi} illustrate that 10-fold stratified cross validation performs best.

\subsection{Repeated cross-validation}
The randomness inherent in the cross-validation splits has an effect on the resulting $\mstop$. This is true for boosting in general, but it is true for real-life survival data, especially. In typical survival time data sets one typically has a small effective sample size (number of observed events). We can easily imagine that for two different splits of the data, we can end up with quite different values for $\mstop$.
It has been very effectively demonstrated that the split of the folds has a large impact on the choice of $\mstop$ \citep{seibold}. \citet{seibold} suggest simply repeating the cross-validation scheme. They show that repeating even 5 times effectively averages out the randomness.  In other words, we divide the data into $K$ folds, and repeat this $J$ times. Now let $\kappa(j, k)$ be the $k$'th fold in the $j$'th split. We end up with a new estimate for the prediction error,
\begin{equation}
    \RCV(m)=\sum_{j=1}^J\sum_{k=1}^K\sum_{i\in\kappa(j,k)}\loss\left((t_i,\delta_i),\theta_m;\x_i\right).
\end{equation}
As before, we choose $\mstop$ to be the minimizer of this error,
\begin{equation}
    \mstop=\argmin_{m}\RCV(m).
\end{equation}
In practice, to ensure we find the minimizing $m$, we let the boosting algorithm run for $m=1$ to $m=M$, where $M$ is a large number that we are sure will result in a overfitted model.