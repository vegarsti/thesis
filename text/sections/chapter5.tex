\chapter{Model fitting}
When looking at a specific data set of individuals with covariates and corresponding responses, a statistician wishes to understand their relationship. In other words, she assumes that there actually is some structured relationship between the input and the output. This relationship is something we wish to model. A model is a simplification or approximation of reality and hence will not reflect all of reality. As the statistician George Box's aphorism goes: All models are wrong, but some are useful. We assume the data set consists of $n$ individuals, where each individual $\x_i$, $i=1,\ldots,n$, has $p$ covariates: $x_{i1},x_{i2},\ldots,x_{ip}$. We express these in vector form
\begin{equation*}
    \x_i=\left(x_{i1},x_{i2},\ldots,x_{ip}\right).
\end{equation*}
The individual has a corresponding real-valued output $y_i$. The statistician will assume that the relationship between an covariate vector $x_i$ and an output value $y_i$ can be expressed as
\begin{equation*}
    y_i=f(\x_i)+\eps,
\end{equation*}
where $f(\cdot)$ is some model which the statistician will have to choose, and $\eps$ is the error term, the difference between the value which the statistician's model gives, and the actual output. This model is perfectly general. Choosing a model for $f(\cdot)$ is what most of statistics is about. In the machine learning field, one has typically been more focused on choosing a model which has a low error term. But one has not necessarily cared about the model $f(\cdot)$ being interpretable. Being interpretable means that you can say something about the effect of an increase in a specific covariate, say. We call these non-interpretable models for black-box models. To be sure, black box models will often perform incredibly well in predicting an outcome which is correct. In statistics, however, we are more interested in a model which is interpretable, such that we can quantify the relation between one or more predictor variables and the expectation of the response.

\section{Model selection}
Given that all models are wrong, it is easy to see that there is no perfect model for a data set. But some are better than others. There exist many selection criteria one can use to do this. In general, though, what one wishes to do in all cases is have the model result in as low error terms as possible, while at the same time not overfitting. One way to think of overfitting is that the model starts to learn the structure of the error terms. One might think of the manydimensional space of the input matrix $\X$, which has the individuals $\x_{1},\x_{2},\ldots,\x_{n}$ as rows, where the corresponding outputs of the individuals are points in this space. If a model is overfitting, the curvature of the model in this space is very high. If the model has no curvature, it means that the output will be the same for all individuals, so this will also entail a very high error. In other words, this will not be a good model. In general, one wishes to have low error terms, and low curvature. This is a trade-off one has to make which is called the bias-variance tradeoff. In the case of overfitting, where one has high curvature, there is a high variance between predictions. Even if one takes one observation and shifts it somewhat, then the predicted value will be very different. On the other hand, a flat curvature model has no variance, but very high bias. One can understand, then, that there exists a middle ground which is a good compromise between the two: A model which has both reasonably low bias and reasonably low variance.

\section{Variable selection}
Having chosen a specific type of model -- a linear regression, say -- an important next step is to select variables to use in our model $f(\cdot)$. This can be done in many ways. One way is to try all possible combinations. The number of combinations explodes combinatorially with the number of predictors, since the number of possible combinations is the factorial $p!$ of $p$. Two other main methods are often used in the case of reasonably few predictors. One is to start with the null model, that is, no predictors. Then, for all predictors, one can try adding it to the model. One proceeds with the predictor which gives the best improvement. This is called forward stepwise variable selection. What easily comes to mind now is to do the reverse, namely backwards stepwise variable selection. Start with a model which contains all predictors. In each step, remove that predictor which gives the least improvement compared to the model without that predictor. In both cases, one typically chooses a significance level $1-\alpha$ which is a stopping point for the procedure. This means that in the forward stepwise method, one stops when there are no predictors with a p-value of less than $\alpha$ left to add into the model. In the case of backwards stepwise, then, there are no predictors with a p-value of more than $\alpha$ left.

With a model $f(\cdot)$, we have something which can predict the response variable $y$ for any given $\x$. We can call this prediction for $\hat{y}$. The error term $\eps$ is then $y-\hat{y}$. To calculate the difference between the estimated outcome and the actual outcome, we need a loss function $\rho$. It measures the difference, or distance, between the true outcome $y$ and the predicted ou. A proper loss function must be symmetric and convex. Examples of choices for $\rho$ are the absolute loss $|y-\eta(x)|$, which leads to a regression model for the median, and the quadratic loss (the $L_2$ loss), which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Note that we will use the negative log likelihood as the loss function, due to the aim of \textit{minimizing} the loss function, whereas the log likelihood increases as the model fits better to the data. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.

\section{Combatting overfitting}

\section{High-dimensional data}