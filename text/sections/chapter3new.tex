\chapter{Statistical boosting}
Boosting is one of the most promising methodological approaches for data analysis developed in the last two decades \citep{mayr14a}. It is now a staple part of the statistical learning toolbox because it is a flexible tool for estimating interpretable statistical models. Boosting, however, originated as a black box algorithm in the fields of computational learning theory and machine learning, not in statistics.

Computer scientists Michael Kearns and Leslie Valiant, who were working on computational learning theory, posed the following question: Could any weak learner be transformed to become a strong learner? \citep{kearnsvaliant} A weak learner, sometimes also simple or base learner, means one which has a low signal-to-noise ratio, and which in general performs poorly. For classification purposes it is easy to give a good example: A weak learner is one which performs only slightly better than random uniform chance. In the binary classification setting, then, it would only perform slightly better than a coin flip. For regression, a weak learner is for example a linear least squares model of only one variable, and having only a small parameter effect for that variable. Meanwhile, a strong learner should be able to perform in a near-perfect fashion, for example attaining 99\% accuracy on a prediction task. I will first attend to give a summary of the history of boosting, starting with AdaBoost \citep{adaboost}, which proved that the answer to the original question above was yes. For another overview, consult also the literature review article by \citet{mayr14a}.

\section{AdaBoost}
The original AdaBoost, also called Discrete AdaBoost \citep{adaboost} is an iterative algorithm for constructing a binary classifier $F(\cdot)$. It was the first \textit{adaptive} boosting algorithm, as it automatically adjusted its parameters to the data based on its perfomance. In the binary classification problem, we are given a set of observations $(\x_i,y_i)_{i=1,\ldots,n}$, where $x\in\R^p$ and $y\in\{-1,1\}$, i.e., positive or negative; yes or no. We want to find a rule which best separates these observations into the correct buckets, as well as being able to classify new, unseen observations $\x_{\text{new}}$ of the same form. Some observations are hard to classify, whereas some are not. One way to look at binary classification is to imagine the $p$-dimensional space of the observations $\x$, and think of the classifier as finding the line which best splits the observations into their corresponding label. Some observations are not at all close to the boundary, and so they are easily classified. Other observations, however, are close to the boundary. \citet{adaboost} realized that one could assign a weight to each observation. First, assign equal weight to each observation. Then, use a weak learner $h(\cdot)$ to make an initial classifier, minimizing the weighted sum of misclassified points, which initially is a plain sum. After this initial classification, some points will be correctly classified, and some will be misclassified. Change the weights in a certain way such that the misclassified ones get increased weights, and normalize the weights afterwards. Based on the misclassification rate, calculate a weight $\alpha$ to give to this classifier. This results in the correctly classified observations having smaller weight than before. Currently, the classifier is $F_1(\cdot)=\alpha_1h_1(\cdot).$ In the next iteration, make a new weak learner which minimizes the weighted sum of the observations and reweight observations accordingly as before. Again calculate a weight to give to this new classifier, and add it to the previous classifier, such that $F_2(\cdot)=\alpha_1h_1(\cdot)+\alpha_2h_2(\cdot)$. Continue iterating in this fashion until an iteration $m$. The resulting final classifier, the AdaBoost classifier, becomes $F(\cdot)=F_m(\cdot)=\sum_{i=1}^m\alpha_ih_i(\cdot)$. It is a linear combination of the weak classifiers, and in essence a weighted majority vote of weak learners given the observations.

The AdaBoost algorithm often carries out highly accurate prediction. In practice, it is often used with stumps: Decision trees with one split. For example, \citet{bauer-kohavi} report an average 27\% relative improvement in the misclassification error for AdaBoost compared to a single decision tree. They conclude that boosting not only reduces the variance in the prediction error from using different training data sets, but that it also is able to reduce the average difference between the predicted and the true class, i.e., the bias. \citet{breiman1998} supports this analysis. Because of its plug-and-play nature and the fact that it never seemed to overfit (overfitting occurs when the learned classifier degrades in test error because of being too specialized on its training set), Breiman remarked that ``boosting is the best off-the-shelf classifier in the world'' \citep{ESL}.

Something about overfitting?

In its original formulation, the AdaBoost classifier does not have interpretable coefficients, and as such it is a so-called black-box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, however, we are interested in models which are interpretable.

See some figure for a schematic overview of the algorithm.

\section{Boosting as Functional Gradient Descent}
While originally developed for binary classification, boosting is now used to estimate the unknown quantities in more general statistical models and settings. We therefore extend our discussion to a more general regression scheme where the outcome variable $Y$ can be continuous. We are interested in interpreting how the different covariates of $\X$ affect $\E(\cdot)$. A choice for $F(\X)$ which is amenable to such interpretation is the generalized additive model (GAM),
\begin{equation}\label{eq:gam}
    F(\x)=\alpha+\sum_{j=1}^pf_j(x_j),
\end{equation}
\citet{friedman2000} showed that AdaBoost fits a GAM with a forward stagewise algorithm, for a particular exponential loss function. This provided a way of viewing boosting through a statistical lens. \citet{friedman2001} continued to investigate the topic, providing further insight into boosting.

We arrive at the general FGD meta-algorithm.

\section{$L_2$Boost}
Lorem ipsum.

\section{Statistical boosting}
While AdaBoost often has good predictive performance, in its original formulation, it is a black box algorithm. This means that we are unable to infer anything about the effect of different covariates. In statistics, we often want to estimate the relation between observed predictor variables and the expectation of the response,
\begin{equation}\label{eq:exp-f}
    \E(Y|\X=\x)=F(\x).
\end{equation}
In addition to using boosting for classification, like in the original AdaBoost, we would also like to use it in more general settings, and we therefore extend our discussion to a more general regression scheme. Let $D=\{x^{(i)},y^{(i)}\}_{i=1,\ldots,n}$ be a learning data set sampled iid from a distribution over the joint space $\setX\times\setY$, where the input space $\setX$ $p$-dimensional and the output space is a low-dimensional space $\setY$. For the majority of applications, the output space $\setY$ is one-dimensional and continuous, e.g., in the standard regression setting. In the censored survival data setting, however, it is two-dimensional, since we have $y^{(i)}=(t_i,d_i)$. The response variable is estimated through an additive model where the expectation given $x$ is \todo{What?}
\begin{equation}
    \E[y|x]=F(\eta(x)),
\end{equation}
where $F(\cdot)$ is some function and where $\eta\colon\setX\to\R$ is an additive predictor
\begin{equation}\label{eq:eta}
    \eta(x)=\beta_0+\sum_{j=1}^Jf_j(x|\beta_j),
\end{equation}
with a \textbf{constant intercept coefficient $\beta_0$}, and additive effects $f_j(x|\beta_j)$ derived from the pre-defined set of base learners.
These base learners are usually relatively simple, regularized (see definition of regularization) parametric effects of $\beta_j$. Typical examples are such as linear least squares, stumps (trees with one split; see \citet{buhlmann2007} and \citet{ESL}), or splines with a few degrees of freedom. In many cases, each base learner is defined on exactly one element $x_j$ of $x$, and thus \eqref{eq:eta} simplifies to
\begin{equation}\label{eq:eta-componentwise}
    \eta(x)=\beta_0+\sum_{j=1}^pf_j(x_j|\beta_j).
\end{equation}
To estimate the parameters $\beta_1,\ldots,\beta_j$ of the additive predictor, the boosting algorithm minimizes the empirical risk $\rho\colon\setY\times\R\to\R$, also called the in-sample error and the training error, summed over all samples in the learning data set $D$,
\begin{equation}
    R(D)=\sum_{i=1}^n\rho(y^{(i)},\eta(x^{(i)})).
\end{equation}
The loss function $\rho$ measures the difference, or distance, between the true outcome $y^{(i)}$ and the additive predictor $\eta(x^{(i)})$. A loss function must be symmetric and convex. Examples of $\rho$ are the absolute loss $|y^{(i)}-\eta(x^{(i)})|$, which leads to a regression model for the median, and the quadratic ($L_2$), loss, which leads to the usual regression model for the mean. Very often, the loss is derived from the \textbf{negative} log likelihood of the distribution of $\setY$, depending on the desired model. Keep in mind that the negative is used, due to the aim of \textit{minimizing} the loss function. In the survival data setting, typical loss functions are ROC and Briar score \citep{bovelstadborgan}.
\begin{equation}
    \rho(y^{(i)},\eta(x^{(i)})=-l(y_0(x^{(i)}), \mu(x^{(i)}), t_i, d_i)
\end{equation}
Let
\begin{equation}
    y_0(x)=\bbeta^T\x=\beta_0+\sum_{j=1}^p \beta_jx_j
\end{equation}
and
\begin{equation}
    \mu(z)=\bgamma^T\z=\gamma_0+\sum_{j=1}^p \gamma_jz_j
\end{equation}

The goal is to estimate a function which minimizes the loss over an unseen ``hold-out'' sample, often called the out-of-sample error, the generalization error or the test error.
\begin{equation}
    \text{Err}=\rho(y,\eta(x)).
\end{equation}

The main idea of boosting is to fit simple base learners $h(\cdot)$ one by one to the negative gradient vector of the loss $u=(u^{(1)},u^{(2)},\ldots,u^{(n)})$, instead of to the true outcomes $y=(y^{(1)},y^{(2)},\ldots,y^{(n)})$. Base learners are chosen in such a way that they approximate the effect $\hat{f}(x|\beta_j)=\sum_mh_j(\cdot)$. The negative gradient vector in iteration $m$, evaluated at the estimated additive predictor $\hat{\eta}^{[m-1]}(x^{(i)})$, is defined as
\begin{equation*}
    \u=\left(-\frac{\pd}{\pd\eta}\rho(y,\eta)\rvert_{\eta=\hat{\eta}^{[m-1]}(x^{(i)}),y=y^{(i)}}\right)_{i=1,\ldots,n}.
\end{equation*}
In every boosting iteration, each base learner is fitted separately to the negative gradient vector by least-squares or penalized least-squares regression.
The best fitting base learner is selected based on the residual sum of squares with respect to $u$,
\begin{equation*}
    j^*=\argmin_{j\in\{1,\ldots,J\}}\sum_{i=1}^{n}(u^{(i)}-\hat{h}_j(x^{(i)}))^2.
\end{equation*}
Only the best performing base learner $\hat{h}_{j^*}(\cdot)$ will be used to update the current additive predictor, leading to the update being
\begin{equation*}
    \eta^{[m]}(\cdot)=\eta^{[m-1]}(\cdot)+\text{sl}\cdot\hat{h}_{j^*}(\cdot),
\end{equation*}
where $0<\text{sl}<1$ denotes the step length or learning rate. The choice of step length is not of critical importance as long as it is sufficiently small \citep{schmid-hothorn}, but the convention is to use $\text{sl}=0.1$ \citep{mayr14a}.

With a fixed step length (learning rate), the main tuning parameter for gradient boosting is the number of iterations $m$ that are performed before the algorithm is stopped. We denote the resulting parameter $\mstop$. If $\mstop$ is too small, the model will underfit and it cannot fully incorporate the influence of the effects on the response and will consequently have poor performance. On the other hand, too many iterations will result in overfitting, leading to poor generalization.