% Encoding: UTF-8

@Comment{jabref-meta: databaseType:biblatex;}

@article{threg,
   author = {Tao Xiao and G. Whitmore and Xin He and Mei-Ling Lee},
   title = {The R Package threg to Implement Threshold Regression Models},
   journal = {Journal of Statistical Software, Articles},
   volume = {66},
   number = {8},
   year = {2015},
   keywords = {},
   abstract = {This paper introduces the R package threg, which implements the estimation procedure of a threshold regression model, which is based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data, and serves as an important alternative to the Cox proportional hazards model.This new package includes four functions: threg, and the methods hr, predict and plot for ‘threg’ objects returned by threg. The threg function is the model-fitting function which is used to calculate regression coefficient estimates, asymptotic standard errors and p values. The hr method for ‘threg’ objects is the hazard-ratio calculation function which provides the estimates of hazard ratios at selected time points for specified scenarios (based on given categories or value settings of covariates). The predict method for ‘threg objects is used for prediction. And the plot method for ‘threg’ objects provides plots for curves of estimated hazard functions, survival functions and probability density functions of the first-hitting-time; function curves corresponding to different scenarios can be overlaid in the same plot for comparison to give additional research insights.},
   issn = {1548-7660},
   pages = {1--16},
   doi = {10.18637/jss.v066.i08},
   url = {https://www.jstatsoft.org/v066/i08}
}

@book {caroni2017,
  title = {First Hitting Time Regression Models},
  author = {Caroni, Chrysseis},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {9781119437260},
  url = {http://dx.doi.org/10.1002/9781119437260.ch2},
  doi = {10.1002/9781119437260.ch2},
  keywords = {Birnbaum-Saunders distribution, first hitting time regression models, inverse Gaussian distribution, Ornstein-Uhlenbeck process, parent stochastic process, Wiener process},
  booktitle = {First Hitting Time Regression Models},
  year = {2017},
}

@article{lee2010,
  author="Lee, Mei-Ling Ting
  and Whitmore, G. A.",
  title="Proportional hazards and threshold regression: their theoretical and practical connections",
  journal="Lifetime Data Analysis",
  year="2010",
  month="Apr",
  day="01",
  volume="16",
  number="2",
  pages="196--214",
  abstract="Proportional hazards (PH) regression is a standard methodology for analyzing survival and time-to-event data. The proportional hazards assumption of PH regression, however, is not always appropriate. In addition, PH regression focuses mainly on hazard ratios and thus does not offer many insights into underlying determinants of survival. These limitations have led statistical researchers to explore alternative methodologies. Threshold regression (TR) is one of these alternative methodologies (see Lee and Whitmore, Stat Sci 21:501--513, 2006, for a review). The connection between PH regression and TR has been examined in previous published work but the investigations have been limited in scope. In this article, we study the connections between these two regression methodologies in greater depth and show that PH regression is, for most purposes, a special case of TR. We show two methods of construction by which TR models can yield PH functions for survival times, one based on altering the TR time scale and the other based on varying the TR boundary. We discuss how to estimate the TR time scale and boundary, with or without the PH assumption. A case demonstration is used to highlight the greater understanding of scientific foundations that TR can offer in comparison to PH regression. Finally, we discuss the potential benefits of positioning PH regression within the first-hitting-time context of TR regression.",
  issn="1572-9249",
  doi="10.1007/s10985-009-9138-0",
  url="https://doi.org/10.1007/s10985-009-9138-0"
}

@book{chhikara1988,
  title={The Inverse Gaussian Distribution: Theory: Methodology, and Applications},
  author={Chhikara, R.},
  isbn={9780824779979},
  lccn={88020271},
  series={Statistics:  A Series of Textbooks and Monographs},
  url={https://books.google.no/books?id=bGyHxrEALI0C},
  year={1988},
  publisher={Taylor \& Francis}
}

@article{leewhitmore2006,
  author = "Lee, Mei-Ling Ting and Whitmore, G. A.",
  doi = "10.1214/088342306000000330",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "11",
  number = "4",
  pages = "501--513",
  publisher = "The Institute of Mathematical Statistics",
  title = "Threshold Regression for Survival Analysis: Modeling Event Times by a Stochastic Process Reaching a Boundary",
  url = "https://doi.org/10.1214/088342306000000330",
  volume = "21",
  year = "2006"
}


@article{gamboost,
  author = {Tutz, Gerhard and Binder, Harald},
  title = {Generalized Additive Modeling with Implicit Variable Selection by Likelihood-Based Boosting},
  journal = {Biometrics},
  volume = {62},
  number = {4},
  publisher = {Blackwell Publishing Inc},
  issn = {1541-0420},
  url = {http://dx.doi.org/10.1111/j.1541-0420.2006.00578.x},
  doi = {10.1111/j.1541-0420.2006.00578.x},
  pages = {961--971},
  keywords = {Generalized additive models, Likelihood-based boosting, Penalized stumps, Selection of smoothing parameters, Variable selection},
  year = {2006}
}


@article{mayr14a,
  Author = {Mayr, A. and Binder, H. and Gefeller, O. and Schmid, M.},
  Date-Added = {2018-03-02 09:17:00 +0000},
  Date-Modified = {2018-03-02 09:17:00 +0000},
  Doi = {10.3414/ME13-01-0122},
  Isbn = {0026-1270},
  Journal = {Methods of Information in Medicine},
  Keywords = {Machine Learning; Classification; Algorithms; statistical models; statistical computing},
  La = {EN},
  Number = {6},
  Pages = {419--427},
  Publisher = {Schattauer Publishers},
  Title = {The Evolution of Boosting Algorithms. From Machine Learning to Statistical Modelling},
  Ty = {JOUR},
  Url = {http://dx.doi.org/10.3414/ME13-01-0122},
  Volume = {53},
  Year = {2014},
}


@article{mayr14b,
  Author = {Mayr, A. and Binder, H. and Gefeller, O. and Schmid, M.},
  Date-Added = {2018-03-08 12:26:46 +0000},
  Date-Modified = {2018-03-08 12:26:46 +0000},
  Doi = {10.3414/ME13-01-0123},
  Isbn = {0026-1270},
  Journal = {Methods of Information in Medicine},
  Keywords = {Machine Learning; Classification; Algorithms; statistical models; statistical computing},
  La = {EN},
  Number = {6},
  Pages = {428--435},
  Publisher = {Schattauer Publishers},
  Title = {Extending Statistical Boosting. An Overview of Recent Methodological Developments},
  Ty = {JOUR},
  Url = {http://dx.doi.org/10.3414/ME13-01-0123},
  Volume = {53},
  Year = {2014},
}

@inproceedings{adaboost,
 author = {Freund, Yoav and Schapire, Robert E.},
 title = {Experiments with a New Boosting Algorithm},
 booktitle = {Proceedings of the Thirteenth International Conference on International Conference on Machine Learning},
 series = {ICML'96},
 year = {1996},
 isbn = {1-55860-419-7},
 location = {Bari, Italy},
 pages = {148--156},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3091696.3091715},
 acmid = {3091715},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@Article{schapire1990,
  author="Schapire, Robert E.",
  title="The strength of weak learnability",
  journal="Machine Learning",
  year="1990",
  month="Jun",
  day="01",
  volume="5",
  number="2",
  pages="197--227",
  abstract="This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.",
  issn="1573-0565",
  doi="10.1007/BF00116037",
  url="https://doi.org/10.1007/BF00116037"
}

@article{friedman2000,
  author = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
  doi = "10.1214/aos/1016218223",
  fjournal = "The Annals of Statistics",
  journal = "Ann. Statist.",
  month = "04",
  number = "2",
  pages = "337--407",
  publisher = "The Institute of Mathematical Statistics",
  title = "Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors)",
  url = "https://doi.org/10.1214/aos/1016218223",
  volume = "28",
  year = "2000"
}

@article{friedman2001,
  author = "Friedman, Jerome H.",
  doi = "10.1214/aos/1013203451",
  fjournal = "The Annals of Statistics",
  journal = "Ann. Statist.",
  month = "10",
  number = "5",
  pages = "1189--1232",
  publisher = "The Institute of Mathematical Statistics",
  title = "Greedy function approximation: A gradient boosting        machine.",
  url = "https://doi.org/10.1214/aos/1013203451",
  volume = "29",
  year = "2001"
}



@article{rejoinder,
  author = "Bühlmann, Peter and Hothorn, Torsten",
  doi = "10.1214/07-STS242REJ",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "11",
  number = "4",
  pages = "516--522",
  publisher = "The Institute of Mathematical Statistics",
  title = "Rejoinder: Boosting Algorithms: Regularization, Prediction and Model Fitting",
  url = "https://doi.org/10.1214/07-STS242REJ",
  volume = "22",
  year = "2007"
}

@Article{mboost-tutorial,
  author="Hofner, Benjamin
  and Mayr, Andreas
  and Robinzonov, Nikolay
  and Schmid, Matthias",
  title="Model-based boosting in R: a hands-on tutorial using the R package mboost",
  journal="Computational Statistics",
  year="2014",
  month="Feb",
  day="01",
  volume="29",
  number="1",
  pages="3--35",
  abstract="We provide a detailed hands-on tutorial for the R add-on package mboost. The package implements boosting for optimizing general risk functions utilizing component-wise (penalized) least squares estimates as base-learners for fitting various kinds of generalized linear and generalized additive models to potentially high-dimensional data. We give a theoretical background and demonstrate how mboost can be used to fit interpretable models of different complexity. As an example we use mboost to predict the body fat based on anthropometric measurements throughout the tutorial.",
  issn="1613-9658",
  doi="10.1007/s00180-012-0382-5",
  url="https://doi.org/10.1007/s00180-012-0382-5"
}

@Article{DeBin2016,
  author="De Bin, Riccardo",
  title="Boosting in Cox regression: a comparison between the likelihood-based and the model-based approaches with focus on the R-packages CoxBoost and mboost                  ",
  journal="Computational Statistics",
  year="2016",
  month="Jun",
  day="01",
  volume="31",
  number="2",
  pages="513--531",
  abstract="Despite the limitations imposed by the proportional hazards assumption, the Cox model is probably the most popular statistical tool used to analyze survival data, thanks to its flexibility and ease of interpretation. For this reason, novel statistical/machine learning techniques are usually adapted to fit its requirements, including boosting. Boosting is an iterative technique originally developed in the machine learning community to handle classification problems, and later extended to the statistical field, where it is used in many situations, including regression and survival analysis. The popularity of boosting has been further driven by the availability of user-friendly software such as the R packages mboost and CoxBoost, both of which allow the implementation of boosting in conjunction with the Cox model. Despite the common underlying boosting principles, these two packages use different techniques: the former is an adaptation of model-based boosting, while the latter adapts likelihood-based boosting. Here we contrast these two boosting techniques as implemented in the R packages from an analytic point of view; we further examine solutions adopted within these packages to treat mandatory variables, i.e. variables that---for several reasons---must be included in the model. We explore the possibility of extending solutions currently only implemented in one package to the other. A simulation study and a real data example are added for illustration.",
  issn="1613-9658",
  doi="10.1007/s00180-015-0642-2",
  url="https://doi.org/10.1007/s00180-015-0642-2"
}

@inproceedings{kearnsthoughts,
  title={Thoughts on Hypothesis Boosting Machine Learning class project , Dec . 1988},
  author={Michael S. Kearns}
}

@inproceedings{kearnsvaliant,
 author = {Kearns, M. and Valiant, L. G.},
 title = {Crytographic Limitations on Learning Boolean Formulae and Finite Automata},
 booktitle = {Proceedings of the Twenty-first Annual ACM Symposium on Theory of Computing},
 series = {STOC '89},
 year = {1989},
 isbn = {0-89791-307-8},
 location = {Seattle, Washington, USA},
 pages = {433--444},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/73007.73049},
 doi = {10.1145/73007.73049},
 acmid = {73049},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{buhlmann-yu,
  author = {Peter Bühlmann and Bin Yu},
  title = {Boosting With the L2 Loss},
  journal = {Journal of the American Statistical Association},
  volume = {98},
  number = {462},
  pages = {324-339},
  year  = {2003},
  publisher = {Taylor & Francis},
  doi = {10.1198/016214503000125},
  URL = { 
          https://doi.org/10.1198/016214503000125
  },
}

@article{buhlmann2006,
  author = "Bühlmann, Peter",
  doi = "10.1214/009053606000000092",
  fjournal = "The Annals of Statistics",
  journal = "Ann. Statist.",
  month = "04",
  number = "2",
  pages = "559--583",
  publisher = "The Institute of Mathematical Statistics",
  title = "Boosting for high-dimensional linear models",
  url = "https://doi.org/10.1214/009053606000000092",
  volume = "34",
  year = "2006"
}


@article{mayr-hofner,
  Author = {Mayr, A. and Hofner, B. and Schmid, M.},
  Date-Added = {2018-04-12 10:50:36 +0000},
  Date-Modified = {2018-04-12 10:50:36 +0000},
  Doi = {10.3414/ME11-02-0030},
  Isbn = {0026-1270},
  Journal = {Methods of Information in Medicine},
  Keywords = {Variable Selection; Gradient boosting; resampling methods; early stopping; penalized regression},
  La = {EN},
  Number = {2},
  Pages = {178--186},
  Publisher = {Schattauer Publishers},
  Title = {The Importance of Knowing When to Stop. A Sequential Stopping Rule for Component-wise Gradient Boosting},
  Ty = {JOUR},
  Url = {http://dx.doi.org/10.3414/ME11-02-0030},
  Volume = {51},
  Year = {2012},
  Bdsk-Url-1 = {http://dx.doi.org/10.3414/ME11-02-0030}
}

@book{tukey,
  title={Exploratory Data Analysis},
  author={Tukey, J.W.},
  isbn={9780201076165},
  lccn={76005080},
  series={Addison-Wesley series in behavioral science},
  url={https://books.google.no/books?id=UT9dAAAAIAAJ},
  year={1977},
  publisher={Addison-Wesley Publishing Company}
}

@article{stogiannis-2010,
    author = {D. Stogiannis and C. Caroni and C. E. Anagnostopoulos and I. K. Toumpoulis},
    title = {Comparing first hitting time and proportional hazards regression models},
    journal = {Journal of Applied Statistics},
    volume = {38},
    number = {7},
    pages = {1483-1492},
    year  = {2011},
    publisher = {Taylor & Francis},
    doi = {10.1080/02664763.2010.505954},

    URL = { 
            https://doi.org/10.1080/02664763.2010.505954
        
    },
    eprint = { 
            https://doi.org/10.1080/02664763.2010.505954
        
    }
}

@article{stogiannis-2013,
  author = {D. Stogiannis and C. Caroni},
  title = {Issues in Fitting Inverse Gaussian First Hitting Time Regression Models for Lifetime Data},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {42},
  number = {9},
  pages = {1948-1960},
  year  = {2013},
  publisher = {Taylor & Francis},
  doi = {10.1080/03610918.2012.687061},
  URL = { 
          https://doi.org/10.1080/03610918.2012.687061   
  },
  eprint = { 
          https://doi.org/10.1080/03610918.2012.687061
      
  }
}


@article{whitmore1986,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2987525},
 abstract = {Recent research has demonstrated the practical and theoretical value of using first-passage-time distributions as models of duration data (e.g., equipment lives, hospital stays, employee service times). The inverse Gaussian distribution is the principal example of this type. Methods have been developed for handling regression structures and censoring for the inverse Gaussian model in order to cope with these features in real data. This paper discusses the use of first-passage-time distributions connected with multidimensional Brownian motion as models for duration data which are subject to competing risks (i.e., multiple failure modes). The approach is a natural extension of the regression methodology for censored inverse Gaussian data published previously by the author. Several case applications of the competing-risks formulation are presented.},
 author = {G. A. Whitmore},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {2},
 pages = {207--219},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {First-Passage-Time Models for Duration Data: Regression Structures and Competing Risks},
 volume = {35},
 year = {1986}
}

@article{aalengjessing2001,
  author = "Aalen, Odd O. and Gjessing, Håkon K.",
  doi = "10.1214/ss/998929473",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "02",
  number = "1",
  pages = "1--22",
  publisher = "The Institute of Mathematical Statistics",
  title = "Understanding the shape of the hazard rate: a process point of view (With comments and a rejoinder by the authors)",
  url = "https://doi.org/10.1214/ss/998929473",
  volume = "16",
  year = "2001"
}

@article{leewhitmore2004,
  author = {Mei‐Ling Ting Lee and G. A. Whitmore and Francine Laden and Jaime E. Hart and Eric Garshick},
  title = {Assessing lung cancer risk in railroad workers using a first hitting time regression model},
  journal = {Environmetrics},
  volume = {15},
  year = {2004},
  number = {5},
  pages = {501-512},
  keywords = {death, disease progression, environmetrics, exposure risk, first hitting time, health status, latent process, lung cancer, stochastic process, survival analysis, Wiener process, work environment},
  doi = {10.1002/env.683},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.683},
}


@MISC{ridgeway99,
    author = {Greg Ridgeway},
    title = {The State of Boosting},
    year = {1999}
}


@article{BinderSchumacher2008,
  title={Adapting prediction error estimates for biased complexity selection in high-dimensional bootstrap samples.},
  author={Harald Binder and Martin Schumacher},
  journal={Statistical applications in genetics and molecular biology},
  year={2008},
  volume={7 1},
  pages={Article12}
}

@article{bovelstadborgan,
  author = {Bøvelstad, Hege M. and Borgan, {\O}rnulf},
  year = {2011},
  title = {Assessment of evaluation criteria for survival prediction from genomic data},
  journal = {Biometrical Journal},
  publisher = {Wiley Online Library},
  issn = {1521-4036},
  doi = {10.1002/bimj.201000048},
  volume = {53},
  month = {3},
  pages = {202--216},
  number = {2},
  url = {http:https://doi.org/10.1002/bimj.201000048},
  abstract = {Survival prediction from high-dimensional genomic data is dependent on a proper regularization method. With an increasing number of such methods proposed in the literature, comparative studies are called for and some have been performed. However, there is currently no consensus on which prediction assessment criterion should be used for time-to-event data. Without a firm knowledge about whether the choice of evaluation criterion may affect the conclusions made as to which regularization method performs best, these comparative studies may be of limited value. In this paper, four evaluation criteria are investigated: the log-rank test for two groups, the area under the time-dependent ROC curve (AUC), an R2-measure based on the Cox partial likelihood, and an R2-measure based on the Brier score. The criteria are compared according to how they rank six widely used regularization methods that are based on the Cox regression model, namely univariate selection, principal components regression (PCR), supervised PCR, partial least squares regression, ridge regression, and the lasso. Based on our application to three microarray gene expression data sets, we find that the results obtained from the widely used log-rank test deviate from the other three criteria studied. For future studies, where one also might want to include non-likelihood or non-model-based regularization methods, we argue in favor of AUC and the R2-measure based on the Brier score, as these do not suffer from the arbitrary splitting into two groups nor depend on the Cox partial likelihood.}
}


@Article{seibold,
  author="Seibold, Heidi
  and Bernau, Christoph
  and Boulesteix, Anne-Laure
  and De Bin, Riccardo",
  title="On the choice and influence of the number of boosting steps for high-dimensional linear Cox-models",
  journal="Computational Statistics",
  year="2018",
  month="Sep",
  day="01",
  volume="33",
  number="3",
  pages="1195--1215",
  abstract="In biomedical research, boosting-based regression approaches have gained much attention in the last decade. Their intrinsic variable selection procedure and ability to shrink the estimates of the regression coefficients toward 0 make these techniques appropriate to fit prediction models in the case of high-dimensional data, e.g. gene expressions. Their prediction performance, however, highly depends on specific tuning parameters, in particular on the number of boosting iterations to perform. This crucial parameter is usually selected via cross-validation. The cross-validation procedure may highly depend on a completely random component, namely the considered fold partition. We empirically study how much this randomness affects the results of the boosting techniques, in terms of selected predictors and prediction ability of the related models. We use four publicly available data sets related to four different diseases. In these studies, the goal is to predict survival end-points when a large number of continuous candidate predictors are available. We focus on two well known boosting approaches implemented in the R-packages CoxBoost and mboost, assuming the validity of the proportional hazards assumption and the linearity of the effects of the predictors. We show that the variability in selected predictors and prediction ability of the model is reduced by averaging over several repetitions of cross-validation in the selection of the tuning parameters.",
  issn="1613-9658",
  doi="10.1007/s00180-017-0773-8",
  url="https://doi.org/10.1007/s00180-017-0773-8"
}




@article{hastie2007,
  author = "Hastie, Trevor",
  doi = "10.1214/07-STS242A",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "11",
  number = "4",
  pages = "513--515",
  publisher = "The Institute of Mathematical Statistics",
  title = "Comment: Boosting Algorithms: Regularization, Prediction and Model Fitting",
  url = "https://doi.org/10.1214/07-STS242A",
  volume = "22",
  year = "2007"
}

@article{buhlmann2007,
author = "Bühlmann, Peter and Hothorn, Torsten",
doi = "10.1214/07-STS242",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "11",
number = "4",
pages = "477--505",
publisher = "The Institute of Mathematical Statistics",
title = "Boosting Algorithms: Regularization, Prediction and Model Fitting",
url = "https://doi.org/10.1214/07-STS242",
volume = "22",
year = "2007"
}


@article{inversegauss,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2285899},
 abstract = {This note deals with a method of evaluating the distribution function of the Inverse Gaussian Distribution, from the Standard Normal Distribution.},
 author = {Jonathan Shuster},
 journal = {Journal of the American Statistical Association},
 number = {324},
 pages = {1514--1516},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {On the Inverse Gaussian Distribution Function},
 volume = {63},
 year = {1968}
}



@Article{bin2014,
author="Bin, Riccardo De
and Herold, Tobias
and Boulesteix, Anne-Laure",
title="Added predictive value of omics data: specific issues related to validation illustrated by two case studies",
journal="BMC Medical Research Methodology",
year="2014",
month="Oct",
day="28",
volume="14",
number="1",
pages="117",
abstract="In the last years, the importance of independent validation of the prediction ability of a new gene signature has been largely recognized. Recently, with the development of gene signatures which integrate rather than replace the clinical predictors in the prediction rule, the focus has been moved to the validation of the added predictive value of a gene signature, i.e. to the verification that the inclusion of the new gene signature in a prediction model is able to improve its prediction ability.",
issn="1471-2288",
doi="10.1186/1471-2288-14-117",
url="https://doi.org/10.1186/1471-2288-14-117"
}


@article{chang2010,
  title = "Early stopping in L2Boosting",
  journal = "Computational Statistics \& Data Analysis",
  volume = "54",
  number = "10",
  pages = "2203 - 2213",
  year = "2010",
  issn = "0167-9473",
  doi = "https://doi.org/10.1016/j.csda.2010.03.024",
  url = "http://www.sciencedirect.com/science/article/pii/S016794731000126X",
  author = "Yuan-Chin Ivan Chang and Yufen Huang and Yu-Pai Huang",
  keywords = ", BIC, gMDL, Change point detection method, Boosting, LogitBoost, Stopping rule"
}

@article{waldmann,
  author = {Waldmann, Elisabeth and Taylor-Robinson, David and Klein, Nadja and Kneib, Thomas and Pressler, Tania and Schmid, Matthias and Mayr, Andreas},
  title = {Boosting joint models for longitudinal and time-to-event data},
  journal = {Biometrical Journal},
  volume = {59},
  number = {6},
  pages = {1104-1121},
  keywords = {Boosting, High-dimensional data, Joint modeling, Longitudinal models, Time-to-event analysis, Variable selection},
  doi = {10.1002/bimj.201600158},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201600158},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201600158},
  abstract = {Abstract Joint models for longitudinal and time-to-event data have gained a lot of attention in the last few years as they are a helpful technique clinical studies where longitudinal outcomes are recorded alongside event times. Those two processes are often linked and the two outcomes should thus be modeled jointly in order to prevent the potential bias introduced by independent modeling. Commonly, joint models are estimated in likelihood-based expectation maximization or Bayesian approaches using frameworks where variable selection is problematic and that do not immediately work for high-dimensional data. In this paper, we propose a boosting algorithm tackling these challenges by being able to simultaneously estimate predictors for joint models and automatically select the most influential variables even in high-dimensional data situations. We analyze the performance of the new algorithm in a simulation study and apply it to the Danish cystic fibrosis registry that collects longitudinal lung function data on patients with cystic fibrosis together with data regarding the onset of pulmonary infections. This is the first approach to combine state-of-the art algorithms from the field of machine-learning with the model class of joint models, providing a fully data-driven mechanism to select variables and predictor effects in a unified framework of boosting joint models.}
}

@Article{schmid,
  author="Schmid, Matthias
  and Potapov, Sergej
  and Pfahlberg, Annette
  and Hothorn, Torsten",
  title="Estimation and regularization techniques for regression models with multidimensional prediction functions",
  journal="Statistics and Computing",
  year="2010",
  month="Apr",
  day="01",
  volume="20",
  number="2",
  pages="139--150",
  abstract="Boosting is one of the most important methods for fitting regression models and building prediction rules. A notable feature of boosting is that the technique can be modified such that it includes a built-in mechanism for shrinking coefficient estimates and variable selection. This regularization mechanism makes boosting a suitable method for analyzing data characterized by small sample sizes and large numbers of predictors. We extend the existing methodology by developing a boosting method for prediction functions with multiple components. Such multidimensional functions occur in many types of statistical models, for example in count data models and in models involving outcome variables with a mixture distribution. As will be demonstrated, the new algorithm is suitable for both the estimation of the prediction function and regularization of the estimates. In addition, nuisance parameters can be estimated simultaneously with the prediction function.",
  issn="1573-1375",
  doi="10.1007/s11222-009-9162-7",
  url="https://doi.org/10.1007/s11222-009-9162-7"
}


@inproceedings{kohavi,
 author = {Kohavi, Ron},
 title = {A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model Selection},
 booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2},
 series = {IJCAI'95},
 year = {1995},
 isbn = {1-55860-363-8},
 location = {Montreal, Quebec, Canada},
 pages = {1137--1143},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=1643031.1643047},
 acmid = {1643047},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}


@article{lachenbruch,
    author = { Peter A.   Lachenbruch  and  M.   Ray   Mickey },
    title = {Estimation of Error Rates in Discriminant Analysis},
    journal = {Technometrics},
    volume = {10},
    number = {1},
    pages = {1-11},
    year  = {1968},
    publisher = {Taylor & Francis},
    doi = {10.1080/00401706.1968.10490530},

    URL = { 
            https://www.tandfonline.com/doi/abs/10.1080/00401706.1968.10490530
        
    },
    eprint = { 
            https://www.tandfonline.com/doi/pdf/10.1080/00401706.1968.10490530
        
    }
}

@article{stone1974,
  author = {Stone, Mervyn},
  biburl = {https://www.bibsonomy.org/bibtex/26e8ddd35883bc15b77e69c32afddb67d/becker},
  interhash = {104ded9bd84198810624e40961f90e11},
  intrahash = {6e8ddd35883bc15b77e69c32afddb67d},
  journal = {Journal of the royal statistical society. Series B (Methodological)},
  keywords = {inthesis diss cross validation model comparison citedby:scholar:count:6909 citedby:scholar:timestamp:2017-12-12},
  pages = {111--147},
  publisher = {JSTOR},
  timestamp = {2017-12-12T11:15:21.000+0100},
  title = {Cross-validatory choice and assessment of statistical predictions},
  year = {1974}
}

@article{10.2307/3314888,
 ISSN = {03195724},
 URL = {http://www.jstor.org/stable/3314888},
 abstract = {Multiple regression methods are considered for progressively right-censored inverse-Gaussian data. Maximum-likelihood estimators are derived using the EM algorithm, and their asymptotic distributional properties are presented. The methodology is demonstrated using two case illustrations, one of which involves the defective feature of the inverse-Gaussian distribution. The relationship of the methodology to regression methods for complete inverse-Gaussian samples and to other regression methods for censored survival data is discussed.},
 author = {G. A. Whitmore},
 journal = {The Canadian Journal of Statistics},
 number = {4},
 pages = {305--315},
 publisher = {[Statistical Society of Canada, Wiley]},
 title = {A Regression Method for Censored Inverse-Gaussian Data},
 volume = {11},
 year = {1983}
}

@book{ABG,
  title={Survival and Event History Analysis: A Process Point of View},
  author={Aalen, O. and Borgan, O. and Gjessing, H.},
  isbn={9780387685601},
  lccn={2008927364},
  series={Statistics for Biology and Health},
  url={https://books.google.no/books?id=wEi26X-VuCIC},
  year={2008},
  publisher={Springer New York}
}

@Inbook{cox-model,
author="Cox, David R.",
title="Regression Models and Life-Tables",
bookTitle="Breakthroughs in Statistics: Methodology and Distribution",
year="1992",
publisher="Springer New York",
address="New York, NY",
pages="527--541",
abstract="The analysis of censored failure times is considered. It is assumed that on each individual arc available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.",
isbn="978-1-4612-4380-9",
doi="10.1007/978-1-4612-4380-9_37",
url="https://doi.org/10.1007/978-1-4612-4380-9_37"
}

@book{cox1965,
  title={The theory of stochastic processes},
  author={Cox, D.R. and Miller, H.D.},
  lccn={65028988},
  series={Wiley publications in statistics},
  url={https://books.google.no/books?id=5rpEAAAAIAAJ},
  year={1965},
  publisher={Wiley}
}


@Manual{Rlang,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2013},
  url = {http://www.R-project.org/},
}

@Article{thomas2018,
  author="Thomas, Janek
  and Mayr, Andreas
  and Bischl, Bernd
  and Schmid, Matthias
  and Smith, Adam
  and Hofner, Benjamin",
  title="Gradient boosting for distributional regression: faster tuning and improved variable selection via noncyclical updates",
  journal="Statistics and Computing",
  year="2018",
  month="May",
  day="01",
  volume="28",
  number="3",
  pages="673--687",
  abstract="We present a new algorithm for boosting generalized additive models for location, scale and shape (GAMLSS) that allows to incorporate stability selection, an increasingly popular way to obtain stable sets of covariates while controlling the per-family error rate. The model is fitted repeatedly to subsampled data, and variables with high selection frequencies are extracted. To apply stability selection to boosted GAMLSS, we develop a new ``noncyclical'' fitting algorithm that incorporates an additional selection step of the best-fitting distribution parameter in each iteration. This new algorithm has the additional advantage that optimizing the tuning parameters of boosting is reduced from a multi-dimensional to a one-dimensional problem with vastly decreased complexity. The performance of the novel algorithm is evaluated in an extensive simulation study. We apply this new algorithm to a study to estimate abundance of common eider in Massachusetts, USA, featuring excess zeros, overdispersion, nonlinearity and spatiotemporal structures. Eider abundance is estimated via boosted GAMLSS, allowing both mean and overdispersion to be regressed on covariates. Stability selection is used to obtain a sparse set of stable predictors.",
  issn="1573-1375",
  doi="10.1007/s11222-017-9754-6",
  url="https://doi.org/10.1007/s11222-017-9754-6"
}

@article{gamboostlss-paper,
 ISSN = "00359254, 14679876",
 URL = "http://www.jstor.org/stable/23251131",
 abstract = "Generalized additive models for location, scale and shape (GAMLSSs) are a popular semiparametric modelling approach that, in contrast with conventional generalized additive models, regress not only the expected mean but also every distribution parameter (e.g. location, scale and shape) to a set of covariates. Current fitting procedures for GAMLSSs are infeasible for high dimensional data set-ups and require variable selection based on (potentially problematic) information criteria. The present work describes a boosting algorithm for high dimensional GAMLSSs that was developed to overcome these limitations. Specifically, the new algorithm was designed to allow the simultaneous estimation of predictor effects and variable selection. The algorithm proposed was applied to Munich rental guide data, which are used by landlords and tenants as a reference for the average rent of a flat depending on its characteristics and spatial features. The net rent predictions that resulted from the high dimensional GAMLSSs were found to be highly competitive and covariate-specific prediction intervals showed a major improvement over classical generalized additive models.",
 author = "Andreas Mayr and Nora Fenske and Benjamin Hofner and Thomas Kneib and Matthias Schmid",
 journal = "Journal of the Royal Statistical Society. Series C (Applied Statistics)",
 number = "3",
 pages = "403--427",
 publisher = "Wiley",
 title = "Generalized additive models for location, scale and shape for high dimensional data—a flexible approach based on boosting ",
 volume = "61",
 year = "2012"
}

@book{ESL,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  isbn={9780387848846},
  lccn={2008941148},
  series={Springer series in statistics},
  url={https://books.google.no/books?id=eBSgoAEACAAJ},
  year={2009},
  publisher={Springer}
}


@article{schmid-hothorn,
 author = {Schmid, Matthias and Hothorn, Torsten},
 title = {Boosting Additive Models Using Component-wise P-Splines},
 journal = {Comput. Stat. Data Anal.},
 issue_date = {December, 2008},
 volume = {53},
 number = {2},
 month = dec,
 year = {2008},
 issn = {0167-9473},
 pages = {298--311},
 numpages = {14},
 url = {http://dx.doi.org/10.1016/j.csda.2008.09.009},
 doi = {10.1016/j.csda.2008.09.009},
 acmid = {1461036},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
} 

@Article{bauer-kohavi,
  author="Bauer, Eric
  and Kohavi, Ron",
  title="An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants",
  journal="Machine Learning",
  year="1999",
  month="Jul",
  day="01",
  volume="36",
  number="1",
  pages="105--139",
  abstract="Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only ``hard'' areas but also outliers and noise.",
  issn="1573-0565",
  doi="10.1023/A:1007515423169",
  url="https://doi.org/10.1023/A:1007515423169"
}



@article{breiman1998,
  author = "Breiman, Leo",
  doi = "10.1214/aos/1024691079",
  fjournal = "The Annals of Statistics",
  journal = "Ann. Statist.",
  month = "06",
  number = "3",
  pages = "801--849",
  publisher = "The Institute of Mathematical Statistics",
  title = "Arcing classifier (with discussion and a rejoinder by the     author)",
  url = "https://doi.org/10.1214/aos/1024691079",
  volume = "26",
  year = "1998"
}

@article{gamlss,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/3592732},
 abstract = {A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton-Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
 author = {R. A. Rigby and D. M. Stasinopoulos},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {3},
 pages = {507--554},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Generalized Additive Models for Location, Scale and Shape},
 volume = {54},
 year = {2005}
}




@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}


@inproceedings{saddlepoints,
 author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
 title = {Identifying and Attacking the Saddle Point Problem in High-dimensional Non-convex Optimization},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {2933--2941},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969154},
 acmid = {2969154},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{brier1950,
    author = {Brier, Glenn W.},
    day = {1},
    doi = {10.1175/1520-0493(1950)078\%3C0001:vofeit\%3E2.0.co;2},
    journal = {Monthly Weather Review},
    keywords = {brier, score},
    month = jan,
    number = {1},
    pages = {1--3},
    posted-at = {2011-09-26 08:03:08},
    priority = {2},
    publisher = {American Meteorological Society},
    title = {{Verification of Forecasts expressed in terms of probability}},
    url = {http://dx.doi.org/10.1175/1520-0493(1950)078\%3C0001:vofeit\%3E2.0.co;2},
    volume = {78},
    year = {1950}
}


@article{graf,
author = {Graf, Erika and Schmoor, Claudia and Sauerbrei, Willi and Schumacher, Martin},
title = {Assessment and comparison of prognostic classification schemes for survival data},
journal = {Statistics in Medicine},
volume = {18},
number = {17‐18},
pages = {2529-2545},
doi = {10.1002/(SICI)1097-0258(19990915/30)18:17/18<2529::AID-SIM274>3.0.CO;2-5},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819990915/30%2918%3A17/18%3C2529%3A%3AAID-SIM274%3E3.0.CO%3B2-5},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-0258%2819990915/30%2918%3A17/18%3C2529%3A%3AAID-SIM274%3E3.0.CO%3B2-5},
abstract = {Abstract Prognostic classification schemes have often been used in medical applications, but rarely subjected to a rigorous examination of their adequacy. For survival data, the statistical methodology to assess such schemes consists mainly of a range of ad hoc approaches, and there is an alarming lack of commonly accepted standards in this field. We review these methods and develop measures of inaccuracy which may be calculated in a validation study in order to assess the usefulness of estimated patient-specific survival probabilities associated with a prognostic classification scheme. These measures are meaningful even when the estimated probabilities are misspecified, and asymptotically they are not affected by random censorship. In addition, they can be used to derive R2-type measures of explained residual variation. A breast cancer study will serve for illustration throughout the paper. Copyright © 1999 John Wiley \& Sons, Ltd.},
year = {1999}
}

@article{hastie1986,
  author = "Hastie, Trevor and Tibshirani, Robert",
  doi = "10.1214/ss/1177013604",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "08",
  number = "3",
  pages = "297--310",
  publisher = "The Institute of Mathematical Statistics",
  title = "Generalized Additive Models",
  url = "https://doi.org/10.1214/ss/1177013604",
  volume = "1",
  year = "1986"
}

@article{eaton-whitmore,
author = { William W.   Eaton  and  G. A.   Whitmore },
title = {Length of stay as a stochastic process: A general approach and application to hospitalization for schizophrenia},
journal = {The Journal of Mathematical Sociology},
volume = {5},
number = {2},
pages = {273-292},
year  = {1977},
publisher = {Routledge},
doi = {10.1080/0022250X.1977.9989877},

URL = { 
        https://doi.org/10.1080/0022250X.1977.9989877
    
},
eprint = { 
        https://doi.org/10.1080/0022250X.1977.9989877
    
}

}
% A general approach to the study of length of stay (LOS) for hospitalization is presented. Data on first hospitalization for schizophrenia from the Maryland Psychiatric Case Register are applied to discussions of the Life Table and seven stochastic models of the LOS process. As far as possible, prior applications of the various models to this process are reviewed, and the models are conceptualized on the individual and aggregate level. The models are the exponential, mixed exponential, type XI, Weibull, gamma, lognormal and Inverse Gaussian. The lognormal and Inverse Gaussian show the best fits to the data in terms of the maximum absolute deviation. However, the Inverse Gaussian is superior due to its attractive statistical characterization. Special attention is given to the relatively new Inverse Gaussian, and there is a brief section on LOS and theory verification. Recommendations are made for future LOS research.

@article{lancaster,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344321},
 abstract = {This paper is a study of the nature of the frequency distribution of the duration of strikes in the United Kingdom as recorded by the Department of Employment. The settlement of a strike is regarded as a probabilistic process and the duration of a strike is treated as an observation on a random variable. A model for this random duration is created by supposing that at each point of time after the commencement of a strike there exists an index of the difference between the parties to the dispute. This index is itself regarded as a one-dimensional stochastic process in continuous time and space and determines the duration of the strike by the first point of time at which the difference index passes through an absorbing barrier representing "agreement". The duration of a strike then becomes the First Passage time of a stochastic process to a single absorbing barrier. As a first approximation the index process is assumed to be simple Brownian motion with drift and in consequence the duration of a strike has the Inverse Gaussian for its probability distribution. The fit of this distribution to the observations is shown to be, with few exceptions, very close.},
 author = {Tony Lancaster},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {2},
 pages = {257--271},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A Stochastic Model for the Duration of a Strike},
 volume = {135},
 year = {1972}
}

@article{whitmore1975,
author = {Whitmore, G. A.},
year = {1975},
month = {02},
pages = {297-302},
title = {The inverse Gaussian distribution as a model of hospital stay},
volume = {10},
journal = {Health services research}
}

@article{mayr17,
author = {Mayr, Andreas and Hofner, Benjamin and Waldmann, Elisabeth and Hepp, Tobias and Meyer, Sebastian and Gefeller, Olaf},
year = {2017},
month = {08},
pages = {1-12},
title = {An Update on Statistical Boosting in Biomedicine},
volume = {2017},
journal = {Computational and Mathematical Methods in Medicine},
doi = {10.1155/2017/6083072}
}

@article{whitmore1979,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2982553},
 abstract = {A Wiener process with drift is presented as a model of employee job attachment. The model implies a defective inverse Gaussian distribution form for employee service times. The inverse Gaussian form is fitted to some published employee-service data with encouraging results. The model's contribution to our understanding of occupational variables and employment experience is discussed. It is shown that the model provides a useful basis for formulating and testing hypotheses about labour turnover phenomena. Although the primary emphasis is on the statistical aspects of the subject, some practical implications for the personnel and labour fields are considered.},
 author = {G. A. Whitmore},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {4},
 pages = {468--478},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {An Inverse Gaussian Model for Labour Turnover},
 volume = {142},
 year = {1979}
}


@Article{whitmore1995,
author="Whitmore, G. A.",
title="Estimating degradation by a wiener diffusion process subject to measurement error",
journal="Lifetime Data Analysis",
year="1995",
month="Sep",
day="01",
volume="1",
number="3",
pages="307--319",
abstract="Most materials and components degrade physically before they fail. Engineering degradation tests are designed to measure these degradation processes. Measurements in the tests reflect the inherent randomness of degradation itself as well as measurement errors created by imperfect instruments, procedures and environments. This paper describes a statistical model for measured degradation data that takes both sources of variation into account. The degradation process in the model is taken to be a Wiener diffusion process. The measurement errors are assumed to be independent normal random outcomes that are independent of the degradation process. The paper describes inference procedures for the model and discusses some practical issues that must be considered in dealing with the statistical problem. A case study is presented.",
issn="1572-9249",
doi="10.1007/BF00985762",
url="https://doi.org/10.1007/BF00985762"
}

@article{kaplan-meier,
author = { E. L.   Kaplan  and  Paul   Meier },
title = {Nonparametric Estimation from Incomplete Observations},
journal = {Journal of the American Statistical Association},
volume = {53},
number = {282},
pages = {457-481},
year  = {1958},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1958.10501452},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501452
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1958.10501452
    
}

}


@article{aalen1978,
author = "Aalen, Odd",
doi = "10.1214/aos/1176344247",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "07",
number = "4",
pages = "701--726",
publisher = "The Institute of Mathematical Statistics",
title = "Nonparametric Inference for a Family of Counting Processes",
url = "https://doi.org/10.1214/aos/1176344247",
volume = "6",
year = "1978"
}

@article{nelson,
author = { Wayne   Nelson },
title = {Theory and Applications of Hazard Plotting for Censored Failure Data},
journal = {Technometrics},
volume = {14},
number = {4},
pages = {945-966},
year  = {1972},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1972.10488991},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/00401706.1972.10488991
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/00401706.1972.10488991

}

}

@book{gam-book,
  title={Generalized Additive Models},
  author={Hastie, T.J. and Tibshirani, R.J.},
  isbn={9780412343902},
  lccn={99015185},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.no/books?id=qa29r1Ze1coC},
  year={1990},
  publisher={Taylor \& Francis}
}

@article{leewhitmore2004a,
author = {Lee, Mei-Ling and Whitmore, G. A.},
year = {2003},
month = {12},
pages = {537-543},
title = {First Hitting Time Models for Lifetime Data},
volume = {23},
journal = {Handbook of Statistics},
doi = {10.1016/S0169-7161(03)23030-3}
}

@article{singpurwalla1995,
  author = "Singpurwalla, Nozer D.",
  doi = "10.1214/ss/1177010132",
  fjournal = "Statistical Science",
  journal = "Statist. Sci.",
  month = "02",
  number = "1",
  pages = "86--103",
  publisher = "The Institute of Mathematical Statistics",
  title = "Survival in Dynamic Environments",
  url = "https://doi.org/10.1214/ss/1177010132",
  volume = "10",
  year = "1995"
}

@Article{lawless2004,
author="Lawless, Jerry
and Crowder, Martin",
title="Covariates and Random Effects in a Gamma Process Model with Application to Degradation and Failure",
journal="Lifetime Data Analysis",
year="2004",
month="Sep",
day="01",
volume="10",
number="3",
pages="213--227",
abstract="The gamma process is a natural model for degradation processes in which deterioration is supposed to take place gradually over time in a sequence of tiny increments. When units or individuals are observed over time it is often apparent that they degrade at different rates, even though no differences in treatment or environment are present. Thus, in applying gamma-process models to such data, it is necessary to allow for such unexplained differences. In the present paper this is accomplished by constructing a tractable gamma-process model incorporating a random effect. The model is fitted to some data on crack growth and corresponding goodness-of-fit tests are carried out. Prediction calculations for failure times defined in terms of degradation level passages are developed and illustrated.",
issn="1572-9249",
doi="10.1023/B:LIDA.0000036389.14073.dd",
url="https://doi.org/10.1023/B:LIDA.0000036389.14073.dd"
}

@book{lawless2011,
  title={Statistical Models and Methods for Lifetime Data},
  author={Lawless, J.F.},
  isbn={9781118031254},
  lccn={2002151805},
  series={Wiley Series in Probability and Statistics},
  url={https://books.google.no/books?id=bvTgR4qbN50C},
  year={2011},
  publisher={Wiley}
}

@article{gamlssR,
title = {Generalized Additive Models for Location Scale and Shape (GAMLSS) in R},
author = {Stasinopoulos, D. Mikis and Rigby, Robert A.},
year = {2007},
journal = {Journal of Statistical Software},
volume = {023},
number = {i07},
abstract = {GAMLSS is a general framework for fitting regression type models where the distribution of the response variable does not have to belong to the exponential family and includes highly skew and kurtotic continuous and discrete distribution. GAMLSS allows all the parameters of the distribution of the response variable to be modelled as linear/non-linear or smooth functions of the explanatory variables. This paper starts by defining the statistical framework of GAMLSS, then describes the current implementation of GAMLSS in R and finally gives four different data examples to demonstrate how GAMLSS can be used for statistical modelling.},
url = {https://EconPapers.repec.org/RePEc:jss:jstsof:v:023:i07}
}



@article{oberthuer-data,
  author = {Oberthuer, André and Kaderali, Lars and Kahlert, Yvonne and Hero, Barbara and Westermann, Frank and Berthold, Frank and Brors, Benedikt and Eils, Roland and Fischer, Matthias},
  year = {2008},
  month = {10},
  pages = {6590-6601},
  title = {Subclassification and Individual Survival Time Prediction from Gene Expression Data of Neuroblastoma Patients by Using CASPAR},
  volume = {14},
  journal = {Clinical cancer research : an official journal of the American Association for Cancer Research},
  doi = {10.1158/1078-0432.CCR-07-4377}
}

@inproceedings{marisa-data,
  title={Gene Expression Classification of Colon Cancer into Molecular Subtypes: Characterization, Validation, and Prognostic Value},
  author={Laetitia Marisa and Aur{\'e}lien de Reyni{\`e}s and A Duval and Janick S{\`e}lves and Marie Pierre Gaub and Laure Vescovo and Marie-Christine Etienne-Grimaldi and Renaud Schiappa and Dominique Guenot and Mira Ayadi and Sylvain Kirzin and Maurice Chazal and Jean-François Fl{\'e}jou and Daniel Benchimol and Anne Dunn Berger and Arnaud Lagarde and Erwan Pencreach and F. Piard and Dominique Elias and Yann R Parc and Sylviane Olschwang and G{\'e}rard Milano and Pierre Laurent-Puig and Val{\'e}rie Boige},
  booktitle={PLoS medicine},
  year={2013}
}

@Article{bovelstad2009,
  author="B{\o}velstad, Hege M.
  and Nyg{\aa}rd, St{\aa}le
  and Borgan, {\O}rnulf",
  title="Survival prediction from clinico-genomic models - a comparative study",
  journal="BMC Bioinformatics",
  year="2009",
  month="Dec",
  day="13",
  volume="10",
  number="1",
  pages="413",
  abstract="Survival prediction from high-dimensional genomic data is an active field in today's medical research. Most of the proposed prediction methods make use of genomic data alone without considering established clinical covariates that often are available and known to have predictive value. Recent studies suggest that combining clinical and genomic information may improve predictions, but there is a lack of systematic studies on the topic. Also, for the widely used Cox regression model, it is not obvious how to handle such combined models.",
  issn="1471-2105",
  doi="10.1186/1471-2105-10-413",
  url="https://doi.org/10.1186/1471-2105-10-413"
}



@article{efron2004,
  author = "Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert",
  doi = "10.1214/009053604000000067",
  fjournal = "The Annals of Statistics",
  journal = "Ann. Statist.",
  month = "04",
  number = "2",
  pages = "407--499",
  publisher = "The Institute of Mathematical Statistics",
  title = "Least angle regression",
  url = "https://doi.org/10.1214/009053604000000067",
  volume = "32",
  year = "2004"
}



@article{efron1975,
title = "Biased versus unbiased estimation",
journal = "Advances in Mathematics",
volume = "16",
number = "3",
pages = "259 - 277",
year = "1975",
issn = "0001-8708",
doi = "https://doi.org/10.1016/0001-8708(75)90114-0",
url = "http://www.sciencedirect.com/science/article/pii/0001870875901140",
author = "Bradley Efron",
abstract = "Statisticians have begun to realize that certain deliberately induced biases can dramatically improve estimation properties when there are several parameters to be estimated. This represents a radical departure from the tradition of unbiased estimation which has dominated statistical thinking since the work of Gauss. We briefly describe the new methods and give three examples of their practical application."
}

@article{copas1983,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345402},
 abstract = {The fit of a regression predictor to new data is nearly always worse than its fit to the original data. Anticipating this shrinkage leads to Stein-type predictors which, under certain assumptions, give a uniformly lower prediction mean squared error than least squares. Shrinkage can be particularly marked when stepwise fitting is used: the shrinkage is then closer to that expected of the full regression rather than of the subset regression actually fitted. Preshrunk predictors for selected subsets are proposed and tested on a number of practical examples. Both multiple and binary (logistic) regression models are considered.},
 author = {J. B. Copas},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {3},
 pages = {311--354},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression, Prediction and Shrinkage},
 volume = {45},
 year = {1983}
}



@article{bovelstad2007,
    author = {Frigessi, A. and Størvold, H.L. and Bøvelstad, H.M. and Aldrin, M. and Borgan, {\O}. and Lingjærde, O.C. and Nygård, S.},
    title = "{Predicting survival from microarray data—a comparative study}",
    journal = {Bioinformatics},
    volume = {23},
    number = {16},
    pages = {2080-2087},
    year = {2007},
    month = {06},
    abstract = "{Motivation: Survival prediction from gene expression data and other high-dimensional genomic data has been subject to much research during the last years. These kinds of data are associated with the methodological problem of having many more gene expression values than individuals. In addition, the responses are censored survival times. Most of the proposed methods handle this by using Cox's proportional hazards model and obtain parameter estimates by some dimension reduction or parameter shrinkage estimation technique. Using three well-known microarray gene expression data sets, we compare the prediction performance of seven such methods: univariate selection, forward stepwise selection, principal components regression (PCR), supervised principal components regression, partial least squares regression (PLS), ridge regression and the lasso.Results: Statistical learning from subsets should be repeated several times in order to get a fair comparison between methods. Methods using coefficient shrinkage or linear combinations of the gene expression values have much better performance than the simple variable selection methods. For our data sets, ridge regression has the overall best performance.Availability: Matlab and R code for the prediction methods are available at http://www.med.uio.no/imb/stat/bmms/software/microsurv/.Contact:hegembo@math.uio.no}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btm305},
    url = {https://doi.org/10.1093/bioinformatics/btm305},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/23/16/2080/503301/btm305.pdf},
}


@book{maller1996survival,
  title={Survival Analysis with Long-Term Survivors},
  author={Maller, R.A. and Zhou, X.},
  isbn={9780471962014},
  lccn={96026979},
  series={Wiley Series in Child Care and Protection},
  url={https://books.google.no/books?id=V3hFAAAAYAAJ},
  year={1996},
  publisher={Wiley}
}



@article{simulation-studies,
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  title = {Using simulation studies to evaluate statistical methods},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074-2102},
  keywords = {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
  doi = {10.1002/sim.8086},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  year = {2019}
}

@article{blasi-hjort,
    author = {DE BLASI, PIERPAOLO and HJORT, NILS LID},
    title = {Bayesian Survival Analysis in Proportional Hazard Models with Logistic Relative Risk},
    journal = {Scandinavian Journal of Statistics},
    volume = {34},
    number = {1},
    pages = {229-257},
    keywords = {Bayesian semiparametrics, Bernshteĭn, von Mises theorem, Beta processes, hazard regression, Poisson random measures},
    doi = {10.1111/j.1467-9469.2006.00543.x},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2006.00543.x},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2006.00543.x},
    abstract = {Abstract.  The traditional Cox proportional hazards regression model uses an exponential relative risk function. We argue that under various plausible scenarios, the relative risk part of the model should be bounded, suggesting also that the traditional model often might overdramatize the hazard rate assessment for individuals with unusual covariates. This motivates our working with proportional hazards models where the relative risk function takes a logistic form. We provide frequentist methods, based on the partial likelihood, and then go on to semiparametric Bayesian constructions. These involve a Beta process for the cumulative baseline hazard function and any prior with a density, for example that dictated by a Jeffreys-type argument, for the regression coefficients. The posterior is derived using machinery for Lévy processes, and a simulation recipe is devised for sampling from the posterior distribution of any quantity. Our methods are illustrated on real data. A Bernshteĭn–von Mises theorem is reached for our class of semiparametric priors, guaranteeing asymptotic normality of the posterior processes.},
    year = {2007}
}

@article{bretagnolle,
 ISSN = {03036898, 14679469},
 URL = {http://www.jstor.org/stable/4616093},
 abstract = {In the usual linear regression with independent explanatory variables, one can omit some of them and still get consistent estimates for the coefficients of the remainders. We show here that it is not the case in Cox's regression model for survival data with censoring, and predict the sign of the bias in the two following cases: first, when there is only one covariate left in the used model whatever be the number of omitted covariates. Then the effect on the survival of the covariate under study is always underestimated. Secondly, in case of several covariates remaining in the studied model we prove that the same result of underestimation holds for each of them at least up to some fixed time t0 which, in practical cases, is reasonably long. The asymptotic bias resulting from such omissions is not negligible as we show from explicit computations. In practical cases, with sample sizes not exceeding 100 or 200, this bias results in changing the risk for confidence intervals from 5 to 50%.},
 author = {Jean Bretagnolle and Catherine Huber-Carol},
 journal = {Scandinavian Journal of Statistics},
 number = {2},
 pages = {125--138},
 publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
 title = {Effects of Omitting Covariates in Cox's Model for Survival Data},
 volume = {15},
 year = {1988}
}


@article{kneib2013,
  author = {Thomas Kneib},
  title ={Beyond mean regression},
  journal = {Statistical Modelling},
  volume = {13},
  number = {4},
  pages = {275-303},
  year = {2013},
  doi = {10.1177/1471082X13494159},

  URL = { 
          https://doi.org/10.1177/1471082X13494159
      
  },
  eprint = { 
          https://doi.org/10.1177/1471082X13494159
      
  }
  ,
      abstract = { Usual exponential family regression models focus on only one designated quantity of the response distribution, namely the mean. While this entails easy interpretation of the estimated regression effects, it may often lead to incomplete analyses when more complex relationships are indeed present and also bears the risk of false conclusions about the significance/importance of covariates. We will therefore give an overview on extended types of regression models that allows us to go beyond mean regression. More specifically, we will consider generalized additive models for location, scale and shape as well as semiparametric quantile and expectile regression. We will review the basic properties of all three approaches and compare them with respect to the flexibility in terms of the supported types of predictor specification, the availability of software and the support for different types of inferential procedures. The considered model classes are illustrated using a data set on rents for flats in the City of Munich. }
}